{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "utility-italian",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "noted-driver",
   "metadata": {},
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "diagnostic-heaven",
   "metadata": {},
   "source": [
    "For $k = 0, 1, 2$ we have the following files:\n",
    "* Xtrk.csv - the training sequences.\n",
    "* Xtek.csv - the test sequences.\n",
    "* Ytrk.csv - labels for the training sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "reverse-sociology",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtr0_mat100 = np.genfromtxt(\"data/Xtr0_mat100.csv\", delimiter='')\n",
    "Ytr0 = np.genfromtxt(\"data/Ytr0.csv\", delimiter=',', skip_header=1)\n",
    "\n",
    "Xtr1_mat100 = np.genfromtxt(\"data/Xtr1_mat100.csv\", delimiter='')\n",
    "Ytr1 = np.genfromtxt(\"data/Ytr1.csv\", delimiter=',', skip_header=1)\n",
    "\n",
    "Xtr2_mat100 = np.genfromtxt(\"data/Xtr2_mat100.csv\", delimiter='')\n",
    "Ytr2 = np.genfromtxt(\"data/Ytr2.csv\", delimiter=',', skip_header=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "coordinate-chassis",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_true,y_pred, mode='SVM'):\n",
    "    n = y_true.shape[0]\n",
    "    if mode == 'SVM':\n",
    "        predictions = np.ones(n)\n",
    "        predictions[y_pred < 0] = 0\n",
    "    else:\n",
    "        predictions = np.zeros(n)\n",
    "        predictions[y_pred >= 0.5] = 1\n",
    "    \n",
    "    return np.sum(y_true == predictions) / n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "invalid-present",
   "metadata": {},
   "source": [
    "# Implementing some kernels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "white-lounge",
   "metadata": {},
   "source": [
    "## Linear Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "congressional-authority",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_kernel(X_train, X_valid, scale=True, mode=\"train\"):\n",
    "    \n",
    "    if scale:\n",
    "        X_tr = (X_train-X_train.mean(axis=0)) / X_train.std(axis=0)\n",
    "        X_va = (X_valid-X_train.mean(axis=0)) / X_train.std(axis=0)\n",
    "        \n",
    "        K_va = X_va @ X_tr.T\n",
    "        \n",
    "        if mode == \"test\":\n",
    "            return(K_va)\n",
    "        \n",
    "        K_tr = X_tr @ X_tr.T\n",
    "        \n",
    "    else:\n",
    "        K_va = X_valid @ X_train.T\n",
    "        \n",
    "        if mode == \"test\":\n",
    "            return(K_va)\n",
    "        \n",
    "        K_tr = X_train @ X_train.T\n",
    "        \n",
    "    return(K_tr, K_va)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adequate-momentum",
   "metadata": {},
   "source": [
    "## Gaussian Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "proprietary-costa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Idea : efficient computation of the pairwise distances\n",
    "def gaussian_kernel(X_train, X_valid, sigma=None, scale=True, scale_sigma=True, mode=\"train\"):\n",
    "    \n",
    "    n, p = X_train.shape\n",
    "    \n",
    "    if scale:\n",
    "        X_tr = (X_train-X_train.mean(axis=0)) / X_train.std(axis=0)\n",
    "        X_va = (X_valid-X_train.mean(axis=0)) / X_train.std(axis=0)\n",
    "    if scale_sigma:\n",
    "        sigma = p\n",
    "        \n",
    "    K_va = np.linalg.norm(X_va[:, None, ...] - X_tr[None, ...], axis=-1)**2\n",
    "    K_va = np.exp((-K_va)/(sigma))\n",
    "    \n",
    "    if mode==\"test\":\n",
    "        return(K_va)\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        K_tr = ((X_tr[:, :, None] - X_tr[:, :, None].T) ** 2).sum(1)\n",
    "        K_tr = np.exp((-K_tr)/(sigma))\n",
    "        return(K_tr, K_va)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "handmade-preview",
   "metadata": {},
   "source": [
    "## Polynomial Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "fatal-senate",
   "metadata": {},
   "outputs": [],
   "source": [
    "def polynomial_kernel(X_train, X_valid, d=3, c=1, scale=True, mode=\"train\"):\n",
    "    \n",
    "    # k(x,y) = (<x,y> + c)**d\n",
    "    if scale:\n",
    "        \n",
    "        X_tr = (X_train-X_train.mean(axis=0)) / X_train.std(axis=0)\n",
    "        X_va = (X_valid-X_train.mean(axis=0)) / X_train.std(axis=0)\n",
    "        \n",
    "        K_va = X_va @ X_tr.T + c\n",
    "        K_va = K_va**d\n",
    "        \n",
    "        if mode==\"test\":\n",
    "            return(K_va)\n",
    "        \n",
    "        K_tr = X_tr @ X_tr.T + c\n",
    "        K_tr = K_tr**d\n",
    "    \n",
    "        \n",
    "    else:\n",
    "        K_va = X_valid @ X_train.T + c\n",
    "        K_va = K_va**d\n",
    "        \n",
    "        if mode==\"test\":\n",
    "            return(K_va)\n",
    "        \n",
    "        K_tr = X_train @ X_train.T + c\n",
    "        K_tr = K_tr**d\n",
    "        \n",
    "    return(K_tr, K_va)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "common-exclusion",
   "metadata": {},
   "source": [
    "## Kernel Ridge Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "appreciated-november",
   "metadata": {},
   "source": [
    "* Consider RKHS $\\mathcal H$, associated to a p.d. kernel K on $\\mathcal X$\n",
    "* Let $y = (y_1, \\dots, y_n)^T \\in \\mathbb R ^n$\n",
    "* Let $\\alpha = (\\alpha_1, \\dots, \\alpha_n)^T \\in \\mathbb R ^n$\n",
    "* Let $K$ be the $n\\times n$ Gram Matrix such that $K_{i,j} = K(x_i, x_j)$\n",
    "* We can then write\n",
    "$$\n",
    "(\\hat f(x_1), \\dots, \\hat f(x_n))^T = K\\alpha\n",
    "$$\n",
    "* The norm is $||\\hat f||^2_{\\mathcal H} = \\alpha^T K \\alpha$\n",
    "* KRR $\\leftrightarrow \\text{argmin}_{\\alpha \\in \\mathbb R^n} \\frac{1}{n} (K\\alpha - y)^T(K\\alpha - y) + \\lambda \\alpha^T K \\alpha$\n",
    "* Solution for $\\lambda > 0$:\n",
    "$$\n",
    "\\alpha = (K+\\lambda nI)^{-1}y\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "scientific-andrews",
   "metadata": {},
   "outputs": [],
   "source": [
    "def KRR(K, y, Kval, yval, lambd):\n",
    "    \"\"\"\n",
    "    takes the kernel matrix as an input and computes the MSE and the predictions for each value in lambd (list)\n",
    "    \"\"\"\n",
    "    assert K.shape[0] == y.shape[0]\n",
    "    assert len(lambd) > 0\n",
    "    n = K.shape[0]\n",
    "    \n",
    "    loss = []\n",
    "    acc = []\n",
    "    \n",
    "    loss_val = []\n",
    "    acc_val = []\n",
    "    alphas = []\n",
    "    \n",
    "    for l in lambd:\n",
    "        \n",
    "        assert l >= 0\n",
    "        # find the parameter alpha\n",
    "        alpha = np.linalg.solve((K + l*n*np.eye(n)), y)\n",
    "        # predict\n",
    "        \n",
    "        loss_lambda = MSE(K, y, l, alpha)\n",
    "        acc_lambda = accuracy(y,K@alpha, mode=\"KRR\")\n",
    "        \n",
    "        loss_lambdaval = MSE(Kval, yval, l, alpha)\n",
    "        acc_lambdaval = accuracy(yval,Kval@alpha, mode=\"KRR\")\n",
    "\n",
    "        print(f\"***********lambda = {l}***********\")\n",
    "        print(f\"Training: loss = {loss_lambda:.4f}, accuracy = {acc_lambda:.6f}\")\n",
    "        print(f\"Validation: loss = {loss_lambdaval:.4f}, accuracy = {acc_lambdaval:.6f}\")\n",
    "        \n",
    "        loss += [loss_lambda]\n",
    "        acc += [acc_lambda]\n",
    "        \n",
    "        loss_val += [loss_lambdaval]\n",
    "        acc_val += [acc_lambdaval]\n",
    "        \n",
    "        \n",
    "        alphas +=[alpha]\n",
    "        \n",
    "    return(alphas, loss, acc, loss_val, acc_val)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "progressive-respect",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MSE(K, y, lambd, alpha):\n",
    "    n = y.shape[0]\n",
    "    data_term = (np.linalg.norm(np.dot(K, alpha.reshape(-1,1)) - y)**2)/n\n",
    "    reg_term = alpha @ K @ alpha\n",
    "    return(data_term + lambd * reg_term)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spatial-purple",
   "metadata": {},
   "source": [
    "## Kernel Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "marine-association",
   "metadata": {},
   "source": [
    "- Binary Classificaiton setup: $\\mathcal Y = \\{-1, 1\\}$\n",
    "- $\\mathcal l_{\\text{logistic}}(f(x),y) = -\\log p(y|f(x)) = \\log(1 + e^{-yf(x)})$ where $p(y|f(x)) = \\sigma(y(f(x))$\n",
    "\n",
    "Objective:\n",
    "\\begin{align*}\n",
    "\\hat f &= \\text{argmin}_{f\\in \\mathcal H} \\frac{1}{n} \\sum_{i=1}^n \\log(1+e^{-y_if(x_i)}) + \\frac{\\lambda}{2}||f||^2_{\\mathcal H}\\\\\n",
    "\\alpha &= \\text{argmin}_{\\alpha \\in \\mathbb R^n} \\frac{1}{n} \\sum_{i=1}^n \\log(1+e^{-y_i[K\\alpha]_i}) + \\frac{\\lambda}{2} \\alpha^T K \\alpha\n",
    "\\end{align*}\n",
    "\n",
    "We define the following fonctions and vectors:\n",
    "* $\\mathcal l _\\text{logistic}(u) = \\log(1+e^{-u})$\n",
    "* $\\mathcal l' _\\text{logistic}(u) = -\\sigma(-u)$\n",
    "* $\\mathcal l'' _\\text{logistic}(u) = \\sigma(u)\\sigma(-u)$\n",
    "\n",
    "* for $i = 1, \\dots, n$, $P_i(\\alpha) = \\mathcal l' _\\text{logistic}(y_i[K\\alpha]_i)$\n",
    "* for $i = 1, \\dots, n$, $W_i(\\alpha) = \\mathcal l'' _\\text{logistic}(y_i[K\\alpha]_i)$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\\begin{align*}\n",
    "J(\\alpha) &= \\frac{1}{n} \\sum_{i=1}^n \\log(1+e^{-y_i[K\\alpha]_i}) + \\frac{\\lambda}{2} \\alpha^T K \\alpha\\\\\n",
    "\\nabla J(\\alpha) &= \\frac{1}{n} KP(\\alpha) y + \\lambda K \\alpha \\quad \\text{where } P(\\alpha) = \\text{diag}(P_1(\\alpha), \\dots, P_n(\\alpha))\\\\\n",
    "\\nabla^2 J(\\alpha) &= \\frac{1}{n}KW(\\alpha)K+\\lambda K \\quad \\text{where } W(\\alpha) = \\text{diag}(W_1(\\alpha), \\dots, W_n(\\alpha))\n",
    "\\end{align*}\n",
    "\n",
    "We are interested in the quadratic approximation of $J$ near a point $\\alpha_0$:\n",
    "\\begin{align*}\n",
    "J_q(\\alpha) &= J(\\alpha_0) + (\\alpha - \\alpha_0)^T \\nabla J(\\alpha_0) + \\frac{1}{2} (\\alpha - \\alpha_0)^T \\nabla^2 J(\\alpha_0)(\\alpha - \\alpha_0)\\\\\n",
    "2J_q(\\alpha) &= -\\frac{2}{n} \\alpha^T KW(K\\alpha_0-W^{-1}Py)+\\frac{1}{n}\\alpha^TKWK\\alpha+ \\lambda\\alpha^TK\\alpha +C\\\\\n",
    "&= \\frac{1}{n} (K\\alpha - z)^TW(K\\alpha - z) + \\lambda\\alpha^TK\\alpha + C \\quad \\text{where} z = K\\alpha_0 - W^{-1} P y\n",
    "\\end{align*}\n",
    "\n",
    "The WKRR problem is presented as:\n",
    "$$\n",
    "\\text{argmin}_{\\alpha \\in \\mathbb R^n} \\frac{1}{n}(K\\alpha - y)^TW(K\\alpha - y) + \\lambda \\alpha^TK\\alpha\n",
    "$$\n",
    "and has as solution:\n",
    "$$\n",
    "\\alpha = W^{1/2} (W^{1/2}KW^{1/2}+n\\lambda I)^{-1} W^{1/2}y\n",
    "$$\n",
    "\n",
    "So, in order to solve KRL, we use IRLS on a WKRR problem until convergence:\n",
    "$$\\alpha^{t+1} \\gets \\text{solveWKRR}(K, W^t, z^t)$$\n",
    "With the updates for $W^t$ and $z^t$ from $\\alpha^t$ are:\n",
    "- $m_i \\gets [K\\alpha^t]_i$\n",
    "- $P_i^t \\gets -\\sigma(-y_im_i)$\n",
    "- $W_i^t \\gets \\sigma(m_i)\\sigma(-m_i)$\n",
    "- $z_i^t \\gets m_i + y_i / \\sigma(-y_im_i)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "vocational-logistics",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "def logistic_loss(y_true, y_pred):\n",
    "    n = y_true.shape[0]\n",
    "    log_term = np.log(sigmoid(y_true*y_pred))\n",
    "    return(-np.sum(log_term)/n)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "marine-trinidad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def KLR(K, y, Kval, yval, lambd, maxIter = 100, tresh = 1e-8):\n",
    "    \n",
    "    # initialize the values\n",
    "    assert K.shape[0] == y.shape[0]\n",
    "    n = K.shape[0]\n",
    "    \n",
    "    y_ = np.ones(n)\n",
    "    yval_ = np.ones(n)\n",
    "    \n",
    "    y_[y == 0] = -1\n",
    "    yval_[yval == 0] = -1\n",
    "    \n",
    "    \n",
    "    loss = []\n",
    "    acc = []\n",
    "    \n",
    "    loss_val = []\n",
    "    acc_val = []\n",
    "    \n",
    "    \n",
    "    alphas = []\n",
    "    \n",
    "    for l in lambd :\n",
    "        cnt = 0\n",
    "        \n",
    "        P_t, W_t = np.eye(n), np.eye(n)\n",
    "        z_t = K@ np.ones(n) - y_\n",
    "        alpha_t = np.ones(n)\n",
    "        diff_alpha = np.inf\n",
    "\n",
    "\n",
    "        while (diff_alpha > tresh) and (cnt < maxIter):\n",
    "\n",
    "            old_alpha = alpha_t\n",
    "            alpha_t = solveWKRR(K, W_t, z_t, y_, l)\n",
    "\n",
    "            m_t = K@alpha_t\n",
    "            sigma_m = sigmoid(m_t)\n",
    "            sigma_my = sigmoid(-y_*m_t)\n",
    "\n",
    "            P_t = - np.diag(sigma_my)\n",
    "            W_t = np.diag(sigma_m * (1-sigma_m))\n",
    "\n",
    "            z_t = m_t - (P_t@y_)/(sigma_m * (1-sigma_m))\n",
    "\n",
    "            diff_alpha = np.linalg.norm(alpha_t - old_alpha)\n",
    "            cnt+=1\n",
    "            if cnt % 10 == 0:\n",
    "                print(l, cnt)\n",
    "        \n",
    "        loss_lambda = logistic_loss(y_, K@alpha_t)\n",
    "        acc_lambda = accuracy(y,K@alpha_t, mode=\"SVM\")\n",
    "        \n",
    "        loss_lambdaval = logistic_loss(yval_, Kval@alpha_t)\n",
    "        acc_lambdaval = accuracy(yval,Kval@alpha_t, mode=\"SVM\")\n",
    "\n",
    "        \n",
    "        print(f\"***********lambda = {l}***********\")\n",
    "        print(f\"Training: loss = {loss_lambda:.4f}, accuracy = {acc_lambda:.6f}\")\n",
    "        print(f\"Validation: loss = {loss_lambdaval:.4f}, accuracy = {acc_lambdaval:.6f}\")\n",
    "        \n",
    "        \n",
    "        loss += [loss_lambda]\n",
    "        acc += [acc_lambda]\n",
    "        \n",
    "        loss_val += [loss_lambdaval]\n",
    "        acc_val += [acc_lambdaval]\n",
    "        \n",
    "        alphas +=[alpha_t]\n",
    "        \n",
    "    return(alphas, loss, acc, loss_val, acc_val)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "light-recall",
   "metadata": {},
   "source": [
    "## Support Vector Machine approach (SVM)\n",
    "\n",
    "- Binary Classificaiton setup: $\\mathcal Y = \\{-1, 1\\}$\n",
    "- $\\mathcal l_{\\text{hinge}}(f(x),y) = \\max(1- y f(x), 0)$\n",
    "\n",
    "Objective:\n",
    "\\begin{align*}\n",
    "\\hat f &= \\text{argmin}_{f\\in \\mathcal H} \\frac{1}{n} \\sum_{i=1}^n \\max(1- y_i f(x_i), 0) + \\lambda||f||^2_{\\mathcal H}\\\\\n",
    "\\alpha &= \\text{argmin}_{\\alpha \\in \\mathbb R^n} \\frac{1}{n} \\sum_{i=1}^n \\max(y_i[K\\alpha]_i, 0) + \\lambda \\alpha^T K \\alpha\n",
    "\\end{align*}\n",
    "\n",
    "It is a convex optimization problem but the objective is not smooth.\n",
    "\n",
    "By introducing additional slack variables $\\xi_i$, the problem's objective becomes smooth but it is not the case for the constraints anymore. Let us solve the dual formulation instead (which is sparse, leading to faster algorithms). \n",
    "\n",
    "The dual can be rewritten as a quadratic minimization under box constraints : \n",
    "\n",
    "\\begin{align*}\n",
    "\\min_{\\alpha \\in \\mathbb R^n} \\frac{1}{2} \\alpha^TK\\alpha - \\alpha^T y\\\\\n",
    "\\text{s.t. }  0\\leq y_i\\alpha_i\\leq \\frac{1}{2\\lambda n}\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "We will solve it using CVXOpt tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "defensive-harbor",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hinge_loss(y_true, y_pred):\n",
    "    n = y_true.shape[0]\n",
    "    term = np.maximum(1-y_true*y_pred, 0)\n",
    "    return(np.sum(term)/n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "representative-revolution",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Je pense qu'une plus belle façon de faire serait de créer des fonctions de \n",
    "## kernel(X, sigma) et de les appeler avec en paramètres (X_train ou X_val) selon si on \n",
    "## fait le training ou la validation, pour pas avoir à garder en mémoire les kernels train/val\n",
    "## comme on le fait jusqu'à maintenant\n",
    "## Mais bon, là il est 2h47 du matin, j'ai un peu la flemme et j'imagine que toi aussi,\n",
    "## ça marche déjà bien comme ça ^^\n",
    "\n",
    "def _gaussian_kernel(sigma=1):\n",
    "    \"\"\"\n",
    "    Prepares a Gaussian RBF kernel using the provided sigma.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    kernel_function: A callable to the Gaussian RBF kernel function.\n",
    "\n",
    "    \"\"\"\n",
    "    gamma = -1 / (2 * sigma ** 2)\n",
    "    kernel_function = lambda X, y: np.exp(gamma * np.square(X[:, np.newaxis] - y).sum(axis=2))\n",
    "    return kernel_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "norman-firewall",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cvxopt import matrix, solvers\n",
    "\n",
    "def SVM(K, y, K_val, y_val, lambd):\n",
    "    # takes y with values in 0, 1 which need to be turnt into -1,1\n",
    "    # initialize the values\n",
    "    assert K.shape[0] == y.shape[0]\n",
    "    n = K.shape[0]\n",
    "    \n",
    "    y_ = np.ones(n)\n",
    "    yval_ = np.ones(n)\n",
    "    \n",
    "    y_[y == 0] = -1\n",
    "    yval_[y_val == 0] = -1\n",
    "    \n",
    "    y_preds, y_preds_val = [], []\n",
    "    losses, losses_val = [], []\n",
    "    accuracies, accuracies_val = [], []\n",
    "    alphas = []\n",
    "    \n",
    "    \n",
    "    for l in lambd :\n",
    "\n",
    "        ## Solving dual using CVXOpt\n",
    "        P = matrix(K)\n",
    "        q = matrix(-y_)\n",
    "        D = np.diag(-y_)\n",
    "        G = matrix(np.vstack((D,-D)))\n",
    "        h = matrix(np.concatenate((np.zeros(n), 1/(2*l*n) * np.ones(n)), axis=0))\n",
    "        solvers.options['show_progress'] = False\n",
    "        sol=solvers.qp(P, q, G, h)\n",
    "        alpha = sol['x']\n",
    "        alpha = np.reshape(alpha,-1)               \n",
    "\n",
    "        ## predictions\n",
    "        # training\n",
    "        pred_l = K @ alpha\n",
    "        y_preds += [pred_l]\n",
    "        loss_l = hinge_loss(y_, pred_l)\n",
    "        acc_l = accuracy(y, pred_l, mode=\"SVM\")\n",
    "\n",
    "        \n",
    "        # validation\n",
    "        pred_l_val = K_val@alpha\n",
    "        y_preds_val += [pred_l_val]\n",
    "        loss_l_val = hinge_loss(yval_, pred_l_val)\n",
    "        acc_l_val = accuracy(y_val,pred_l_val, mode=\"SVM\")\n",
    "        \n",
    "\n",
    "        print(15*\"-\", f\" lambda = {l} \", 15*\"-\")\n",
    "        print(f\"Training: loss = {loss_l:.6f}, accuracy = {acc_l:.6f}\")\n",
    "        print(f\"Validation: loss = {loss_l_val:.6f}, accuracy = {acc_l_val:.6f}\")\n",
    "        \n",
    "        losses += [loss_l]\n",
    "        accuracies += [acc_l]\n",
    "        \n",
    "        losses_val += [loss_l_val]\n",
    "        accuracies_val += [acc_l_val]\n",
    "    \n",
    "        alphas +=[alpha] \n",
    "        \n",
    "    return(alphas, losses, accuracies, losses_val, accuracies_val)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proud-northeast",
   "metadata": {},
   "source": [
    "# Testing the accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "excellent-module",
   "metadata": {},
   "source": [
    "## Splitting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "criminal-crime",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "Xtr0, Xval0, ytr0, yval0 = train_test_split(Xtr0_mat100, Ytr0, test_size=0.5, random_state=42)\n",
    "Xtr1, Xval1, ytr1, yval1 = train_test_split(Xtr1_mat100, Ytr1, test_size=0.5, random_state=42)\n",
    "Xtr2, Xval2, ytr2, yval2 = train_test_split(Xtr2_mat100, Ytr2, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cheap-lighter",
   "metadata": {},
   "source": [
    "## Create the kernel matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "consecutive-furniture",
   "metadata": {},
   "outputs": [],
   "source": [
    "K_tr0_ln, K_val0_ln = linear_kernel(Xtr0, Xval0)\n",
    "K_tr1_ln, K_val1_ln = linear_kernel(Xtr1, Xval1)\n",
    "K_tr2_ln, K_val2_ln = linear_kernel(Xtr2, Xval2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "convinced-warner",
   "metadata": {},
   "outputs": [],
   "source": [
    "K_tr0, K_val0 = gaussian_kernel(Xtr0, Xval0)\n",
    "K_tr1, K_val1 = gaussian_kernel(Xtr1, Xval1)\n",
    "K_tr2, K_val2 = gaussian_kernel(Xtr2, Xval2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "attended-neutral",
   "metadata": {},
   "outputs": [],
   "source": [
    "K_tr0_poly, K_val0_poly = polynomial_kernel(Xtr0, Xval0, d=3, c=1)\n",
    "K_tr1_poly, K_val1_poly = polynomial_kernel(Xtr1, Xval1, d=3, c=1)\n",
    "K_tr2_poly, K_val2_poly = polynomial_kernel(Xtr2, Xval2, d=3, c=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "violent-mortgage",
   "metadata": {},
   "source": [
    "## Testing KRR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beneficial-audit",
   "metadata": {},
   "source": [
    "### Gaussian Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "surrounded-revision",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************* KRR for dataset 0*************\n",
      "\n",
      "***********lambda = 0***********\n",
      "Training: loss = 497.5500, accuracy = 1.000000\n",
      "Validation: loss = 310.6197, accuracy = 0.570000\n",
      "***********lambda = 1e-10***********\n",
      "Training: loss = 497.5499, accuracy = 1.000000\n",
      "Validation: loss = 310.6197, accuracy = 0.570000\n",
      "***********lambda = 1e-09***********\n",
      "Training: loss = 497.5491, accuracy = 1.000000\n",
      "Validation: loss = 310.6196, accuracy = 0.570000\n",
      "***********lambda = 1e-08***********\n",
      "Training: loss = 497.5411, accuracy = 1.000000\n",
      "Validation: loss = 310.6186, accuracy = 0.570000\n",
      "***********lambda = 1e-07***********\n",
      "Training: loss = 497.4610, accuracy = 1.000000\n",
      "Validation: loss = 310.6092, accuracy = 0.570000\n",
      "***********lambda = 1e-06***********\n",
      "Training: loss = 496.6629, accuracy = 1.000000\n",
      "Validation: loss = 310.5155, accuracy = 0.570000\n",
      "***********lambda = 1e-05***********\n",
      "Training: loss = 488.9667, accuracy = 1.000000\n",
      "Validation: loss = 309.6093, accuracy = 0.573000\n",
      "***********lambda = 0.0001***********\n",
      "Training: loss = 432.4610, accuracy = 1.000000\n",
      "Validation: loss = 302.7290, accuracy = 0.581000\n",
      "***********lambda = 0.001***********\n",
      "Training: loss = 306.7846, accuracy = 0.977000\n",
      "Validation: loss = 282.1138, accuracy = 0.584000\n",
      "***********lambda = 0.01***********\n",
      "Training: loss = 263.4305, accuracy = 0.687000\n",
      "Validation: loss = 267.3457, accuracy = 0.551000\n",
      "***********lambda = 0.1***********\n",
      "Training: loss = 288.7743, accuracy = 0.535000\n",
      "Validation: loss = 303.0325, accuracy = 0.503000\n",
      "***********lambda = 1***********\n",
      "Training: loss = 411.5986, accuracy = 0.535000\n",
      "Validation: loss = 439.5511, accuracy = 0.503000\n",
      "***********lambda = 10***********\n",
      "Training: loss = 458.4916, accuracy = 0.535000\n",
      "Validation: loss = 490.0275, accuracy = 0.503000\n",
      "************* KRR for dataset 1*************\n",
      "\n",
      "***********lambda = 0***********\n",
      "Training: loss = 499.8380, accuracy = 1.000000\n",
      "Validation: loss = 304.1439, accuracy = 0.577000\n",
      "***********lambda = 1e-10***********\n",
      "Training: loss = 499.8379, accuracy = 1.000000\n",
      "Validation: loss = 304.1438, accuracy = 0.577000\n",
      "***********lambda = 1e-09***********\n",
      "Training: loss = 499.8370, accuracy = 1.000000\n",
      "Validation: loss = 304.1437, accuracy = 0.577000\n",
      "***********lambda = 1e-08***********\n",
      "Training: loss = 499.8283, accuracy = 1.000000\n",
      "Validation: loss = 304.1427, accuracy = 0.577000\n",
      "***********lambda = 1e-07***********\n",
      "Training: loss = 499.7408, accuracy = 1.000000\n",
      "Validation: loss = 304.1326, accuracy = 0.577000\n",
      "***********lambda = 1e-06***********\n",
      "Training: loss = 498.8694, accuracy = 1.000000\n",
      "Validation: loss = 304.0313, accuracy = 0.577000\n",
      "***********lambda = 1e-05***********\n",
      "Training: loss = 490.4755, accuracy = 1.000000\n",
      "Validation: loss = 303.0527, accuracy = 0.578000\n",
      "***********lambda = 0.0001***********\n",
      "Training: loss = 429.6232, accuracy = 1.000000\n",
      "Validation: loss = 295.7302, accuracy = 0.586000\n",
      "***********lambda = 0.001***********\n",
      "Training: loss = 301.8628, accuracy = 0.968000\n",
      "Validation: loss = 276.2306, accuracy = 0.578000\n",
      "***********lambda = 0.01***********\n",
      "Training: loss = 264.4813, accuracy = 0.660000\n",
      "Validation: loss = 264.9076, accuracy = 0.555000\n",
      "***********lambda = 0.1***********\n",
      "Training: loss = 292.6725, accuracy = 0.509000\n",
      "Validation: loss = 300.3593, accuracy = 0.490000\n",
      "***********lambda = 1***********\n",
      "Training: loss = 429.3003, accuracy = 0.509000\n",
      "Validation: loss = 445.9723, accuracy = 0.490000\n",
      "***********lambda = 10***********\n",
      "Training: loss = 483.4250, accuracy = 0.509000\n",
      "Validation: loss = 502.1660, accuracy = 0.490000\n",
      "************* KRR for dataset 2*************\n",
      "\n",
      "***********lambda = 0***********\n",
      "Training: loss = 7082.5980, accuracy = 0.616000\n",
      "Validation: loss = 6290.9359, accuracy = 0.552000\n",
      "***********lambda = 1e-10***********\n",
      "Training: loss = 499.9979, accuracy = 1.000000\n",
      "Validation: loss = 345.2061, accuracy = 0.677000\n",
      "***********lambda = 1e-09***********\n",
      "Training: loss = 499.9972, accuracy = 1.000000\n",
      "Validation: loss = 345.2060, accuracy = 0.677000\n",
      "***********lambda = 1e-08***********\n",
      "Training: loss = 499.9904, accuracy = 1.000000\n",
      "Validation: loss = 345.2051, accuracy = 0.677000\n",
      "***********lambda = 1e-07***********\n",
      "Training: loss = 499.9218, accuracy = 1.000000\n",
      "Validation: loss = 345.1958, accuracy = 0.677000\n",
      "***********lambda = 1e-06***********\n",
      "Training: loss = 499.2381, accuracy = 1.000000\n",
      "Validation: loss = 345.1028, accuracy = 0.677000\n",
      "***********lambda = 1e-05***********\n",
      "Training: loss = 492.6360, accuracy = 1.000000\n",
      "Validation: loss = 344.1805, accuracy = 0.679000\n",
      "***********lambda = 0.0001***********\n",
      "Training: loss = 443.6214, accuracy = 1.000000\n",
      "Validation: loss = 336.8586, accuracy = 0.680000\n",
      "***********lambda = 0.001***********\n",
      "Training: loss = 330.2627, accuracy = 0.969000\n",
      "Validation: loss = 312.3465, accuracy = 0.687000\n",
      "***********lambda = 0.01***********\n",
      "Training: loss = 275.0285, accuracy = 0.731000\n",
      "Validation: loss = 275.9335, accuracy = 0.652000\n",
      "***********lambda = 0.1***********\n",
      "Training: loss = 297.7531, accuracy = 0.499000\n",
      "Validation: loss = 295.8680, accuracy = 0.504000\n",
      "***********lambda = 1***********\n",
      "Training: loss = 440.7353, accuracy = 0.499000\n",
      "Validation: loss = 436.2669, accuracy = 0.504000\n",
      "***********lambda = 10***********\n",
      "Training: loss = 493.7018, accuracy = 0.499000\n",
      "Validation: loss = 488.7577, accuracy = 0.504000\n"
     ]
    }
   ],
   "source": [
    "lambdas = [0] + [10**i for i in range(-10,2)]\n",
    "print(\"************* KRR for dataset 0*************\\n\")\n",
    "alphas_tr0, loss_tr0, acc_0, loss_val0, acc_val0 = KRR(K_tr0, ytr0[:,1], K_val0, yval0[:,1], lambdas)\n",
    "print(\"************* KRR for dataset 1*************\\n\")\n",
    "alphas_tr1, loss_tr1, acc_1, loss_val1, acc_val1 = KRR(K_tr1, ytr1[:,1], K_val1, yval1[:,1],lambdas)\n",
    "print(\"************* KRR for dataset 2*************\\n\")\n",
    "alphas_tr2, loss_tr2, acc_2, loss_val2, acc_val2 = KRR(K_tr2, ytr2[:,1], K_val2, yval2[:,1],lambdas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "composite-links",
   "metadata": {},
   "source": [
    "### Linear Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "electrical-electricity",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************* KRR for dataset 0*************\n",
      "\n",
      "***********lambda = 0***********\n",
      "Training: loss = 2797814.3402, accuracy = 0.479000\n",
      "Validation: loss = 3080449.7376, accuracy = 0.501000\n",
      "***********lambda = 1e-10***********\n",
      "Training: loss = 507.2335, accuracy = 0.545000\n",
      "Validation: loss = 535.5736, accuracy = 0.506000\n",
      "***********lambda = 1e-09***********\n",
      "Training: loss = 507.2331, accuracy = 0.545000\n",
      "Validation: loss = 535.5728, accuracy = 0.506000\n",
      "***********lambda = 1e-08***********\n",
      "Training: loss = 507.2330, accuracy = 0.545000\n",
      "Validation: loss = 535.5728, accuracy = 0.506000\n",
      "***********lambda = 1e-07***********\n",
      "Training: loss = 507.2330, accuracy = 0.545000\n",
      "Validation: loss = 535.5728, accuracy = 0.506000\n",
      "***********lambda = 1e-06***********\n",
      "Training: loss = 507.2329, accuracy = 0.545000\n",
      "Validation: loss = 535.5726, accuracy = 0.506000\n",
      "***********lambda = 1e-05***********\n",
      "Training: loss = 507.2317, accuracy = 0.545000\n",
      "Validation: loss = 535.5713, accuracy = 0.506000\n",
      "***********lambda = 0.0001***********\n",
      "Training: loss = 507.2200, accuracy = 0.545000\n",
      "Validation: loss = 535.5583, accuracy = 0.506000\n",
      "***********lambda = 0.001***********\n",
      "Training: loss = 507.1032, accuracy = 0.545000\n",
      "Validation: loss = 535.4287, accuracy = 0.506000\n",
      "***********lambda = 0.01***********\n",
      "Training: loss = 505.9799, accuracy = 0.545000\n",
      "Validation: loss = 534.1858, accuracy = 0.507000\n",
      "***********lambda = 0.1***********\n",
      "Training: loss = 497.7545, accuracy = 0.540000\n",
      "Validation: loss = 525.2868, accuracy = 0.507000\n",
      "***********lambda = 1***********\n",
      "Training: loss = 476.5716, accuracy = 0.535000\n",
      "Validation: loss = 504.8174, accuracy = 0.503000\n",
      "***********lambda = 10***********\n",
      "Training: loss = 465.9272, accuracy = 0.535000\n",
      "Validation: loss = 497.1148, accuracy = 0.503000\n",
      "************* KRR for dataset 1*************\n",
      "\n",
      "***********lambda = 0***********\n",
      "Training: loss = 2941631.4404, accuracy = 0.505000\n",
      "Validation: loss = 3008915.9427, accuracy = 0.479000\n",
      "***********lambda = 1e-10***********\n",
      "Training: loss = 526.8740, accuracy = 0.512000\n",
      "Validation: loss = 535.3450, accuracy = 0.492000\n",
      "***********lambda = 1e-09***********\n",
      "Training: loss = 526.8745, accuracy = 0.512000\n",
      "Validation: loss = 535.3458, accuracy = 0.492000\n",
      "***********lambda = 1e-08***********\n",
      "Training: loss = 526.8745, accuracy = 0.512000\n",
      "Validation: loss = 535.3459, accuracy = 0.492000\n",
      "***********lambda = 1e-07***********\n",
      "Training: loss = 526.8745, accuracy = 0.512000\n",
      "Validation: loss = 535.3459, accuracy = 0.492000\n",
      "***********lambda = 1e-06***********\n",
      "Training: loss = 526.8745, accuracy = 0.512000\n",
      "Validation: loss = 535.3458, accuracy = 0.492000\n",
      "***********lambda = 1e-05***********\n",
      "Training: loss = 526.8735, accuracy = 0.512000\n",
      "Validation: loss = 535.3449, accuracy = 0.492000\n",
      "***********lambda = 0.0001***********\n",
      "Training: loss = 526.8639, accuracy = 0.512000\n",
      "Validation: loss = 535.3357, accuracy = 0.492000\n",
      "***********lambda = 0.001***********\n",
      "Training: loss = 526.7689, accuracy = 0.512000\n",
      "Validation: loss = 535.2442, accuracy = 0.492000\n",
      "***********lambda = 0.01***********\n",
      "Training: loss = 525.8577, accuracy = 0.512000\n",
      "Validation: loss = 534.3708, accuracy = 0.492000\n",
      "***********lambda = 0.1***********\n",
      "Training: loss = 519.2127, accuracy = 0.510000\n",
      "Validation: loss = 528.2059, accuracy = 0.492000\n",
      "***********lambda = 1***********\n",
      "Training: loss = 500.8054, accuracy = 0.509000\n",
      "Validation: loss = 513.8432, accuracy = 0.490000\n",
      "***********lambda = 10***********\n",
      "Training: loss = 491.6785, accuracy = 0.509000\n",
      "Validation: loss = 509.7207, accuracy = 0.490000\n",
      "************* KRR for dataset 2*************\n",
      "\n",
      "***********lambda = 0***********\n",
      "Training: loss = 980304.3085, accuracy = 0.488000\n",
      "Validation: loss = 1126628.2760, accuracy = 0.495000\n",
      "***********lambda = 1e-10***********\n",
      "Training: loss = 574.0246, accuracy = 0.530000\n",
      "Validation: loss = 582.1497, accuracy = 0.532000\n",
      "***********lambda = 1e-09***********\n",
      "Training: loss = 574.0218, accuracy = 0.530000\n",
      "Validation: loss = 582.1467, accuracy = 0.532000\n",
      "***********lambda = 1e-08***********\n",
      "Training: loss = 574.0215, accuracy = 0.530000\n",
      "Validation: loss = 582.1464, accuracy = 0.532000\n",
      "***********lambda = 1e-07***********\n",
      "Training: loss = 574.0214, accuracy = 0.530000\n",
      "Validation: loss = 582.1463, accuracy = 0.532000\n",
      "***********lambda = 1e-06***********\n",
      "Training: loss = 574.0213, accuracy = 0.530000\n",
      "Validation: loss = 582.1462, accuracy = 0.532000\n",
      "***********lambda = 1e-05***********\n",
      "Training: loss = 574.0203, accuracy = 0.530000\n",
      "Validation: loss = 582.1452, accuracy = 0.532000\n",
      "***********lambda = 0.0001***********\n",
      "Training: loss = 574.0102, accuracy = 0.530000\n",
      "Validation: loss = 582.1349, accuracy = 0.532000\n",
      "***********lambda = 0.001***********\n",
      "Training: loss = 573.9095, accuracy = 0.530000\n",
      "Validation: loss = 582.0327, accuracy = 0.532000\n",
      "***********lambda = 0.01***********\n",
      "Training: loss = 572.9275, accuracy = 0.528000\n",
      "Validation: loss = 581.0347, accuracy = 0.532000\n",
      "***********lambda = 0.1***********\n",
      "Training: loss = 564.9947, accuracy = 0.518000\n",
      "Validation: loss = 572.8931, accuracy = 0.524000\n",
      "***********lambda = 1***********\n",
      "Training: loss = 534.9552, accuracy = 0.503000\n",
      "Validation: loss = 541.3932, accuracy = 0.505000\n",
      "***********lambda = 10***********\n",
      "Training: loss = 504.9915, accuracy = 0.499000\n",
      "Validation: loss = 504.5550, accuracy = 0.504000\n"
     ]
    }
   ],
   "source": [
    "lambdas = [0] + [10**i for i in range(-10,2)]\n",
    "print(\"************* KRR for dataset 0*************\\n\")\n",
    "alphas_tr0, loss_tr0, acc_0, loss_val0, acc_val0 = KRR(K_tr0_ln, ytr0[:,1], K_val0_ln, yval0[:,1], lambdas)\n",
    "print(\"************* KRR for dataset 1*************\\n\")\n",
    "alphas_tr1, loss_tr1, acc_1, loss_val1, acc_val1 = KRR(K_tr1_ln, ytr1[:,1], K_val1_ln, yval1[:,1],lambdas)\n",
    "print(\"************* KRR for dataset 2*************\\n\")\n",
    "alphas_tr2, loss_tr2, acc_2, loss_val2, acc_val2 = KRR(K_tr2_ln, ytr2[:,1], K_val2_ln, yval2[:,1],lambdas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spectacular-buyer",
   "metadata": {},
   "source": [
    "### Polynomial Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "assured-valuation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************* KRR for dataset 0*************\n",
      "\n",
      "***********lambda = 0***********\n",
      "Training: loss = 497.5500, accuracy = 1.000000\n",
      "Validation: loss = 406.5435, accuracy = 0.500000\n",
      "***********lambda = 1e-10***********\n",
      "Training: loss = 497.5500, accuracy = 1.000000\n",
      "Validation: loss = 406.5435, accuracy = 0.500000\n",
      "***********lambda = 1e-09***********\n",
      "Training: loss = 497.5500, accuracy = 1.000000\n",
      "Validation: loss = 406.5435, accuracy = 0.500000\n",
      "***********lambda = 1e-08***********\n",
      "Training: loss = 497.5500, accuracy = 1.000000\n",
      "Validation: loss = 406.5435, accuracy = 0.500000\n",
      "***********lambda = 1e-07***********\n",
      "Training: loss = 497.5500, accuracy = 1.000000\n",
      "Validation: loss = 406.5435, accuracy = 0.500000\n",
      "***********lambda = 1e-06***********\n",
      "Training: loss = 497.5500, accuracy = 1.000000\n",
      "Validation: loss = 406.5435, accuracy = 0.500000\n",
      "***********lambda = 1e-05***********\n",
      "Training: loss = 497.5500, accuracy = 1.000000\n",
      "Validation: loss = 406.5435, accuracy = 0.500000\n",
      "***********lambda = 0.0001***********\n",
      "Training: loss = 497.5499, accuracy = 1.000000\n",
      "Validation: loss = 406.5435, accuracy = 0.500000\n",
      "***********lambda = 0.001***********\n",
      "Training: loss = 497.5493, accuracy = 1.000000\n",
      "Validation: loss = 406.5434, accuracy = 0.500000\n",
      "***********lambda = 0.01***********\n",
      "Training: loss = 497.5427, accuracy = 1.000000\n",
      "Validation: loss = 406.5426, accuracy = 0.500000\n",
      "***********lambda = 0.1***********\n",
      "Training: loss = 497.4768, accuracy = 1.000000\n",
      "Validation: loss = 406.5345, accuracy = 0.500000\n",
      "***********lambda = 1***********\n",
      "Training: loss = 496.8207, accuracy = 1.000000\n",
      "Validation: loss = 406.4536, accuracy = 0.500000\n",
      "***********lambda = 10***********\n",
      "Training: loss = 490.4944, accuracy = 1.000000\n",
      "Validation: loss = 405.6788, accuracy = 0.503000\n",
      "************* KRR for dataset 1*************\n",
      "\n",
      "***********lambda = 0***********\n",
      "Training: loss = 499.8380, accuracy = 1.000000\n",
      "Validation: loss = 381.1586, accuracy = 0.505000\n",
      "***********lambda = 1e-10***********\n",
      "Training: loss = 499.8380, accuracy = 1.000000\n",
      "Validation: loss = 381.1586, accuracy = 0.505000\n",
      "***********lambda = 1e-09***********\n",
      "Training: loss = 499.8380, accuracy = 1.000000\n",
      "Validation: loss = 381.1586, accuracy = 0.505000\n",
      "***********lambda = 1e-08***********\n",
      "Training: loss = 499.8380, accuracy = 1.000000\n",
      "Validation: loss = 381.1586, accuracy = 0.505000\n",
      "***********lambda = 1e-07***********\n",
      "Training: loss = 499.8380, accuracy = 1.000000\n",
      "Validation: loss = 381.1586, accuracy = 0.505000\n",
      "***********lambda = 1e-06***********\n",
      "Training: loss = 499.8380, accuracy = 1.000000\n",
      "Validation: loss = 381.1586, accuracy = 0.505000\n",
      "***********lambda = 1e-05***********\n",
      "Training: loss = 499.8380, accuracy = 1.000000\n",
      "Validation: loss = 381.1586, accuracy = 0.505000\n",
      "***********lambda = 0.0001***********\n",
      "Training: loss = 499.8379, accuracy = 1.000000\n",
      "Validation: loss = 381.1586, accuracy = 0.505000\n",
      "***********lambda = 0.001***********\n",
      "Training: loss = 499.8372, accuracy = 1.000000\n",
      "Validation: loss = 381.1585, accuracy = 0.505000\n",
      "***********lambda = 0.01***********\n",
      "Training: loss = 499.8299, accuracy = 1.000000\n",
      "Validation: loss = 381.1575, accuracy = 0.505000\n",
      "***********lambda = 0.1***********\n",
      "Training: loss = 499.7573, accuracy = 1.000000\n",
      "Validation: loss = 381.1477, accuracy = 0.505000\n",
      "***********lambda = 1***********\n",
      "Training: loss = 499.0338, accuracy = 1.000000\n",
      "Validation: loss = 381.0504, accuracy = 0.506000\n",
      "***********lambda = 10***********\n",
      "Training: loss = 492.0760, accuracy = 1.000000\n",
      "Validation: loss = 380.1439, accuracy = 0.508000\n",
      "************* KRR for dataset 2*************\n",
      "\n",
      "***********lambda = 0***********\n",
      "Training: loss = 499.9980, accuracy = 1.000000\n",
      "Validation: loss = 379.3791, accuracy = 0.595000\n",
      "***********lambda = 1e-10***********\n",
      "Training: loss = 499.9980, accuracy = 1.000000\n",
      "Validation: loss = 379.3791, accuracy = 0.595000\n",
      "***********lambda = 1e-09***********\n",
      "Training: loss = 499.9980, accuracy = 1.000000\n",
      "Validation: loss = 379.3791, accuracy = 0.595000\n",
      "***********lambda = 1e-08***********\n",
      "Training: loss = 499.9980, accuracy = 1.000000\n",
      "Validation: loss = 379.3791, accuracy = 0.595000\n",
      "***********lambda = 1e-07***********\n",
      "Training: loss = 499.9980, accuracy = 1.000000\n",
      "Validation: loss = 379.3791, accuracy = 0.595000\n",
      "***********lambda = 1e-06***********\n",
      "Training: loss = 499.9980, accuracy = 1.000000\n",
      "Validation: loss = 379.3791, accuracy = 0.595000\n",
      "***********lambda = 1e-05***********\n",
      "Training: loss = 499.9980, accuracy = 1.000000\n",
      "Validation: loss = 379.3791, accuracy = 0.595000\n",
      "***********lambda = 0.0001***********\n",
      "Training: loss = 499.9979, accuracy = 1.000000\n",
      "Validation: loss = 379.3791, accuracy = 0.595000\n",
      "***********lambda = 0.001***********\n",
      "Training: loss = 499.9974, accuracy = 1.000000\n",
      "Validation: loss = 379.3790, accuracy = 0.595000\n",
      "***********lambda = 0.01***********\n",
      "Training: loss = 499.9921, accuracy = 1.000000\n",
      "Validation: loss = 379.3788, accuracy = 0.595000\n",
      "***********lambda = 0.1***********\n",
      "Training: loss = 499.9393, accuracy = 1.000000\n",
      "Validation: loss = 379.3767, accuracy = 0.595000\n",
      "***********lambda = 1***********\n",
      "Training: loss = 499.4128, accuracy = 1.000000\n",
      "Validation: loss = 379.3542, accuracy = 0.595000\n",
      "***********lambda = 10***********\n",
      "Training: loss = 494.3125, accuracy = 1.000000\n",
      "Validation: loss = 379.0681, accuracy = 0.594000\n"
     ]
    }
   ],
   "source": [
    "lambdas = [0] + [10**i for i in range(-10,2)]\n",
    "print(\"************* KRR for dataset 0*************\\n\")\n",
    "alphas_tr0, loss_tr0, acc_0, loss_val0, acc_val0 = KRR(K_tr0_poly, ytr0[:,1], K_val0_poly, yval0[:,1], lambdas)\n",
    "print(\"************* KRR for dataset 1*************\\n\")\n",
    "alphas_tr1, loss_tr1, acc_1, loss_val1, acc_val1 = KRR(K_tr1_poly, ytr1[:,1], K_val1_poly, yval1[:,1],lambdas)\n",
    "print(\"************* KRR for dataset 2*************\\n\")\n",
    "alphas_tr2, loss_tr2, acc_2, loss_val2, acc_val2 = KRR(K_tr2_poly, ytr2[:,1], K_val2_poly, yval2[:,1],lambdas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "higher-detector",
   "metadata": {},
   "source": [
    "### Conclusion : gaussian is better"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "modular-basics",
   "metadata": {},
   "source": [
    "## Testing KLR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stopped-bangkok",
   "metadata": {},
   "source": [
    "### Gaussian Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "irish-rover",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*************KLR for dataset 0*************\n",
      "\n",
      "***********lambda = 0.001***********\n",
      "Training: loss = 0.6039, accuracy = 0.891000\n",
      "Validation: loss = 0.6708, accuracy = 0.568000\n",
      "***********lambda = 0.01***********\n",
      "Training: loss = 0.6752, accuracy = 0.691000\n",
      "Validation: loss = 0.6861, accuracy = 0.543000\n",
      "***********lambda = 0.1***********\n",
      "Training: loss = 0.6906, accuracy = 0.540000\n",
      "Validation: loss = 0.6922, accuracy = 0.503000\n",
      "*************KLR for dataset 1*************\n",
      "\n",
      "***********lambda = 0.001***********\n",
      "Training: loss = 0.6160, accuracy = 0.891000\n",
      "Validation: loss = 0.6812, accuracy = 0.568000\n",
      "***********lambda = 0.01***********\n",
      "Training: loss = 0.6810, accuracy = 0.769000\n",
      "Validation: loss = 0.6907, accuracy = 0.543000\n",
      "***********lambda = 0.1***********\n",
      "Training: loss = 0.6918, accuracy = 0.718000\n",
      "Validation: loss = 0.6929, accuracy = 0.536000\n",
      "*************KLR for dataset 2*************\n",
      "\n",
      "***********lambda = 0.001***********\n",
      "Training: loss = 0.5748, accuracy = 0.866000\n",
      "Validation: loss = 0.6244, accuracy = 0.697000\n",
      "***********lambda = 0.01***********\n",
      "Training: loss = 0.6621, accuracy = 0.754000\n",
      "Validation: loss = 0.6675, accuracy = 0.682000\n",
      "***********lambda = 0.1***********\n",
      "Training: loss = 0.6890, accuracy = 0.719000\n",
      "Validation: loss = 0.6895, accuracy = 0.680000\n"
     ]
    }
   ],
   "source": [
    "lambdas = [0.001, 0.01, 0.1]\n",
    "\n",
    "print(\"*************KLR for dataset 0*************\\n\")\n",
    "alphas_tr0_klr, loss_tr0_klr, acc_0_klr, loss_val0_klr, acc_val0_klr = KLR(K_tr0, ytr0[:,1], K_val0, yval0[:,1], lambdas, tresh=1e-5)\n",
    "print(\"*************KLR for dataset 1*************\\n\")\n",
    "alphas_tr1_klr, loss_tr1_klr, acc_1_klr, loss_val1_klr, acc_val1_klr = KLR(K_tr1, ytr1[:,1], K_val1, yval1[:,1], lambdas, tresh=1e-5)\n",
    "print(\"*************KLR for dataset 2*************\\n\")\n",
    "alphas_tr2_klr, loss_tr2_klr, acc_2_klr, loss_val2_klr, acc_val2_klr = KLR(K_tr2, ytr2[:,1], K_val2, yval2[:,1], lambdas, tresh=1e-5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "palestinian-prayer",
   "metadata": {},
   "source": [
    "### Linear Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "alpine-charlotte",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*************KLR for dataset 0*************\n",
      "\n",
      "***********lambda = 0.001***********\n",
      "Training: loss = 0.6290, accuracy = 0.677000\n",
      "Validation: loss = 0.6792, accuracy = 0.560000\n",
      "***********lambda = 0.01***********\n",
      "Training: loss = 0.6316, accuracy = 0.678000\n",
      "Validation: loss = 0.6777, accuracy = 0.561000\n",
      "***********lambda = 0.1***********\n",
      "Training: loss = 0.6469, accuracy = 0.674000\n",
      "Validation: loss = 0.6747, accuracy = 0.571000\n",
      "*************KLR for dataset 1*************\n",
      "\n",
      "***********lambda = 0.001***********\n",
      "Training: loss = 0.6389, accuracy = 0.658000\n",
      "Validation: loss = 0.6862, accuracy = 0.559000\n",
      "***********lambda = 0.01***********\n",
      "Training: loss = 0.6409, accuracy = 0.663000\n",
      "Validation: loss = 0.6849, accuracy = 0.560000\n",
      "***********lambda = 0.1***********\n",
      "Training: loss = 0.6532, accuracy = 0.652000\n",
      "Validation: loss = 0.6830, accuracy = 0.559000\n",
      "*************KLR for dataset 2*************\n",
      "\n",
      "***********lambda = 0.001***********\n",
      "Training: loss = 0.5810, accuracy = 0.740000\n",
      "Validation: loss = 0.6118, accuracy = 0.687000\n",
      "***********lambda = 0.01***********\n",
      "Training: loss = 0.5836, accuracy = 0.735000\n",
      "Validation: loss = 0.6124, accuracy = 0.692000\n",
      "***********lambda = 0.1***********\n",
      "Training: loss = 0.6003, accuracy = 0.726000\n",
      "Validation: loss = 0.6187, accuracy = 0.700000\n"
     ]
    }
   ],
   "source": [
    "lambdas = [0.001, 0.01, 0.1]\n",
    "\n",
    "print(\"*************KLR for dataset 0*************\\n\")\n",
    "alphas_tr0_klr, loss_tr0_klr, acc_0_klr, loss_val0_klr, acc_val0_klr = KLR(K_tr0_ln, ytr0[:,1], K_val0_ln, yval0[:,1], lambdas, tresh=1e-5)\n",
    "print(\"*************KLR for dataset 1*************\\n\")\n",
    "alphas_tr1_klr, loss_tr1_klr, acc_1_klr, loss_val1_klr, acc_val1_klr = KLR(K_tr1_ln, ytr1[:,1], K_val1_ln, yval1[:,1], lambdas, tresh=1e-5)\n",
    "print(\"*************KLR for dataset 2*************\\n\")\n",
    "alphas_tr2_klr, loss_tr2_klr, acc_2_klr, loss_val2_klr, acc_val2_klr = KLR(K_tr2_ln, ytr2[:,1], K_val2_ln, yval2[:,1], lambdas, tresh=1e-5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alive-election",
   "metadata": {},
   "source": [
    "### Polynomial kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "german-boston",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*************KLR for dataset 0*************\n",
      "\n",
      "***********lambda = 0.001***********\n",
      "Training: loss = 0.3133, accuracy = 1.000000\n",
      "Validation: loss = 0.6800, accuracy = 0.571000\n",
      "***********lambda = 0.01***********\n",
      "Training: loss = 0.3133, accuracy = 1.000000\n",
      "Validation: loss = 0.6800, accuracy = 0.571000\n",
      "***********lambda = 0.1***********\n",
      "Training: loss = 0.3135, accuracy = 1.000000\n",
      "Validation: loss = 0.6800, accuracy = 0.571000\n",
      "*************KLR for dataset 1*************\n",
      "\n",
      "***********lambda = 0.001***********\n",
      "Training: loss = 0.3133, accuracy = 1.000000\n",
      "Validation: loss = 0.6816, accuracy = 0.556000\n",
      "***********lambda = 0.01***********\n",
      "Training: loss = 0.3133, accuracy = 1.000000\n",
      "Validation: loss = 0.6816, accuracy = 0.556000\n",
      "***********lambda = 0.1***********\n",
      "Training: loss = 0.3135, accuracy = 1.000000\n",
      "Validation: loss = 0.6816, accuracy = 0.556000\n",
      "*************KLR for dataset 2*************\n",
      "\n",
      "***********lambda = 0.001***********\n",
      "Training: loss = 0.3133, accuracy = 1.000000\n",
      "Validation: loss = 0.6073, accuracy = 0.701000\n",
      "***********lambda = 0.01***********\n",
      "Training: loss = 0.3133, accuracy = 1.000000\n",
      "Validation: loss = 0.6073, accuracy = 0.701000\n",
      "***********lambda = 0.1***********\n",
      "Training: loss = 0.3134, accuracy = 1.000000\n",
      "Validation: loss = 0.6073, accuracy = 0.701000\n"
     ]
    }
   ],
   "source": [
    "lambdas = [0.001, 0.01, 0.1]\n",
    "\n",
    "print(\"*************KLR for dataset 0*************\\n\")\n",
    "alphas_tr0_klr, loss_tr0_klr, acc_0_klr, loss_val0_klr, acc_val0_klr = KLR(K_tr0_poly, ytr0[:,1], K_val0_poly, yval0[:,1], lambdas, tresh=1e-5)\n",
    "print(\"*************KLR for dataset 1*************\\n\")\n",
    "alphas_tr1_klr, loss_tr1_klr, acc_1_klr, loss_val1_klr, acc_val1_klr = KLR(K_tr1_poly, ytr1[:,1], K_val1_poly, yval1[:,1], lambdas, tresh=1e-5)\n",
    "print(\"*************KLR for dataset 2*************\\n\")\n",
    "alphas_tr2_klr, loss_tr2_klr, acc_2_klr, loss_val2_klr, acc_val2_klr = KLR(K_tr2_poly, ytr2[:,1], K_val2_poly, yval2[:,1], lambdas, tresh=1e-5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "involved-momentum",
   "metadata": {},
   "source": [
    "## Testing SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exempt-twenty",
   "metadata": {},
   "source": [
    "### Gaussian Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "super-offset",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************* SVM for dataset 0 *************\n",
      "\n",
      "---------------  lambda = 1e-10  ---------------\n",
      "Training: loss = 0.000000, accuracy = 1.000000\n",
      "Validation: loss = 0.907278, accuracy = 0.558000\n",
      "---------------  lambda = 1e-09  ---------------\n",
      "Training: loss = 0.000000, accuracy = 1.000000\n",
      "Validation: loss = 0.907282, accuracy = 0.558000\n",
      "---------------  lambda = 1e-08  ---------------\n",
      "Training: loss = 0.000000, accuracy = 1.000000\n",
      "Validation: loss = 0.907276, accuracy = 0.558000\n",
      "---------------  lambda = 1e-07  ---------------\n",
      "Training: loss = 0.000000, accuracy = 1.000000\n",
      "Validation: loss = 0.907280, accuracy = 0.558000\n",
      "---------------  lambda = 1e-06  ---------------\n",
      "Training: loss = 0.000000, accuracy = 1.000000\n",
      "Validation: loss = 0.907283, accuracy = 0.558000\n",
      "---------------  lambda = 1e-05  ---------------\n",
      "Training: loss = 0.000000, accuracy = 1.000000\n",
      "Validation: loss = 0.907277, accuracy = 0.558000\n",
      "---------------  lambda = 0.0001  ---------------\n",
      "Training: loss = 0.002915, accuracy = 1.000000\n",
      "Validation: loss = 0.904441, accuracy = 0.557000\n",
      "---------------  lambda = 0.001  ---------------\n",
      "Training: loss = 0.553244, accuracy = 0.841000\n",
      "Validation: loss = 0.882355, accuracy = 0.575000\n",
      "---------------  lambda = 0.01  ---------------\n",
      "Training: loss = 0.892794, accuracy = 0.536000\n",
      "Validation: loss = 0.961101, accuracy = 0.503000\n",
      "---------------  lambda = 0.1  ---------------\n",
      "Training: loss = 0.988204, accuracy = 0.535000\n",
      "Validation: loss = 0.995651, accuracy = 0.503000\n",
      "\n",
      "\n",
      "************* SVM for dataset 1 *************\n",
      "\n",
      "---------------  lambda = 1e-10  ---------------\n",
      "Training: loss = 0.000000, accuracy = 1.000000\n",
      "Validation: loss = 0.914379, accuracy = 0.581000\n",
      "---------------  lambda = 1e-09  ---------------\n",
      "Training: loss = 0.000000, accuracy = 1.000000\n",
      "Validation: loss = 0.914382, accuracy = 0.581000\n",
      "---------------  lambda = 1e-08  ---------------\n",
      "Training: loss = 0.000000, accuracy = 1.000000\n",
      "Validation: loss = 0.914383, accuracy = 0.581000\n",
      "---------------  lambda = 1e-07  ---------------\n",
      "Training: loss = 0.000000, accuracy = 1.000000\n",
      "Validation: loss = 0.914381, accuracy = 0.581000\n",
      "---------------  lambda = 1e-06  ---------------\n",
      "Training: loss = 0.000000, accuracy = 1.000000\n",
      "Validation: loss = 0.914382, accuracy = 0.581000\n",
      "---------------  lambda = 1e-05  ---------------\n",
      "Training: loss = 0.000000, accuracy = 1.000000\n",
      "Validation: loss = 0.914383, accuracy = 0.581000\n",
      "---------------  lambda = 0.0001  ---------------\n",
      "Training: loss = 0.001938, accuracy = 1.000000\n",
      "Validation: loss = 0.913228, accuracy = 0.580000\n",
      "---------------  lambda = 0.001  ---------------\n",
      "Training: loss = 0.588076, accuracy = 0.824000\n",
      "Validation: loss = 0.919535, accuracy = 0.568000\n",
      "---------------  lambda = 0.01  ---------------\n",
      "Training: loss = 0.945238, accuracy = 0.709000\n",
      "Validation: loss = 0.988686, accuracy = 0.533000\n",
      "---------------  lambda = 0.1  ---------------\n",
      "Training: loss = 0.994524, accuracy = 0.709000\n",
      "Validation: loss = 0.998869, accuracy = 0.533000\n",
      "\n",
      "\n",
      "************* SVM for dataset 2 *************\n",
      "\n",
      "---------------  lambda = 1e-10  ---------------\n",
      "Training: loss = 0.000000, accuracy = 1.000000\n",
      "Validation: loss = 0.730171, accuracy = 0.684000\n",
      "---------------  lambda = 1e-09  ---------------\n",
      "Training: loss = 0.000000, accuracy = 1.000000\n",
      "Validation: loss = 0.730165, accuracy = 0.684000\n",
      "---------------  lambda = 1e-08  ---------------\n",
      "Training: loss = 0.000000, accuracy = 1.000000\n",
      "Validation: loss = 0.730170, accuracy = 0.684000\n",
      "---------------  lambda = 1e-07  ---------------\n",
      "Training: loss = 0.000000, accuracy = 1.000000\n",
      "Validation: loss = 0.730171, accuracy = 0.684000\n",
      "---------------  lambda = 1e-06  ---------------\n",
      "Training: loss = 0.000000, accuracy = 1.000000\n",
      "Validation: loss = 0.730169, accuracy = 0.684000\n",
      "---------------  lambda = 1e-05  ---------------\n",
      "Training: loss = 0.000000, accuracy = 1.000000\n",
      "Validation: loss = 0.730170, accuracy = 0.684000\n",
      "---------------  lambda = 0.0001  ---------------\n",
      "Training: loss = 0.001044, accuracy = 1.000000\n",
      "Validation: loss = 0.728830, accuracy = 0.685000\n",
      "---------------  lambda = 0.001  ---------------\n",
      "Training: loss = 0.458995, accuracy = 0.843000\n",
      "Validation: loss = 0.704161, accuracy = 0.695000\n",
      "---------------  lambda = 0.01  ---------------\n",
      "Training: loss = 0.834039, accuracy = 0.724000\n",
      "Validation: loss = 0.853208, accuracy = 0.682000\n",
      "---------------  lambda = 0.1  ---------------\n",
      "Training: loss = 0.982950, accuracy = 0.714000\n",
      "Validation: loss = 0.984789, accuracy = 0.664000\n"
     ]
    }
   ],
   "source": [
    "lambdas = [10**i for i in range(-10, 0)]\n",
    "print(\"************* SVM for dataset 0 *************\\n\")\n",
    "alphas_tr0, loss_tr0, acc_0, loss_val0, acc_val0 = SVM(K_tr0, ytr0[:,1], K_val0, yval0[:,1], lambdas)\n",
    "\n",
    "print(\"\")\n",
    "print(\"\")\n",
    "print(\"************* SVM for dataset 1 *************\\n\")\n",
    "alphas_tr1, loss_tr1, acc_1, loss_val1, acc_val1 = SVM(K_tr1, ytr1[:,1], K_val1, yval1[:,1],lambdas)\n",
    "\n",
    "print(\"\")\n",
    "print(\"\")\n",
    "print(\"************* SVM for dataset 2 *************\\n\")\n",
    "alphas_tr2, loss_tr2, acc_2, loss_val2, acc_val2 = SVM(K_tr2, ytr2[:,1], K_val2, yval2[:,1],lambdas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "headed-diary",
   "metadata": {},
   "source": [
    "### Linear Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "static-technician",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************* SVM for dataset 0 *************\n",
      "\n",
      "---------------  lambda = 1e-10  ---------------\n",
      "Training: loss = 0.716006, accuracy = 0.691000\n",
      "Validation: loss = 0.944198, accuracy = 0.567000\n",
      "---------------  lambda = 1e-09  ---------------\n",
      "Training: loss = 0.716006, accuracy = 0.691000\n",
      "Validation: loss = 0.944198, accuracy = 0.567000\n",
      "---------------  lambda = 1e-08  ---------------\n",
      "Training: loss = 0.716006, accuracy = 0.691000\n",
      "Validation: loss = 0.944198, accuracy = 0.567000\n",
      "---------------  lambda = 1e-07  ---------------\n",
      "Training: loss = 0.716006, accuracy = 0.691000\n",
      "Validation: loss = 0.944198, accuracy = 0.567000\n",
      "---------------  lambda = 1e-06  ---------------\n",
      "Training: loss = 0.716006, accuracy = 0.691000\n",
      "Validation: loss = 0.944198, accuracy = 0.567000\n",
      "---------------  lambda = 1e-05  ---------------\n",
      "Training: loss = 0.716006, accuracy = 0.691000\n",
      "Validation: loss = 0.944198, accuracy = 0.567000\n",
      "---------------  lambda = 0.0001  ---------------\n",
      "Training: loss = 0.716007, accuracy = 0.687000\n",
      "Validation: loss = 0.944387, accuracy = 0.568000\n",
      "---------------  lambda = 0.001  ---------------\n",
      "Training: loss = 0.716057, accuracy = 0.686000\n",
      "Validation: loss = 0.942738, accuracy = 0.567000\n",
      "---------------  lambda = 0.01  ---------------\n",
      "Training: loss = 0.718240, accuracy = 0.684000\n",
      "Validation: loss = 0.931588, accuracy = 0.567000\n",
      "---------------  lambda = 0.1  ---------------\n",
      "Training: loss = 0.756568, accuracy = 0.670000\n",
      "Validation: loss = 0.885010, accuracy = 0.569000\n",
      "\n",
      "\n",
      "************* SVM for dataset 1 *************\n",
      "\n",
      "---------------  lambda = 1e-10  ---------------\n",
      "Training: loss = 0.747396, accuracy = 0.680000\n",
      "Validation: loss = 0.978483, accuracy = 0.553000\n",
      "---------------  lambda = 1e-09  ---------------\n",
      "Training: loss = 0.747396, accuracy = 0.680000\n",
      "Validation: loss = 0.978483, accuracy = 0.553000\n",
      "---------------  lambda = 1e-08  ---------------\n",
      "Training: loss = 0.747396, accuracy = 0.680000\n",
      "Validation: loss = 0.978486, accuracy = 0.553000\n",
      "---------------  lambda = 1e-07  ---------------\n",
      "Training: loss = 0.747396, accuracy = 0.680000\n",
      "Validation: loss = 0.978481, accuracy = 0.553000\n",
      "---------------  lambda = 1e-06  ---------------\n",
      "Training: loss = 0.747396, accuracy = 0.680000\n",
      "Validation: loss = 0.978478, accuracy = 0.553000\n",
      "---------------  lambda = 1e-05  ---------------\n",
      "Training: loss = 0.747396, accuracy = 0.679000\n",
      "Validation: loss = 0.978487, accuracy = 0.553000\n",
      "---------------  lambda = 0.0001  ---------------\n",
      "Training: loss = 0.747396, accuracy = 0.679000\n",
      "Validation: loss = 0.978662, accuracy = 0.549000\n",
      "---------------  lambda = 0.001  ---------------\n",
      "Training: loss = 0.747410, accuracy = 0.681000\n",
      "Validation: loss = 0.977546, accuracy = 0.549000\n",
      "---------------  lambda = 0.01  ---------------\n",
      "Training: loss = 0.749357, accuracy = 0.663000\n",
      "Validation: loss = 0.955720, accuracy = 0.562000\n",
      "---------------  lambda = 0.1  ---------------\n",
      "Training: loss = 0.777011, accuracy = 0.653000\n",
      "Validation: loss = 0.930706, accuracy = 0.560000\n",
      "\n",
      "\n",
      "************* SVM for dataset 2 *************\n",
      "\n",
      "---------------  lambda = 1e-10  ---------------\n",
      "Training: loss = 0.590808, accuracy = 0.752000\n",
      "Validation: loss = 0.720112, accuracy = 0.709000\n",
      "---------------  lambda = 1e-09  ---------------\n",
      "Training: loss = 0.590808, accuracy = 0.752000\n",
      "Validation: loss = 0.720112, accuracy = 0.709000\n",
      "---------------  lambda = 1e-08  ---------------\n",
      "Training: loss = 0.590808, accuracy = 0.752000\n",
      "Validation: loss = 0.720112, accuracy = 0.709000\n",
      "---------------  lambda = 1e-07  ---------------\n",
      "Training: loss = 0.590808, accuracy = 0.752000\n",
      "Validation: loss = 0.720112, accuracy = 0.709000\n",
      "---------------  lambda = 1e-06  ---------------\n",
      "Training: loss = 0.590808, accuracy = 0.752000\n",
      "Validation: loss = 0.720112, accuracy = 0.709000\n",
      "---------------  lambda = 1e-05  ---------------\n",
      "Training: loss = 0.590808, accuracy = 0.752000\n",
      "Validation: loss = 0.720112, accuracy = 0.709000\n",
      "---------------  lambda = 0.0001  ---------------\n",
      "Training: loss = 0.590808, accuracy = 0.752000\n",
      "Validation: loss = 0.720111, accuracy = 0.709000\n",
      "---------------  lambda = 0.001  ---------------\n",
      "Training: loss = 0.590832, accuracy = 0.751000\n",
      "Validation: loss = 0.718276, accuracy = 0.707000\n",
      "---------------  lambda = 0.01  ---------------\n",
      "Training: loss = 0.592171, accuracy = 0.744000\n",
      "Validation: loss = 0.712661, accuracy = 0.704000\n",
      "---------------  lambda = 0.1  ---------------\n",
      "Training: loss = 0.619272, accuracy = 0.723000\n",
      "Validation: loss = 0.697893, accuracy = 0.698000\n"
     ]
    }
   ],
   "source": [
    "lambdas = [10**i for i in range(-10, 0)]\n",
    "print(\"************* SVM for dataset 0 *************\\n\")\n",
    "alphas_tr0, loss_tr0, acc_0, loss_val0, acc_val0 = SVM(K_tr0_ln, ytr0[:,1], K_val0_ln, yval0[:,1], lambdas)\n",
    "\n",
    "print(\"\")\n",
    "print(\"\")\n",
    "print(\"************* SVM for dataset 1 *************\\n\")\n",
    "alphas_tr1, loss_tr1, acc_1, loss_val1, acc_val1 = SVM(K_tr1_ln, ytr1[:,1], K_val1_ln, yval1[:,1],lambdas)\n",
    "\n",
    "print(\"\")\n",
    "print(\"\")\n",
    "print(\"************* SVM for dataset 2 *************\\n\")\n",
    "alphas_tr2, loss_tr2, acc_2, loss_val2, acc_val2 = SVM(K_tr2_ln, ytr2[:,1], K_val2_ln, yval2[:,1],lambdas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "western-saskatchewan",
   "metadata": {},
   "source": [
    "### Polynomial Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "magnetic-science",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************* SVM for dataset 0 *************\n",
      "\n",
      "---------------  lambda = 1e-10  ---------------\n",
      "Training: loss = 0.000000, accuracy = 1.000000\n",
      "Validation: loss = 0.934925, accuracy = 0.569000\n",
      "---------------  lambda = 1e-09  ---------------\n",
      "Training: loss = 0.000000, accuracy = 1.000000\n",
      "Validation: loss = 0.934940, accuracy = 0.569000\n",
      "---------------  lambda = 1e-08  ---------------\n",
      "Training: loss = 0.000000, accuracy = 1.000000\n",
      "Validation: loss = 0.934915, accuracy = 0.569000\n",
      "---------------  lambda = 1e-07  ---------------\n",
      "Training: loss = 0.000000, accuracy = 1.000000\n",
      "Validation: loss = 0.934917, accuracy = 0.569000\n",
      "---------------  lambda = 1e-06  ---------------\n",
      "Training: loss = 0.000000, accuracy = 1.000000\n",
      "Validation: loss = 0.934881, accuracy = 0.569000\n",
      "---------------  lambda = 1e-05  ---------------\n",
      "Training: loss = 0.000000, accuracy = 1.000000\n",
      "Validation: loss = 0.934884, accuracy = 0.569000\n",
      "---------------  lambda = 0.0001  ---------------\n",
      "Training: loss = 0.000000, accuracy = 1.000000\n",
      "Validation: loss = 0.934956, accuracy = 0.569000\n",
      "---------------  lambda = 0.001  ---------------\n",
      "Training: loss = 0.000000, accuracy = 1.000000\n",
      "Validation: loss = 0.934875, accuracy = 0.570000\n",
      "---------------  lambda = 0.01  ---------------\n",
      "Training: loss = 0.000000, accuracy = 1.000000\n",
      "Validation: loss = 0.934932, accuracy = 0.569000\n",
      "---------------  lambda = 0.1  ---------------\n",
      "Training: loss = 0.000000, accuracy = 1.000000\n",
      "Validation: loss = 0.934918, accuracy = 0.569000\n",
      "\n",
      "\n",
      "************* SVM for dataset 1 *************\n",
      "\n",
      "---------------  lambda = 1e-10  ---------------\n",
      "Training: loss = 0.000000, accuracy = 1.000000\n",
      "Validation: loss = 0.948479, accuracy = 0.562000\n",
      "---------------  lambda = 1e-09  ---------------\n",
      "Training: loss = 0.000000, accuracy = 1.000000\n",
      "Validation: loss = 0.948411, accuracy = 0.562000\n",
      "---------------  lambda = 1e-08  ---------------\n",
      "Training: loss = 0.000000, accuracy = 1.000000\n",
      "Validation: loss = 0.948474, accuracy = 0.562000\n",
      "---------------  lambda = 1e-07  ---------------\n",
      "Training: loss = 0.000000, accuracy = 1.000000\n",
      "Validation: loss = 0.948482, accuracy = 0.562000\n",
      "---------------  lambda = 1e-06  ---------------\n",
      "Training: loss = 0.000000, accuracy = 1.000000\n",
      "Validation: loss = 0.948455, accuracy = 0.562000\n",
      "---------------  lambda = 1e-05  ---------------\n",
      "Training: loss = 0.000000, accuracy = 1.000000\n",
      "Validation: loss = 0.948473, accuracy = 0.562000\n",
      "---------------  lambda = 0.0001  ---------------\n",
      "Training: loss = 0.000000, accuracy = 1.000000\n",
      "Validation: loss = 0.948481, accuracy = 0.562000\n",
      "---------------  lambda = 0.001  ---------------\n",
      "Training: loss = 0.000000, accuracy = 1.000000\n",
      "Validation: loss = 0.948466, accuracy = 0.562000\n",
      "---------------  lambda = 0.01  ---------------\n",
      "Training: loss = 0.000000, accuracy = 1.000000\n",
      "Validation: loss = 0.948482, accuracy = 0.562000\n",
      "---------------  lambda = 0.1  ---------------\n",
      "Training: loss = 0.000000, accuracy = 1.000000\n",
      "Validation: loss = 0.948480, accuracy = 0.562000\n",
      "\n",
      "\n",
      "************* SVM for dataset 2 *************\n",
      "\n",
      "---------------  lambda = 1e-10  ---------------\n",
      "Training: loss = 0.000000, accuracy = 1.000000\n",
      "Validation: loss = 0.770196, accuracy = 0.700000\n",
      "---------------  lambda = 1e-09  ---------------\n",
      "Training: loss = 0.000000, accuracy = 1.000000\n",
      "Validation: loss = 0.770226, accuracy = 0.700000\n",
      "---------------  lambda = 1e-08  ---------------\n",
      "Training: loss = 0.000000, accuracy = 1.000000\n",
      "Validation: loss = 0.770187, accuracy = 0.700000\n",
      "---------------  lambda = 1e-07  ---------------\n",
      "Training: loss = 0.000000, accuracy = 1.000000\n",
      "Validation: loss = 0.770204, accuracy = 0.700000\n",
      "---------------  lambda = 1e-06  ---------------\n",
      "Training: loss = 0.000000, accuracy = 1.000000\n",
      "Validation: loss = 0.770135, accuracy = 0.700000\n",
      "---------------  lambda = 1e-05  ---------------\n",
      "Training: loss = 0.000000, accuracy = 1.000000\n",
      "Validation: loss = 0.770157, accuracy = 0.700000\n",
      "---------------  lambda = 0.0001  ---------------\n",
      "Training: loss = 0.000000, accuracy = 1.000000\n",
      "Validation: loss = 0.770236, accuracy = 0.700000\n",
      "---------------  lambda = 0.001  ---------------\n",
      "Training: loss = 0.000000, accuracy = 1.000000\n",
      "Validation: loss = 0.770184, accuracy = 0.700000\n",
      "---------------  lambda = 0.01  ---------------\n",
      "Training: loss = 0.000000, accuracy = 1.000000\n",
      "Validation: loss = 0.770209, accuracy = 0.700000\n",
      "---------------  lambda = 0.1  ---------------\n",
      "Training: loss = 0.000000, accuracy = 1.000000\n",
      "Validation: loss = 0.770191, accuracy = 0.700000\n"
     ]
    }
   ],
   "source": [
    "lambdas = [10**i for i in range(-10, 0)]\n",
    "print(\"************* SVM for dataset 0 *************\\n\")\n",
    "alphas_tr0, loss_tr0, acc_0, loss_val0, acc_val0 = SVM(K_tr0_poly, ytr0[:,1], K_val0_poly, yval0[:,1], lambdas)\n",
    "\n",
    "print(\"\")\n",
    "print(\"\")\n",
    "print(\"************* SVM for dataset 1 *************\\n\")\n",
    "alphas_tr1, loss_tr1, acc_1, loss_val1, acc_val1 = SVM(K_tr1_poly, ytr1[:,1], K_val1_poly, yval1[:,1],lambdas)\n",
    "\n",
    "print(\"\")\n",
    "print(\"\")\n",
    "print(\"************* SVM for dataset 2 *************\\n\")\n",
    "alphas_tr2, loss_tr2, acc_2, loss_val2, acc_val2 = SVM(K_tr2_poly, ytr2[:,1], K_val2_poly, yval2[:,1],lambdas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "written-dakota",
   "metadata": {},
   "source": [
    "## Making predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lovely-adaptation",
   "metadata": {},
   "source": [
    "### First create the kernels for each testing set with the chosen parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "recognized-congo",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xte0 = np.genfromtxt(\"data/Xte0_mat100.csv\", delimiter='')\n",
    "Xte1 = np.genfromtxt(\"data/Xte1_mat100.csv\", delimiter='')\n",
    "Xte2 = np.genfromtxt(\"data/Xte2_mat100.csv\", delimiter='')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "photographic-eagle",
   "metadata": {},
   "source": [
    "Please make sure to use the same parameters as those that were used to create the initial kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "experimental-detective",
   "metadata": {},
   "outputs": [],
   "source": [
    "K_te0 = gaussian_kernel(Xtr0, Xte0, mode=\"test\")\n",
    "K_te1 = gaussian_kernel(Xtr1, Xte1, mode=\"test\")\n",
    "K_te2 = gaussian_kernel(Xtr2, Xte2, mode=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "measured-adjustment",
   "metadata": {},
   "outputs": [],
   "source": [
    "K_te0_ln = linear_kernel(Xtr0, Xte0, scale=True, mode=\"test\")\n",
    "K_te1_ln = linear_kernel(Xtr1, Xte1, scale=True, mode=\"test\")\n",
    "K_te2_ln = linear_kernel(Xtr2, Xte2, scale=True, mode=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "competent-driver",
   "metadata": {},
   "outputs": [],
   "source": [
    "K_te0_poly = polynomial_kernel(Xtr0, Xte0, d=3, c=1, mode=\"test\")\n",
    "K_te1_poly = polynomial_kernel(Xtr1, Xte1, d=3, c=1, mode=\"test\")\n",
    "K_te2_poly = polynomial_kernel(Xtr2, Xte2, d=3, c=1, mode=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "cooperative-schema",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_predictions_csv(test_kernels, test_alphas, path):\n",
    "    \n",
    "    predictions = np.zeros(3000, dtype=int)\n",
    "    \n",
    "    for i in range(3):\n",
    "        y_pred = test_kernels[i] @ test_alphas[i]\n",
    "        y_pred[y_pred >= 0.5] = 1\n",
    "        y_pred[y_pred < 0.5] = 0\n",
    "        \n",
    "        predictions[1000*i:1000*(i+1)] = y_pred\n",
    "    \n",
    "    #predictions = predictions.astype(int)\n",
    "    pred = pd.DataFrame({\"Bound\" : predictions})\n",
    "    pred.to_csv(path, index=True,index_label=\"Id\")\n",
    "    print(\"saving predictions\")\n",
    "    #np.savetxt(\"data/Ytest_KRR.csv\", predictions, header = \"Id, Bound\", delimiter =\",\")\n",
    "    print(\"saved predictions\")\n",
    "    return(predictions)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cubic-response",
   "metadata": {},
   "source": [
    "Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "major-collapse",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving predictions\n",
      "saved predictions\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_kernels = [K_te0, K_te1, K_te2]\n",
    "#test_alphas = [alphas_tr0[-4], alphas_tr1[-4], alphas_tr2[-3]] # il faut choisir l'alpha associé à un bon lambda!\n",
    "test_alphas = [alphas_tr0_klr[0], alphas_tr1_klr[0], alphas_tr2_klr[0]]\n",
    "write_predictions_csv(test_kernels, test_alphas, path =\"data/Ytest_KLR.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "signal-radical",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
