{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "utility-italian",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "through-indicator",
   "metadata": {},
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "detailed-perspective",
   "metadata": {},
   "source": [
    "For $k = 0, 1, 2$ we have the following files:\n",
    "* Xtrk.csv - the training sequences.\n",
    "* Xtek.csv - the test sequences.\n",
    "* Ytrk.csv - labels for the training sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "straight-april",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtr0_mat100 = np.genfromtxt(\"data/Xtr0_mat100.csv\", delimiter='')\n",
    "Ytr0 = np.genfromtxt(\"data/Ytr0.csv\", delimiter=',', skip_header=1)\n",
    "\n",
    "Xtr1_mat100 = np.genfromtxt(\"data/Xtr1_mat100.csv\", delimiter='')\n",
    "Ytr1 = np.genfromtxt(\"data/Ytr1.csv\", delimiter=',', skip_header=1)\n",
    "\n",
    "Xtr2_mat100 = np.genfromtxt(\"data/Xtr2_mat100.csv\", delimiter='')\n",
    "Ytr2 = np.genfromtxt(\"data/Ytr2.csv\", delimiter=',', skip_header=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "juvenile-facial",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_true,y_pred):\n",
    "    n = y_true.shape[0]\n",
    "    predictions = np.zeros(n)\n",
    "    predictions[y_pred >= 0.5] = 1\n",
    "    return np.sum(y_true == predictions) / n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bearing-religion",
   "metadata": {},
   "source": [
    "# Implementing some kernels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "revised-underwear",
   "metadata": {},
   "source": [
    "## Linear Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "alike-terminal",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_kernel(X):\n",
    "    return(X @ X.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "american-postcard",
   "metadata": {},
   "source": [
    "## Gaussian Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "original-happiness",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian(x,y, sigma):\n",
    "    exp_term = np.linalg.norm(x-y)**2 /(2*sigma)\n",
    "    return(np.exp(-exp_term))\n",
    "\n",
    "# Naive computation of the gaussian kernel that can be easily improved\n",
    "def gaussian_kernel(X,sigma):\n",
    "    n = X.shape[0]\n",
    "    K = np.eye(n) # One along the diagonals because K(x,x) = exp(0) = 1\n",
    "    for i in range(n):\n",
    "        for j in range(i+1,n):\n",
    "            val = gaussian(X[i], X[j], sigma)\n",
    "            K[i,j] = val\n",
    "            K[j,i] = val\n",
    "    return(K)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compressed-basics",
   "metadata": {},
   "source": [
    "## Polynomial Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "atmospheric-radar",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "choice-charleston",
   "metadata": {},
   "source": [
    "## Kernel Ridge Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "limiting-burning",
   "metadata": {},
   "source": [
    "* Consider RKHS $\\mathcal H$, associated to a p.d. kernel K on $\\mathcal X$\n",
    "* Let $y = (y_1, \\dots, y_n)^T \\in \\mathbb R ^n$\n",
    "* Let $\\alpha = (\\alpha_1, \\dots, \\alpha_n)^T \\in \\mathbb R ^n$\n",
    "* Let $K$ be the $n\\times n$ Gram Matrix such that $K_{i,j} = K(x_i, x_j)$\n",
    "* We can then write\n",
    "$$\n",
    "(\\hat f(x_1), \\dots, \\hat f(x_n))^T = K\\alpha\n",
    "$$\n",
    "* The norm is $||\\hat f||^2_{\\mathcal H} = \\alpha^T K \\alpha$\n",
    "* KRR $\\leftrightarrow \\text{argmin}_{\\alpha \\in \\mathbb R^n} \\frac{1}{n} (K\\alpha - y)^T(K\\alpha - y) + \\lambda \\alpha^T K \\alpha$\n",
    "* Solution for $\\lambda > 0$:\n",
    "$$\n",
    "\\alpha = (K+\\lambda nI)^{-1}y\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "entertaining-grounds",
   "metadata": {},
   "outputs": [],
   "source": [
    "def KRR(K, y, Kval, yval, lambd):\n",
    "    \"\"\"\n",
    "    takes the kernel matrix as an input and computes the MSE and the predictions for each value in lambd (list)\n",
    "    \"\"\"\n",
    "    assert K.shape[0] == y.shape[0]\n",
    "    assert len(lambd) > 0\n",
    "    n = K.shape[0]\n",
    "    \n",
    "    loss = []\n",
    "    acc = []\n",
    "    \n",
    "    loss_val = []\n",
    "    acc_val = []\n",
    "    alphas = []\n",
    "    \n",
    "    for l in lambd:\n",
    "        \n",
    "        assert l >= 0\n",
    "        # find the parameter alpha\n",
    "        alpha = np.linalg.solve((K + l*n*np.eye(n)), y)\n",
    "        # predict\n",
    "        \n",
    "        loss_lambda = MSE(K, y, l, alpha)\n",
    "        acc_lambda = accuracy(y,K@alpha)\n",
    "        \n",
    "        loss_lambdaval = MSE(Kval, yval, l, alpha)\n",
    "        acc_lambdaval = accuracy(yval,Kval@alpha)\n",
    "\n",
    "        print(f\"***********lambda = {l}***********\")\n",
    "        print(f\"Training: loss = {loss_lambda:.4f}, accuracy = {acc_lambda:.6f}\")\n",
    "        print(f\"Validation: loss = {loss_lambdaval:.4f}, accuracy = {acc_lambdaval:.6f}\")\n",
    "        \n",
    "        loss += [loss_lambda]\n",
    "        acc += [acc_lambda]\n",
    "        \n",
    "        loss_val += [loss_lambdaval]\n",
    "        acc_val += [acc_lambdaval]\n",
    "        \n",
    "        \n",
    "        alphas +=[alpha]\n",
    "        \n",
    "    return(alphas, loss, acc, loss_val, acc_val)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "limiting-general",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MSE(K, y, lambd, alpha):\n",
    "    n = y.shape[0]\n",
    "    data_term = (np.linalg.norm(np.dot(K, alpha.reshape(-1,1)) - y)**2)/n\n",
    "    reg_term = alpha @ K @ alpha\n",
    "    return(data_term + lambd * reg_term)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "speaking-interpretation",
   "metadata": {},
   "source": [
    "## Kernel Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hidden-argument",
   "metadata": {},
   "source": [
    "- Binary Classificaiton setup: $\\mathcal Y = \\{-1, 1\\}$\n",
    "- $\\mathcal l_{\\text{logistic}}(f(x),y) = -\\log p(y|f(x)) = \\log(1 + e^{-yf(x)})$ where $p(y|f(x)) = \\sigma(y(f(x))$\n",
    "\n",
    "Objective:\n",
    "\\begin{align*}\n",
    "\\hat f &= \\text{argmin}_{f\\in \\mathcal H} \\frac{1}{n} \\sum_{i=1}^n \\log(1+e^{-y_if(x_i)}) + \\frac{\\lambda}{2}||f||^2_{\\mathcal H}\\\\\n",
    "\\alpha &= \\text{argmin}_{\\alpha \\in \\mathbb R^n} \\frac{1}{n} \\sum_{i=1}^n \\log(1+e^{-y_i[K\\alpha]_i}) + \\frac{\\lambda}{2} \\alpha^T K \\alpha\n",
    "\\end{align*}\n",
    "\n",
    "We define the following fonctions and vectors:\n",
    "* $\\mathcal l _\\text{logistic}(u) = \\log(1+e^{-u})$\n",
    "* $\\mathcal l' _\\text{logistic}(u) = -\\sigma(-u)$\n",
    "* $\\mathcal l'' _\\text{logistic}(u) = \\sigma(u)\\sigma(-u)$\n",
    "\n",
    "* for $i = 1, \\dots, n$, $P_i(\\alpha) = \\mathcal l' _\\text{logistic}(y_i[K\\alpha]_i)$\n",
    "* for $i = 1, \\dots, n$, $W_i(\\alpha) = \\mathcal l'' _\\text{logistic}(y_i[K\\alpha]_i)$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\\begin{align*}\n",
    "J(\\alpha) &= \\frac{1}{n} \\sum_{i=1}^n \\log(1+e^{-y_i[K\\alpha]_i}) + \\frac{\\lambda}{2} \\alpha^T K \\alpha\\\\\n",
    "\\nabla J(\\alpha) &= \\frac{1}{n} KP(\\alpha) y + \\lambda K \\alpha \\quad \\text{where } P(\\alpha) = \\text{diag}(P_1(\\alpha), \\dots, P_n(\\alpha))\\\\\n",
    "\\nabla^2 J(\\alpha) &= \\frac{1}{n}KW(\\alpha)K+\\lambda K \\quad \\text{where } W(\\alpha) = \\text{diag}(W_1(\\alpha), \\dots, W_n(\\alpha))\n",
    "\\end{align*}\n",
    "\n",
    "We are interested in the quadratic approximation of $J$ near a point $\\alpha_0$:\n",
    "\\begin{align*}\n",
    "J_q(\\alpha) &= J(\\alpha_0) + (\\alpha - \\alpha_0)^T \\nabla J(\\alpha_0) + \\frac{1}{2} (\\alpha - \\alpha_0)^T \\nabla^2 J(\\alpha_0)(\\alpha - \\alpha_0)\\\\\n",
    "2J_q(\\alpha) &= -\\frac{2}{n} \\alpha^T KW(K\\alpha_0-W^{-1}Py)+\\frac{1}{n}\\alpha^TKWK\\alpha+ \\lambda\\alpha^TK\\alpha +C\\\\\n",
    "&= \\frac{1}{n} (K\\alpha - z)^TW(K\\alpha - z) + \\lambda\\alpha^TK\\alpha + C \\quad \\text{where} z = K\\alpha_0 - W^{-1} P y\n",
    "\\end{align*}\n",
    "\n",
    "The WKRR problem is presented as:\n",
    "$$\n",
    "\\text{argmin}_{\\alpha \\in \\mathbb R^n} \\frac{1}{n}(K\\alpha - y)^TW(K\\alpha - y) + \\lambda \\alpha^TK\\alpha\n",
    "$$\n",
    "and has as :\n",
    "$$\n",
    "\\alpha = W^{1/2} (W^{1/2}KW^{1/2}+n\\lambda I)^{-1} W^{1/2}y\n",
    "$$\n",
    "\n",
    "So, in order to solve KRL, we use IRLS on a WKRR problem until convergence:\n",
    "$$\\alpha^{t+1} \\gets \\text{solveWKRR}(K, W^t, z^t)$$\n",
    "With the updates for $W^t$ and $z^t$ from $\\alpha^t$ are:\n",
    "- $m_i \\gets [K\\alpha^t]_i$\n",
    "- $P_i^t \\gets -\\sigma(-y_im_i)$\n",
    "- $W_i^t \\gets \\sigma(m_i)\\sigma(-m_i)$\n",
    "- $z_i^t \\gets m_i + y_i / \\sigma(-y_im_i)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "backed-earthquake",
   "metadata": {},
   "outputs": [],
   "source": [
    "def solveWKRR(K,W,z,y,lambd):\n",
    "    \n",
    "    assert np.all(W >= 0)\n",
    "    \n",
    "    W_sq = np.sqrt(W)\n",
    "    n = K.shape[0]\n",
    "    inv_matrix = np.linalg.solve((W_sq @ K @ W_sq + n * lambd * np.eye(n)), W_sq @ y)\n",
    "    alpha = W_sq @ inv_matrix\n",
    "    \n",
    "    return alpha\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "level-queue",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "def logistic_loss(y_true, y_pred):\n",
    "    n = y_true.shape[0]\n",
    "    log_term = np.log(sigmoid(y_true*y_pred))\n",
    "    return(-np.sum(log_term)/n)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "greatest-earthquake",
   "metadata": {},
   "outputs": [],
   "source": [
    "def KLR(K, y, lambd, maxIter = 100, tresh = 1e-8):\n",
    "    \n",
    "    # initialize the values\n",
    "    assert K.shape[0] == y.shape[0]\n",
    "    n = K.shape[0]\n",
    "    \n",
    "    y_preds = []\n",
    "    loss = []\n",
    "    accuracies = []\n",
    "    alphas = []\n",
    "    \n",
    "    for l in lambd :\n",
    "        cnt = 0\n",
    "        \n",
    "        P_t, W_t = np.eye(n), np.eye(n)\n",
    "        z_t = K@ np.ones(n) - y\n",
    "        alpha_t = np.ones(n)\n",
    "        diff_alpha = np.inf\n",
    "\n",
    "\n",
    "        while (diff_alpha > tresh) and (cnt < maxIter):\n",
    "\n",
    "            old_alpha = alpha_t\n",
    "            alpha_t = solveWKRR(K, W_t, z_t, y, l)\n",
    "\n",
    "            m_t = K@alpha_t\n",
    "            sigma_m = sigmoid(m_t)\n",
    "            sigma_my = sigmoid(-y*m_t)\n",
    "\n",
    "            P_t = - np.diag(sigma_my)\n",
    "            W_t = np.diag(sigma_m * (1-sigma_m))\n",
    "\n",
    "            z_t = m_t - (P_t@y)/(sigma_m * (1-sigma_m))\n",
    "\n",
    "            diff_alpha = np.linalg.norm(alpha_t - old_alpha)\n",
    "            cnt+=1\n",
    "            if cnt % 10 == 0:\n",
    "                print(l, cnt)\n",
    "\n",
    "        pred_l = K@alpha_t\n",
    "        y_preds += [pred_l]\n",
    "        loss_l = logistic_loss(y, pred_l)\n",
    "        loss += [loss_l]\n",
    "        accuracy_l = accuracy(y, pred_l)\n",
    "        accuracies += [accuracy_l]\n",
    "        alphas +=[alpha_t]\n",
    "        print(f\"The logistic loss and accuracy for lambda = {l} are : {loss_l:.4f}, {accuracy_l:.6f} \")\n",
    "        \n",
    "    \n",
    "    return alphas, loss, accuracies\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "consolidated-rolling",
   "metadata": {},
   "source": [
    "# Testing the accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "local-cornell",
   "metadata": {},
   "source": [
    "## Splitting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "preceding-allowance",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "Xtr0, Xval0, ytr0, yval0 = train_test_split(Xtr0_mat100, Ytr0, test_size=0.5, random_state=42)\n",
    "Xtr1, Xval1, ytr1, yval1 = train_test_split(Xtr1_mat100, Ytr1, test_size=0.5, random_state=42)\n",
    "Xtr2, Xval2, ytr2, yval2 = train_test_split(Xtr2_mat100, Ytr2, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "simple-principal",
   "metadata": {},
   "source": [
    "## Create the kernel matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "broken-defense",
   "metadata": {},
   "outputs": [],
   "source": [
    "K_tr0 = gaussian_kernel(Xtr0,1)\n",
    "K_tr1 = gaussian_kernel(Xtr1,1)\n",
    "K_tr2 = gaussian_kernel(Xtr2,1)\n",
    "\n",
    "K_val0 = gaussian_kernel(Xval0,1)\n",
    "K_val1 = gaussian_kernel(Xval1,1)\n",
    "K_val2 = gaussian_kernel(Xval2,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "healthy-abraham",
   "metadata": {},
   "source": [
    "## Testing KRR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "id": "featured-endorsement",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*************KRR for dataset 0*************\n",
      "\n",
      "***********lambda = 0***********\n",
      "Training: loss = 497.5500, accuracy = 1.000000\n",
      "Validation: loss = 3708479274.0792, accuracy = 0.484000\n",
      "***********lambda = 1e-10***********\n",
      "Training: loss = 496.5732, accuracy = 1.000000\n",
      "Validation: loss = 3651845485.1501, accuracy = 0.484000\n",
      "***********lambda = 1e-09***********\n",
      "Training: loss = 488.3212, accuracy = 1.000000\n",
      "Validation: loss = 3216798580.0829, accuracy = 0.484000\n",
      "***********lambda = 1e-08***********\n",
      "Training: loss = 435.7197, accuracy = 1.000000\n",
      "Validation: loss = 1447588604.8629, accuracy = 0.490000\n",
      "***********lambda = 1e-07***********\n",
      "Training: loss = 334.5539, accuracy = 0.956000\n",
      "Validation: loss = 109421856.4602, accuracy = 0.479000\n",
      "***********lambda = 1e-06***********\n",
      "Training: loss = 293.8359, accuracy = 0.763000\n",
      "Validation: loss = 2141724.4853, accuracy = 0.480000\n",
      "***********lambda = 1e-05***********\n",
      "Training: loss = 283.1786, accuracy = 0.698000\n",
      "Validation: loss = 33924.7992, accuracy = 0.488000\n",
      "***********lambda = 0.0001***********\n",
      "Training: loss = 262.2817, accuracy = 0.655000\n",
      "Validation: loss = 672.0894, accuracy = 0.490000\n",
      "***********lambda = 0.001***********\n",
      "Training: loss = 250.1031, accuracy = 0.586000\n",
      "Validation: loss = 258.7598, accuracy = 0.496000\n",
      "***********lambda = 0.01***********\n",
      "Training: loss = 248.8345, accuracy = 0.535000\n",
      "Validation: loss = 251.7460, accuracy = 0.503000\n",
      "***********lambda = 0.1***********\n",
      "Training: loss = 250.6277, accuracy = 0.535000\n",
      "Validation: loss = 255.6777, accuracy = 0.503000\n",
      "***********lambda = 1***********\n",
      "Training: loss = 303.6614, accuracy = 0.535000\n",
      "Validation: loss = 320.9084, accuracy = 0.503000\n",
      "***********lambda = 10***********\n",
      "Training: loss = 427.9550, accuracy = 0.535000\n",
      "Validation: loss = 457.2874, accuracy = 0.503000\n",
      "*************KRR for dataset 1*************\n",
      "\n",
      "***********lambda = 0***********\n",
      "Training: loss = 499.8380, accuracy = 1.000000\n",
      "Validation: loss = 5059892005.7636, accuracy = 0.472000\n",
      "***********lambda = 1e-10***********\n",
      "Training: loss = 498.7564, accuracy = 1.000000\n",
      "Validation: loss = 5004815764.6524, accuracy = 0.472000\n",
      "***********lambda = 1e-09***********\n",
      "Training: loss = 489.6075, accuracy = 1.000000\n",
      "Validation: loss = 4556706596.1752, accuracy = 0.472000\n",
      "***********lambda = 1e-08***********\n",
      "Training: loss = 431.8375, accuracy = 1.000000\n",
      "Validation: loss = 2313455839.5608, accuracy = 0.475000\n",
      "***********lambda = 1e-07***********\n",
      "Training: loss = 326.3506, accuracy = 0.942000\n",
      "Validation: loss = 230245916.0673, accuracy = 0.474000\n",
      "***********lambda = 1e-06***********\n",
      "Training: loss = 288.3705, accuracy = 0.738000\n",
      "Validation: loss = 5909602.7202, accuracy = 0.482000\n",
      "***********lambda = 1e-05***********\n",
      "Training: loss = 279.3917, accuracy = 0.664000\n",
      "Validation: loss = 100425.6149, accuracy = 0.484000\n",
      "***********lambda = 0.0001***********\n",
      "Training: loss = 261.9437, accuracy = 0.666000\n",
      "Validation: loss = 1552.6370, accuracy = 0.484000\n",
      "***********lambda = 0.001***********\n",
      "Training: loss = 251.3408, accuracy = 0.626000\n",
      "Validation: loss = 295.8968, accuracy = 0.490000\n",
      "***********lambda = 0.01***********\n",
      "Training: loss = 250.0104, accuracy = 0.509000\n",
      "Validation: loss = 252.5946, accuracy = 0.490000\n",
      "***********lambda = 0.1***********\n",
      "Training: loss = 251.9788, accuracy = 0.509000\n",
      "Validation: loss = 254.3798, accuracy = 0.490000\n",
      "***********lambda = 1***********\n",
      "Training: loss = 311.0924, accuracy = 0.509000\n",
      "Validation: loss = 320.9121, accuracy = 0.490000\n",
      "***********lambda = 10***********\n",
      "Training: loss = 449.6963, accuracy = 0.509000\n",
      "Validation: loss = 467.0267, accuracy = 0.490000\n",
      "*************KRR for dataset 2*************\n",
      "\n",
      "***********lambda = 0***********\n",
      "Training: loss = 21655837548.5069, accuracy = 0.500000\n",
      "Validation: loss = 1351670897768402133227423865721847808.0000, accuracy = 0.490000\n",
      "***********lambda = 1e-10***********\n",
      "Training: loss = 499.1654, accuracy = 1.000000\n",
      "Validation: loss = 921119824.6499, accuracy = 0.509000\n",
      "***********lambda = 1e-09***********\n",
      "Training: loss = 492.1161, accuracy = 1.000000\n",
      "Validation: loss = 746492425.7063, accuracy = 0.509000\n",
      "***********lambda = 1e-08***********\n",
      "Training: loss = 446.3420, accuracy = 1.000000\n",
      "Validation: loss = 420187867.7965, accuracy = 0.515000\n",
      "***********lambda = 1e-07***********\n",
      "Training: loss = 358.3211, accuracy = 0.957000\n",
      "Validation: loss = 97401714.8793, accuracy = 0.482000\n",
      "***********lambda = 1e-06***********\n",
      "Training: loss = 324.9344, accuracy = 0.802000\n",
      "Validation: loss = 4825475.6961, accuracy = 0.493000\n",
      "***********lambda = 1e-05***********\n",
      "Training: loss = 315.0190, accuracy = 0.750000\n",
      "Validation: loss = 67825.7441, accuracy = 0.495000\n",
      "***********lambda = 0.0001***********\n",
      "Training: loss = 285.7911, accuracy = 0.716000\n",
      "Validation: loss = 857.7379, accuracy = 0.494000\n",
      "***********lambda = 0.001***********\n",
      "Training: loss = 254.5973, accuracy = 0.686000\n",
      "Validation: loss = 256.9641, accuracy = 0.496000\n",
      "***********lambda = 0.01***********\n",
      "Training: loss = 250.1251, accuracy = 0.669000\n",
      "Validation: loss = 250.0715, accuracy = 0.486000\n",
      "***********lambda = 0.1***********\n",
      "Training: loss = 252.1529, accuracy = 0.499000\n",
      "Validation: loss = 251.6198, accuracy = 0.504000\n",
      "***********lambda = 1***********\n",
      "Training: loss = 313.7000, accuracy = 0.499000\n",
      "Validation: loss = 311.1107, accuracy = 0.504000\n",
      "***********lambda = 10***********\n",
      "Training: loss = 457.9851, accuracy = 0.499000\n",
      "Validation: loss = 453.4159, accuracy = 0.504000\n"
     ]
    }
   ],
   "source": [
    "lambdas = [0] + [10**i for i in range(-10,2)]\n",
    "print(\"*************KRR for dataset 0*************\\n\")\n",
    "alphas_tr0, loss_tr0, acc_0, loss_val0, acc_val0 = KRR(K_tr0, ytr0[:,1], K_val0, yval0[:,1], lambdas)\n",
    "print(\"*************KRR for dataset 1*************\\n\")\n",
    "alphas_tr1, loss_tr1, acc_1, loss_val1, acc_val1 = KRR(K_tr1, ytr1[:,1], K_val1, yval1[:,1],lambdas)\n",
    "print(\"*************KRR for dataset 2*************\\n\")\n",
    "alphas_tr2, loss_tr2, acc_2, loss_val2, acc_val2 = KRR(K_tr2, ytr2[:,1], K_val2, yval2[:,1],lambdas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "original-houston",
   "metadata": {},
   "source": [
    "## Testing KLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "suspended-waterproof",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*************KLR for dataset 0*************\n",
      "\n",
      "0 10\n",
      "0 20\n",
      "0 30\n",
      "0 40\n",
      "0 50\n",
      "0 60\n",
      "0 70\n",
      "0 80\n",
      "0 90\n",
      "0 100\n",
      "The logistic loss and accuracy for lambda = 0 are : 0.5104, 1.000000 \n",
      "1e-10 10\n",
      "1e-10 20\n",
      "1e-10 30\n",
      "1e-10 40\n",
      "1e-10 50\n",
      "1e-10 60\n",
      "1e-10 70\n",
      "1e-10 80\n",
      "1e-10 90\n",
      "1e-10 100\n",
      "The logistic loss and accuracy for lambda = 1e-10 are : 0.5126, 1.000000 \n",
      "1e-09 10\n",
      "1e-09 20\n",
      "1e-09 30\n",
      "1e-09 40\n",
      "1e-09 50\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-267-5d082d11d82f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mlambdas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"*************KLR for dataset 0*************\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mpred_klr_tr0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malphas_klr_tr0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_klr_tr0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracies_klr_0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKLR\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mK_tr0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mYtr0\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambdas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtresh\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"*************KLR for dataset 1*************\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mpred_klr_tr1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malphas_klr_tr1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_klr_tr1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracies_klr_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKLR\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mK_tr1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mYtr1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambdas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtresh\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-265-7a28a41b8a7a>\u001b[0m in \u001b[0;36mKLR\u001b[0;34m(K, y, lambd, maxIter, tresh)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0mold_alpha\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0malpha_t\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m             \u001b[0malpha_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msolveWKRR\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0mm_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m@\u001b[0m\u001b[0malpha_t\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-231-a707a87497b7>\u001b[0m in \u001b[0;36msolveWKRR\u001b[0;34m(K, W, z, y, lambd)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mW_sq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0minv_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msolve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW_sq\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mK\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mW_sq\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlambd\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meye\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW_sq\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0malpha\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mW_sq\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0minv_matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "lambdas = [0] + [10**i for i in range(-10,0)]\n",
    "print(\"*************KLR for dataset 0*************\\n\")\n",
    "alphas_klr_tr0, loss_klr_tr0, accuracies_klr_0 = KLR(K_tr0, Ytr0[:,1], lambdas, tresh=1e-5)\n",
    "print(\"*************KLR for dataset 1*************\\n\")\n",
    "alphas_klr_tr1, loss_klr_tr1, accuracies_klr_1 = KLR(K_tr1, Ytr1[:,1], lambdas, tresh=1e-5)\n",
    "print(\"*************KLR for dataset 2*************\\n\")\n",
    "alphas_klr_tr2, loss_klr_tr2, accuracies_klr_2 = KLR(K_tr2, Ytr2[:,1], lambdas, tresh=1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ideal-power",
   "metadata": {},
   "source": [
    "## Making predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wired-fundamental",
   "metadata": {},
   "source": [
    "### First create the kernels for each testing set with the chosen parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "diagnostic-limit",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xte0 = np.genfromtxt(\"data/Xte0_mat100.csv\", delimiter='')\n",
    "Xte1 = np.genfromtxt(\"data/Xte1_mat100.csv\", delimiter='')\n",
    "Xte2 = np.genfromtxt(\"data/Xte2_mat100.csv\", delimiter='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "animated-mongolia",
   "metadata": {},
   "outputs": [],
   "source": [
    "K_te0 = gaussian_kernel(Xte0,1)\n",
    "K_te1 = gaussian_kernel(Xte1,1)\n",
    "K_te2 = gaussian_kernel(Xte2,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "id": "outer-employment",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_predictions_csv(test_kernels, test_alphas):\n",
    "    \n",
    "    predictions = np.zeros((3000,2), dtype=int)\n",
    "    predictions[:,0] = np.arange(0,3000)\n",
    "    \n",
    "    for i in range(3):\n",
    "        y_pred = test_kernels[0] @ test_alphas[0]\n",
    "        y_pred[y_pred >= 0.5] = 1\n",
    "        y_pred[y_pred < 0.5] = 0\n",
    "        \n",
    "        predictions[1000*i:1000*(i+1), 1] = y_pred\n",
    "    predictions = predictions.astype(int)\n",
    "    print(\"saving predictions\")\n",
    "    np.savetxt(\"data/Ytest_KRR.csv\", predictions, header = \"Id, Bound\", delimiter =\",\")\n",
    "    print(\"saved predictions\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "welcome-implementation",
   "metadata": {},
   "source": [
    "Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "id": "rapid-librarian",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving predictions\n",
      "saved predictions\n"
     ]
    }
   ],
   "source": [
    "test_kernels = [K_te0, K_te1, K_te2]\n",
    "test_alphas = [alphas_tr0[0], alphas_tr1[0], alphas_tr2[0]] # il faut choisir l'alpha associé à un bon lambda!\n",
    "\n",
    "write_predictions_csv(test_kernels, test_alphas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "impressive-timber",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
