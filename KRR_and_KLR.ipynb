{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "utility-italian",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "noted-driver",
   "metadata": {},
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "diagnostic-heaven",
   "metadata": {},
   "source": [
    "For $k = 0, 1, 2$ we have the following files:\n",
    "* Xtrk.csv - the training sequences.\n",
    "* Xtek.csv - the test sequences.\n",
    "* Ytrk.csv - labels for the training sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "reverse-sociology",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtr0_mat100 = np.genfromtxt(\"data/Xtr0_mat100.csv\", delimiter='')\n",
    "Ytr0 = np.genfromtxt(\"data/Ytr0.csv\", delimiter=',', skip_header=1)\n",
    "\n",
    "Xtr1_mat100 = np.genfromtxt(\"data/Xtr1_mat100.csv\", delimiter='')\n",
    "Ytr1 = np.genfromtxt(\"data/Ytr1.csv\", delimiter=',', skip_header=1)\n",
    "\n",
    "Xtr2_mat100 = np.genfromtxt(\"data/Xtr2_mat100.csv\", delimiter='')\n",
    "Ytr2 = np.genfromtxt(\"data/Ytr2.csv\", delimiter=',', skip_header=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "coordinate-chassis",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_true,y_pred, mode='SVM'):\n",
    "    n = y_true.shape[0]\n",
    "    if mode == 'SVM':\n",
    "        predictions = np.ones(n)\n",
    "        predictions[y_pred < 0] = 0\n",
    "    else:\n",
    "        predictions = np.zeros(n)\n",
    "        predictions[y_pred >= 0.5] = 1\n",
    "    \n",
    "    return np.sum(y_true == predictions) / n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "invalid-present",
   "metadata": {},
   "source": [
    "# Implementing some kernels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "white-lounge",
   "metadata": {},
   "source": [
    "## Linear Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "congressional-authority",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_kernel(X_train, X_valid, scale=True, mode=\"train\"):\n",
    "    \n",
    "    if scale:\n",
    "        X_tr = (X_train-X_train.mean(axis=0)) / X_train.std(axis=0)\n",
    "        X_va = (X_valid-X_train.mean(axis=0)) / X_train.std(axis=0)\n",
    "        \n",
    "        K_va = X_va @ X_tr.T\n",
    "        \n",
    "        if mode == \"test\":\n",
    "            return(K_va)\n",
    "        \n",
    "        K_tr = X_tr @ X_tr.T\n",
    "        \n",
    "    else:\n",
    "        K_va = X_valid @ X_train.T\n",
    "        \n",
    "        if mode == \"test\":\n",
    "            return(K_va)\n",
    "        \n",
    "        K_tr = X_train @ X_train.T\n",
    "        \n",
    "    return(K_tr, K_va)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adequate-momentum",
   "metadata": {},
   "source": [
    "## Gaussian Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "proprietary-costa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Idea : efficient computation of the pairwise distances\n",
    "def gaussian_kernel(X_train, X_valid, sigma=None, scale=True, mode=\"train\"):\n",
    "    \n",
    "    n, p = X_train.shape\n",
    "    \n",
    "    if scale:\n",
    "        X_tr = (X_train-X_train.mean(axis=0)) / X_train.std(axis=0)\n",
    "        X_va = (X_valid-X_train.mean(axis=0)) / X_train.std(axis=0)\n",
    "    else:\n",
    "        X_tr = np.copy(X_train)\n",
    "        X_va = np.copy(X_valid)\n",
    "    if sigma is None:\n",
    "        sigma = p\n",
    "        \n",
    "    K_va = np.linalg.norm(X_va[:, None, ...] - X_tr[None, ...], axis=-1)**2\n",
    "    K_va = np.exp((-K_va)/(sigma))\n",
    "    \n",
    "    if mode==\"test\":\n",
    "        return(K_va)\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        K_tr = ((X_tr[:, :, None] - X_tr[:, :, None].T) ** 2).sum(1)\n",
    "        K_tr = np.exp((-K_tr)/(sigma))\n",
    "        return(K_tr, K_va)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "handmade-preview",
   "metadata": {},
   "source": [
    "## Polynomial Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "commercial-window",
   "metadata": {},
   "outputs": [],
   "source": [
    "def polynomial_kernel(X_train, X_valid, d=3, c=1, scale=True, mode=\"train\"):\n",
    "    \n",
    "    # k(x,y) = (<x,y> + c)**d\n",
    "    if scale:\n",
    "        \n",
    "        X_tr = (X_train-X_train.mean(axis=0)) / X_train.std(axis=0)\n",
    "        X_va = (X_valid-X_train.mean(axis=0)) / X_train.std(axis=0)\n",
    "        \n",
    "        K_va = X_va @ X_tr.T + c\n",
    "        K_va = K_va**d\n",
    "        \n",
    "        if mode==\"test\":\n",
    "            return(K_va)\n",
    "        \n",
    "        K_tr = X_tr @ X_tr.T + c\n",
    "        K_tr = K_tr**d\n",
    "    \n",
    "        \n",
    "    else:\n",
    "        K_va = X_valid @ X_train.T + c\n",
    "        K_va = K_va**d\n",
    "        \n",
    "        if mode==\"test\":\n",
    "            return(K_va)\n",
    "        \n",
    "        K_tr = X_train @ X_train.T + c\n",
    "        K_tr = K_tr**d\n",
    "        \n",
    "    return(K_tr, K_va)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "common-exclusion",
   "metadata": {},
   "source": [
    "## Kernel Ridge Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "appreciated-november",
   "metadata": {},
   "source": [
    "* Consider RKHS $\\mathcal H$, associated to a p.d. kernel K on $\\mathcal X$\n",
    "* Let $y = (y_1, \\dots, y_n)^T \\in \\mathbb R ^n$\n",
    "* Let $\\alpha = (\\alpha_1, \\dots, \\alpha_n)^T \\in \\mathbb R ^n$\n",
    "* Let $K$ be the $n\\times n$ Gram Matrix such that $K_{i,j} = K(x_i, x_j)$\n",
    "* We can then write\n",
    "$$\n",
    "(\\hat f(x_1), \\dots, \\hat f(x_n))^T = K\\alpha\n",
    "$$\n",
    "* The norm is $||\\hat f||^2_{\\mathcal H} = \\alpha^T K \\alpha$\n",
    "* KRR $\\leftrightarrow \\text{argmin}_{\\alpha \\in \\mathbb R^n} \\frac{1}{n} (K\\alpha - y)^T(K\\alpha - y) + \\lambda \\alpha^T K \\alpha$\n",
    "* Solution for $\\lambda > 0$:\n",
    "$$\n",
    "\\alpha = (K+\\lambda nI)^{-1}y\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "scientific-andrews",
   "metadata": {},
   "outputs": [],
   "source": [
    "def KRR(K, y, Kval, yval, lambd):\n",
    "    \"\"\"\n",
    "    takes the kernel matrix as an input and computes the MSE and the predictions for each value in lambd (list)\n",
    "    \"\"\"\n",
    "    assert K.shape[0] == y.shape[0]\n",
    "    assert len(lambd) > 0\n",
    "    n = K.shape[0]\n",
    "    \n",
    "    loss = []\n",
    "    acc = []\n",
    "    \n",
    "    loss_val = []\n",
    "    acc_val = []\n",
    "    alphas = []\n",
    "    \n",
    "    for l in lambd:\n",
    "        \n",
    "        assert l >= 0\n",
    "        # find the parameter alpha\n",
    "        alpha = np.linalg.solve((K + l*n*np.eye(n)), y)\n",
    "        # predict\n",
    "        \n",
    "        loss_lambda = MSE(K, y, l, alpha)\n",
    "        acc_lambda = accuracy(y,K@alpha, mode=\"KRR\")\n",
    "        \n",
    "        loss_lambdaval = MSE(Kval, yval, l, alpha, valid=True)\n",
    "        acc_lambdaval = accuracy(yval,Kval@alpha, mode=\"KRR\")\n",
    "\n",
    "        print(f\"***********lambda = {l}***********\")\n",
    "        print(f\"Training: loss = {loss_lambda:.4f}, accuracy = {acc_lambda:.6f}\")\n",
    "        print(f\"Validation: loss = {loss_lambdaval:.4f}, accuracy = {acc_lambdaval:.6f}\")\n",
    "        \n",
    "        loss += [loss_lambda]\n",
    "        acc += [acc_lambda]\n",
    "        \n",
    "        loss_val += [loss_lambdaval]\n",
    "        acc_val += [acc_lambdaval]\n",
    "        \n",
    "        \n",
    "        alphas +=[alpha]\n",
    "        \n",
    "    return(alphas, loss, acc, loss_val, acc_val)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "progressive-respect",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MSE(K, y, lambd, alpha, valid=False):\n",
    "    n = y.shape[0]\n",
    "    data_term = (np.linalg.norm(np.dot(K, alpha.reshape(-1,1)) - y)**2)/n\n",
    "    if not valid:\n",
    "        data_term += alpha @ K @ alpha\n",
    "    return(data_term)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spatial-purple",
   "metadata": {},
   "source": [
    "## Kernel Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "marine-association",
   "metadata": {},
   "source": [
    "- Binary Classificaiton setup: $\\mathcal Y = \\{-1, 1\\}$\n",
    "- $\\mathcal l_{\\text{logistic}}(f(x),y) = -\\log p(y|f(x)) = \\log(1 + e^{-yf(x)})$ where $p(y|f(x)) = \\sigma(y(f(x))$\n",
    "\n",
    "Objective:\n",
    "\\begin{align*}\n",
    "\\hat f &= \\text{argmin}_{f\\in \\mathcal H} \\frac{1}{n} \\sum_{i=1}^n \\log(1+e^{-y_if(x_i)}) + \\frac{\\lambda}{2}||f||^2_{\\mathcal H}\\\\\n",
    "\\alpha &= \\text{argmin}_{\\alpha \\in \\mathbb R^n} \\frac{1}{n} \\sum_{i=1}^n \\log(1+e^{-y_i[K\\alpha]_i}) + \\frac{\\lambda}{2} \\alpha^T K \\alpha\n",
    "\\end{align*}\n",
    "\n",
    "We define the following fonctions and vectors:\n",
    "* $\\mathcal l _\\text{logistic}(u) = \\log(1+e^{-u})$\n",
    "* $\\mathcal l' _\\text{logistic}(u) = -\\sigma(-u)$\n",
    "* $\\mathcal l'' _\\text{logistic}(u) = \\sigma(u)\\sigma(-u)$\n",
    "\n",
    "* for $i = 1, \\dots, n$, $P_i(\\alpha) = \\mathcal l' _\\text{logistic}(y_i[K\\alpha]_i)$\n",
    "* for $i = 1, \\dots, n$, $W_i(\\alpha) = \\mathcal l'' _\\text{logistic}(y_i[K\\alpha]_i)$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\\begin{align*}\n",
    "J(\\alpha) &= \\frac{1}{n} \\sum_{i=1}^n \\log(1+e^{-y_i[K\\alpha]_i}) + \\frac{\\lambda}{2} \\alpha^T K \\alpha\\\\\n",
    "\\nabla J(\\alpha) &= \\frac{1}{n} KP(\\alpha) y + \\lambda K \\alpha \\quad \\text{where } P(\\alpha) = \\text{diag}(P_1(\\alpha), \\dots, P_n(\\alpha))\\\\\n",
    "\\nabla^2 J(\\alpha) &= \\frac{1}{n}KW(\\alpha)K+\\lambda K \\quad \\text{where } W(\\alpha) = \\text{diag}(W_1(\\alpha), \\dots, W_n(\\alpha))\n",
    "\\end{align*}\n",
    "\n",
    "We are interested in the quadratic approximation of $J$ near a point $\\alpha_0$:\n",
    "\\begin{align*}\n",
    "J_q(\\alpha) &= J(\\alpha_0) + (\\alpha - \\alpha_0)^T \\nabla J(\\alpha_0) + \\frac{1}{2} (\\alpha - \\alpha_0)^T \\nabla^2 J(\\alpha_0)(\\alpha - \\alpha_0)\\\\\n",
    "2J_q(\\alpha) &= -\\frac{2}{n} \\alpha^T KW(K\\alpha_0-W^{-1}Py)+\\frac{1}{n}\\alpha^TKWK\\alpha+ \\lambda\\alpha^TK\\alpha +C\\\\\n",
    "&= \\frac{1}{n} (K\\alpha - z)^TW(K\\alpha - z) + \\lambda\\alpha^TK\\alpha + C \\quad \\text{where} z = K\\alpha_0 - W^{-1} P y\n",
    "\\end{align*}\n",
    "\n",
    "The WKRR problem is presented as:\n",
    "$$\n",
    "\\text{argmin}_{\\alpha \\in \\mathbb R^n} \\frac{1}{n}(K\\alpha - y)^TW(K\\alpha - y) + \\lambda \\alpha^TK\\alpha\n",
    "$$\n",
    "and has as solution:\n",
    "$$\n",
    "\\alpha = W^{1/2} (W^{1/2}KW^{1/2}+n\\lambda I)^{-1} W^{1/2}y\n",
    "$$\n",
    "\n",
    "So, in order to solve KRL, we use IRLS on a WKRR problem until convergence:\n",
    "$$\\alpha^{t+1} \\gets \\text{solveWKRR}(K, W^t, z^t)$$\n",
    "With the updates for $W^t$ and $z^t$ from $\\alpha^t$ are:\n",
    "- $m_i \\gets [K\\alpha^t]_i$\n",
    "- $P_i^t \\gets -\\sigma(-y_im_i)$\n",
    "- $W_i^t \\gets \\sigma(m_i)\\sigma(-m_i)$\n",
    "- $z_i^t \\gets m_i + y_i / \\sigma(-y_im_i)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "demonstrated-vitamin",
   "metadata": {},
   "source": [
    "We can rewrite the WKRR problem as:\n",
    "\n",
    "$$\n",
    "\\text{argmin}_{\\alpha \\in \\mathbb R^n} = \\frac{1}{2}\\alpha^T(\\frac{2}{n} KWK + 2\\lambda K)\\alpha + (-\\frac{2}{n}KWy)^T\\alpha\n",
    "$$\n",
    "which is a quadratic program with no constraints that can be solved using cvxopt tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "vocational-logistics",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "def logistic_loss(y_true, y_pred):\n",
    "    n = y_true.shape[0]\n",
    "    log_term = np.log(sigmoid(y_true*y_pred))\n",
    "    return(-np.sum(log_term)/n)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "headed-adrian",
   "metadata": {},
   "outputs": [],
   "source": [
    "def solveWKRR(K, W_t, z_t, y_, l):\n",
    "    n = K.shape[0]\n",
    "    W_sq = np.sqrt(W_t)\n",
    "    sol = np.linalg.solve(W_sq @ K @ W_sq + n* l * np.eye(n), W_sq@y_)\n",
    "    return(W_sq @ sol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "marine-trinidad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def KLR(K, y, Kval, yval, lambd, maxIter = 100, tresh = 1e-8):\n",
    "    \n",
    "    # initialize the values\n",
    "    assert K.shape[0] == y.shape[0]\n",
    "    n = K.shape[0]\n",
    "    n_val = Kval.shape[0]\n",
    "    \n",
    "    y_ = np.ones(n)\n",
    "    yval_ = np.ones(n_val)\n",
    "    \n",
    "    y_[y == 0] = -1\n",
    "    yval_[yval == 0] = -1\n",
    "    \n",
    "    \n",
    "    loss = []\n",
    "    acc = []\n",
    "    \n",
    "    loss_val = []\n",
    "    acc_val = []\n",
    "    \n",
    "    \n",
    "    alphas = []\n",
    "    \n",
    "    for l in lambd :\n",
    "        cnt = 0\n",
    "        \n",
    "        P_t, W_t = np.eye(n), np.eye(n)\n",
    "        z_t = K@ np.ones(n) - y_\n",
    "        alpha_t = np.ones(n)\n",
    "        diff_alpha = np.inf\n",
    "\n",
    "\n",
    "        while (diff_alpha > tresh) and (cnt < maxIter):\n",
    "\n",
    "            old_alpha = alpha_t\n",
    "            \n",
    "            ## Solving dual using CVXOpt\n",
    "            #P = matrix(2*((K @ W_t @ K)/n + l*K))\n",
    "            #q = matrix((-2*K@W_t@y_)/n)\n",
    "            #solvers.options['show_progress'] = False\n",
    "            #sol=solvers.qp(P, q)\n",
    "            #alpha_t = sol['x']\n",
    "            #alpha_t = np.reshape(alpha_t,-1)  \n",
    "            \n",
    "            alpha_t = solveWKRR(K, W_t, z_t, y_, l)\n",
    "\n",
    "            m_t = K@alpha_t\n",
    "            sigma_m = sigmoid(m_t)\n",
    "            sigma_my = sigmoid(-y_*m_t)\n",
    "\n",
    "            P_t = - np.diag(sigma_my)\n",
    "            W_t = np.diag(sigma_m * (1-sigma_m))\n",
    "\n",
    "            z_t = m_t - (P_t@y_)/(sigma_m * (1-sigma_m))\n",
    "\n",
    "            diff_alpha = np.linalg.norm(alpha_t - old_alpha)\n",
    "            cnt+=1\n",
    "            if cnt % 10 == 0:\n",
    "                print(l, cnt)\n",
    "        \n",
    "        loss_lambda = logistic_loss(y_, K@alpha_t)\n",
    "        acc_lambda = accuracy(y,K@alpha_t, mode=\"SVM\")\n",
    "        \n",
    "        loss_lambdaval = logistic_loss(yval_, Kval@alpha_t)\n",
    "        acc_lambdaval = accuracy(yval,Kval@alpha_t, mode=\"SVM\")\n",
    "\n",
    "        \n",
    "        print(f\"***********lambda = {l}***********\")\n",
    "        print(f\"Training: loss = {loss_lambda:.4f}, accuracy = {acc_lambda:.6f}\")\n",
    "        print(f\"Validation: loss = {loss_lambdaval:.4f}, accuracy = {acc_lambdaval:.6f}\")\n",
    "        \n",
    "        \n",
    "        loss += [loss_lambda]\n",
    "        acc += [acc_lambda]\n",
    "        \n",
    "        loss_val += [loss_lambdaval]\n",
    "        acc_val += [acc_lambdaval]\n",
    "        \n",
    "        alphas +=[alpha_t]\n",
    "        \n",
    "    return(alphas, loss, acc, loss_val, acc_val)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "light-recall",
   "metadata": {},
   "source": [
    "## Support Vector Machine approach (SVM)\n",
    "\n",
    "- Binary Classificaiton setup: $\\mathcal Y = \\{-1, 1\\}$\n",
    "- $\\mathcal l_{\\text{hinge}}(f(x),y) = \\max(1- y f(x), 0)$\n",
    "\n",
    "Objective:\n",
    "\\begin{align*}\n",
    "\\hat f &= \\text{argmin}_{f\\in \\mathcal H} \\frac{1}{n} \\sum_{i=1}^n \\max(1- y_i f(x_i), 0) + \\lambda||f||^2_{\\mathcal H}\\\\\n",
    "\\alpha &= \\text{argmin}_{\\alpha \\in \\mathbb R^n} \\frac{1}{n} \\sum_{i=1}^n \\max(y_i[K\\alpha]_i, 0) + \\lambda \\alpha^T K \\alpha\n",
    "\\end{align*}\n",
    "\n",
    "It is a convex optimization problem but the objective is not smooth.\n",
    "\n",
    "By introducing additional slack variables $\\xi_i$, the problem's objective becomes smooth but it is not the case for the constraints anymore. Let us solve the dual formulation instead (which is sparse, leading to faster algorithms). \n",
    "\n",
    "The dual can be rewritten as a quadratic minimization under box constraints : \n",
    "\n",
    "\\begin{align*}\n",
    "\\min_{\\alpha \\in \\mathbb R^n} \\frac{1}{2} \\alpha^TK\\alpha - \\alpha^T y\\\\\n",
    "\\text{s.t. }  0\\leq y_i\\alpha_i\\leq \\frac{1}{2\\lambda n}\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "We will solve it using CVXOpt tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "defensive-harbor",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hinge_loss(y_true, y_pred):\n",
    "    n = y_true.shape[0]\n",
    "    term = np.maximum(1-y_true*y_pred, 0)\n",
    "    return(np.sum(term)/n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "representative-revolution",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Je pense qu'une plus belle façon de faire serait de créer des fonctions de \n",
    "## kernel(X, sigma) et de les appeler avec en paramètres (X_train ou X_val) selon si on \n",
    "## fait le training ou la validation, pour pas avoir à garder en mémoire les kernels train/val\n",
    "## comme on le fait jusqu'à maintenant\n",
    "## Mais bon, là il est 2h47 du matin, j'ai un peu la flemme et j'imagine que toi aussi,\n",
    "## ça marche déjà bien comme ça ^^\n",
    "\n",
    "def _gaussian_kernel(sigma=1):\n",
    "    \"\"\"\n",
    "    Prepares a Gaussian RBF kernel using the provided sigma.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    kernel_function: A callable to the Gaussian RBF kernel function.\n",
    "\n",
    "    \"\"\"\n",
    "    gamma = -1 / (2 * sigma ** 2)\n",
    "    kernel_function = lambda X, y: np.exp(gamma * np.square(X[:, np.newaxis] - y).sum(axis=2))\n",
    "    return kernel_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "norman-firewall",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cvxopt import matrix, solvers\n",
    "\n",
    "def SVM(K, y, K_val, y_val, lambd):\n",
    "    # takes y with values in 0, 1 which need to be turnt into -1,1\n",
    "    # initialize the values\n",
    "    assert K.shape[0] == y.shape[0]\n",
    "    n = K.shape[0]\n",
    "    n_val = K_val.shape[0]\n",
    "    \n",
    "    y_ = np.ones(n)\n",
    "    yval_ = np.ones(n_val)\n",
    "    \n",
    "    y_[y == 0] = -1\n",
    "    yval_[y_val == 0] = -1\n",
    "    \n",
    "    y_preds, y_preds_val = [], []\n",
    "    losses, losses_val = [], []\n",
    "    accuracies, accuracies_val = [], []\n",
    "    alphas = []\n",
    "    \n",
    "    \n",
    "    for l in lambd :\n",
    "\n",
    "        ## Solving dual using CVXOpt\n",
    "        P = matrix(K)\n",
    "        q = matrix(-y_)\n",
    "        D = np.diag(-y_)\n",
    "        G = matrix(np.vstack((D,-D)))\n",
    "        h = matrix(np.concatenate((np.zeros(n), 1/(2*l*n) * np.ones(n)), axis=0))\n",
    "        solvers.options['show_progress'] = False\n",
    "        sol=solvers.qp(P, q, G, h)\n",
    "        alpha = sol['x']\n",
    "        alpha = np.reshape(alpha,-1)               \n",
    "\n",
    "        ## predictions\n",
    "        # training\n",
    "        pred_l = K @ alpha\n",
    "        y_preds += [pred_l]\n",
    "        loss_l = hinge_loss(y_, pred_l)\n",
    "        acc_l = accuracy(y, pred_l, mode=\"SVM\")\n",
    "\n",
    "        \n",
    "        # validation\n",
    "        pred_l_val = K_val@alpha\n",
    "        y_preds_val += [pred_l_val]\n",
    "        loss_l_val = hinge_loss(yval_, pred_l_val)\n",
    "        acc_l_val = accuracy(y_val,pred_l_val, mode=\"SVM\")\n",
    "        \n",
    "\n",
    "        print(15*\"-\", f\" lambda = {l} \", 15*\"-\")\n",
    "        print(f\"Training: loss = {loss_l:.6f}, accuracy = {acc_l:.6f}\")\n",
    "        print(f\"Validation: loss = {loss_l_val:.6f}, accuracy = {acc_l_val:.6f}\")\n",
    "        \n",
    "        losses += [loss_l]\n",
    "        accuracies += [acc_l]\n",
    "        \n",
    "        losses_val += [loss_l_val]\n",
    "        accuracies_val += [acc_l_val]\n",
    "    \n",
    "        alphas +=[alpha] \n",
    "        \n",
    "    return(alphas, losses, accuracies, losses_val, accuracies_val)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proud-northeast",
   "metadata": {},
   "source": [
    "# Testing the accuracy on mat100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "excellent-module",
   "metadata": {},
   "source": [
    "## Splitting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "criminal-crime",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "Xtr0, Xval0, ytr0, yval0 = train_test_split(Xtr0_mat100, Ytr0, test_size=0.2, random_state=42)\n",
    "Xtr1, Xval1, ytr1, yval1 = train_test_split(Xtr1_mat100, Ytr1, test_size=0.2, random_state=42)\n",
    "Xtr2, Xval2, ytr2, yval2 = train_test_split(Xtr2_mat100, Ytr2, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cheap-lighter",
   "metadata": {},
   "source": [
    "## Create the kernel matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "enabling-jacob",
   "metadata": {},
   "outputs": [],
   "source": [
    "K_tr0_ln, K_val0_ln = linear_kernel(Xtr0, Xval0)\n",
    "K_tr1_ln, K_val1_ln = linear_kernel(Xtr1, Xval1)\n",
    "K_tr2_ln, K_val2_ln = linear_kernel(Xtr2, Xval2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "convinced-warner",
   "metadata": {},
   "outputs": [],
   "source": [
    "K_tr0, K_val0 = gaussian_kernel(Xtr0, Xval0, sigma= 0.01, scale = False )\n",
    "K_tr1, K_val1 = gaussian_kernel(Xtr1, Xval1, sigma= 0.01, scale = False)\n",
    "K_tr2, K_val2 = gaussian_kernel(Xtr2, Xval2, sigma= 0.01, scale = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "unlikely-armstrong",
   "metadata": {},
   "outputs": [],
   "source": [
    "K_tr0_poly, K_val0_poly = polynomial_kernel(Xtr0, Xval0, d=3, c=1)\n",
    "K_tr1_poly, K_val1_poly = polynomial_kernel(Xtr1, Xval1, d=3, c=1)\n",
    "K_tr2_poly, K_val2_poly = polynomial_kernel(Xtr2, Xval2, d=3, c=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "violent-mortgage",
   "metadata": {},
   "source": [
    "## Testing KRR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "burning-supervision",
   "metadata": {},
   "source": [
    "### Gaussian Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "surrounded-revision",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************* KRR for dataset 0*************\n",
      "\n",
      "***********lambda = 0***********\n",
      "Training: loss = 1393.3497, accuracy = 1.000000\n",
      "Validation: loss = 123.8880, accuracy = 0.605000\n",
      "***********lambda = 1e-10***********\n",
      "Training: loss = 1393.3492, accuracy = 1.000000\n",
      "Validation: loss = 123.8880, accuracy = 0.605000\n",
      "***********lambda = 1e-09***********\n",
      "Training: loss = 1393.3442, accuracy = 1.000000\n",
      "Validation: loss = 123.8880, accuracy = 0.605000\n",
      "***********lambda = 1e-08***********\n",
      "Training: loss = 1393.2947, accuracy = 1.000000\n",
      "Validation: loss = 123.8875, accuracy = 0.605000\n",
      "***********lambda = 1e-07***********\n",
      "Training: loss = 1392.7998, accuracy = 1.000000\n",
      "Validation: loss = 123.8833, accuracy = 0.605000\n",
      "***********lambda = 1e-06***********\n",
      "Training: loss = 1387.8773, accuracy = 1.000000\n",
      "Validation: loss = 123.8409, accuracy = 0.602500\n",
      "***********lambda = 1e-05***********\n",
      "Training: loss = 1341.1438, accuracy = 1.000000\n",
      "Validation: loss = 123.4334, accuracy = 0.600000\n",
      "***********lambda = 0.0001***********\n",
      "Training: loss = 1032.5109, accuracy = 1.000000\n",
      "Validation: loss = 120.5009, accuracy = 0.620000\n",
      "***********lambda = 0.001***********\n",
      "Training: loss = 532.5706, accuracy = 0.940625\n",
      "Validation: loss = 113.5734, accuracy = 0.632500\n",
      "***********lambda = 0.01***********\n",
      "Training: loss = 439.7529, accuracy = 0.636875\n",
      "Validation: loss = 111.8079, accuracy = 0.560000\n",
      "***********lambda = 0.1***********\n",
      "Training: loss = 522.4912, accuracy = 0.524375\n",
      "Validation: loss = 137.3617, accuracy = 0.497500\n",
      "***********lambda = 1***********\n",
      "Training: loss = 707.9997, accuracy = 0.524375\n",
      "Validation: loss = 187.0968, accuracy = 0.497500\n",
      "***********lambda = 10***********\n",
      "Training: loss = 755.0342, accuracy = 0.524375\n",
      "Validation: loss = 199.4390, accuracy = 0.497500\n",
      "************* KRR for dataset 1*************\n",
      "\n",
      "***********lambda = 0***********\n",
      "Training: loss = 1411.2363, accuracy = 1.000000\n",
      "Validation: loss = 120.7098, accuracy = 0.610000\n",
      "***********lambda = 1e-10***********\n",
      "Training: loss = 1411.2357, accuracy = 1.000000\n",
      "Validation: loss = 120.7098, accuracy = 0.610000\n",
      "***********lambda = 1e-09***********\n",
      "Training: loss = 1411.2308, accuracy = 1.000000\n",
      "Validation: loss = 120.7097, accuracy = 0.610000\n",
      "***********lambda = 1e-08***********\n",
      "Training: loss = 1411.1820, accuracy = 1.000000\n",
      "Validation: loss = 120.7093, accuracy = 0.610000\n",
      "***********lambda = 1e-07***********\n",
      "Training: loss = 1410.6934, accuracy = 1.000000\n",
      "Validation: loss = 120.7056, accuracy = 0.610000\n",
      "***********lambda = 1e-06***********\n",
      "Training: loss = 1405.8309, accuracy = 1.000000\n",
      "Validation: loss = 120.6679, accuracy = 0.610000\n",
      "***********lambda = 1e-05***********\n",
      "Training: loss = 1359.4137, accuracy = 1.000000\n",
      "Validation: loss = 120.3057, accuracy = 0.602500\n",
      "***********lambda = 0.0001***********\n",
      "Training: loss = 1045.3600, accuracy = 1.000000\n",
      "Validation: loss = 117.7019, accuracy = 0.607500\n",
      "***********lambda = 0.001***********\n",
      "Training: loss = 524.5869, accuracy = 0.953125\n",
      "Validation: loss = 111.8549, accuracy = 0.647500\n",
      "***********lambda = 0.01***********\n",
      "Training: loss = 440.9524, accuracy = 0.610625\n",
      "Validation: loss = 111.7749, accuracy = 0.592500\n",
      "***********lambda = 0.1***********\n",
      "Training: loss = 531.7974, accuracy = 0.505000\n",
      "Validation: loss = 138.5132, accuracy = 0.477500\n",
      "***********lambda = 1***********\n",
      "Training: loss = 733.6861, accuracy = 0.505000\n",
      "Validation: loss = 193.4084, accuracy = 0.477500\n",
      "***********lambda = 10***********\n",
      "Training: loss = 785.4277, accuracy = 0.505000\n",
      "Validation: loss = 207.2462, accuracy = 0.477500\n",
      "************* KRR for dataset 2*************\n",
      "\n",
      "***********lambda = 0***********\n",
      "Training: loss = 1299.2468, accuracy = 1.000000\n",
      "Validation: loss = 135.9798, accuracy = 0.675000\n",
      "***********lambda = 1e-10***********\n",
      "Training: loss = 1299.2464, accuracy = 1.000000\n",
      "Validation: loss = 135.9798, accuracy = 0.675000\n",
      "***********lambda = 1e-09***********\n",
      "Training: loss = 1299.2424, accuracy = 1.000000\n",
      "Validation: loss = 135.9798, accuracy = 0.675000\n",
      "***********lambda = 1e-08***********\n",
      "Training: loss = 1299.2025, accuracy = 1.000000\n",
      "Validation: loss = 135.9794, accuracy = 0.675000\n",
      "***********lambda = 1e-07***********\n",
      "Training: loss = 1298.8043, accuracy = 1.000000\n",
      "Validation: loss = 135.9755, accuracy = 0.675000\n",
      "***********lambda = 1e-06***********\n",
      "Training: loss = 1294.8469, accuracy = 1.000000\n",
      "Validation: loss = 135.9370, accuracy = 0.675000\n",
      "***********lambda = 1e-05***********\n",
      "Training: loss = 1257.3629, accuracy = 1.000000\n",
      "Validation: loss = 135.5524, accuracy = 0.675000\n",
      "***********lambda = 0.0001***********\n",
      "Training: loss = 1004.3126, accuracy = 1.000000\n",
      "Validation: loss = 132.4989, accuracy = 0.682500\n",
      "***********lambda = 0.001***********\n",
      "Training: loss = 566.7094, accuracy = 0.945000\n",
      "Validation: loss = 122.5976, accuracy = 0.685000\n",
      "***********lambda = 0.01***********\n",
      "Training: loss = 452.3324, accuracy = 0.671875\n",
      "Validation: loss = 109.7329, accuracy = 0.642500\n",
      "***********lambda = 0.1***********\n",
      "Training: loss = 540.2734, accuracy = 0.492500\n",
      "Validation: loss = 125.2078, accuracy = 0.537500\n",
      "***********lambda = 1***********\n",
      "Training: loss = 752.4483, accuracy = 0.492500\n",
      "Validation: loss = 171.5655, accuracy = 0.537500\n",
      "***********lambda = 10***********\n",
      "Training: loss = 805.3161, accuracy = 0.492500\n",
      "Validation: loss = 183.4867, accuracy = 0.537500\n"
     ]
    }
   ],
   "source": [
    "lambdas = [0] + [10**i for i in range(-10,2)]\n",
    "print(\"************* KRR for dataset 0*************\\n\")\n",
    "alphas_tr0, loss_tr0, acc_0, loss_val0, acc_val0 = KRR(K_tr0, ytr0[:,1], K_val0, yval0[:,1], lambdas)\n",
    "print(\"************* KRR for dataset 1*************\\n\")\n",
    "alphas_tr1, loss_tr1, acc_1, loss_val1, acc_val1 = KRR(K_tr1, ytr1[:,1], K_val1, yval1[:,1],lambdas)\n",
    "print(\"************* KRR for dataset 2*************\\n\")\n",
    "alphas_tr2, loss_tr2, acc_2, loss_val2, acc_val2 = KRR(K_tr2, ytr2[:,1], K_val2, yval2[:,1],lambdas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "diverse-appendix",
   "metadata": {},
   "source": [
    "### Linear Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "certain-legislature",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************* KRR for dataset 0*************\n",
      "\n",
      "***********lambda = 0***********\n",
      "Training: loss = -263107891951975328.0000, accuracy = 0.502500\n",
      "Validation: loss = 4536425.7539, accuracy = 0.491875\n",
      "***********lambda = 1e-10***********\n",
      "Training: loss = 8732.8964, accuracy = 0.562500\n",
      "Validation: loss = 959.2869, accuracy = 0.520625\n",
      "***********lambda = 1e-09***********\n",
      "Training: loss = 303.9139, accuracy = 0.562500\n",
      "Validation: loss = 959.2904, accuracy = 0.520625\n",
      "***********lambda = 1e-08***********\n",
      "Training: loss = 218.5145, accuracy = 0.562500\n",
      "Validation: loss = 959.2907, accuracy = 0.520625\n",
      "***********lambda = 1e-07***********\n",
      "Training: loss = 217.6592, accuracy = 0.562500\n",
      "Validation: loss = 959.2907, accuracy = 0.520625\n",
      "***********lambda = 1e-06***********\n",
      "Training: loss = 217.6509, accuracy = 0.562500\n",
      "Validation: loss = 959.2900, accuracy = 0.520625\n",
      "***********lambda = 1e-05***********\n",
      "Training: loss = 217.6499, accuracy = 0.562500\n",
      "Validation: loss = 959.2826, accuracy = 0.520625\n",
      "***********lambda = 0.0001***********\n",
      "Training: loss = 217.6405, accuracy = 0.562500\n",
      "Validation: loss = 959.2087, accuracy = 0.520625\n",
      "***********lambda = 0.001***********\n",
      "Training: loss = 217.5465, accuracy = 0.562500\n",
      "Validation: loss = 958.4747, accuracy = 0.521250\n",
      "***********lambda = 0.01***********\n",
      "Training: loss = 216.6527, accuracy = 0.555000\n",
      "Validation: loss = 951.5485, accuracy = 0.521875\n",
      "***********lambda = 0.1***********\n",
      "Training: loss = 210.5142, accuracy = 0.547500\n",
      "Validation: loss = 906.7140, accuracy = 0.520000\n",
      "***********lambda = 1***********\n",
      "Training: loss = 195.7631, accuracy = 0.532500\n",
      "Validation: loss = 817.7409, accuracy = 0.515625\n",
      "***********lambda = 10***********\n",
      "Training: loss = 187.8474, accuracy = 0.532500\n",
      "Validation: loss = 779.3577, accuracy = 0.515625\n",
      "************* KRR for dataset 1*************\n",
      "\n",
      "***********lambda = 0***********\n",
      "Training: loss = -23059419639876632.0000, accuracy = 0.500000\n",
      "Validation: loss = 247643.4902, accuracy = 0.510000\n",
      "***********lambda = 1e-10***********\n",
      "Training: loss = -4505.0254, accuracy = 0.560000\n",
      "Validation: loss = 968.7287, accuracy = 0.510000\n",
      "***********lambda = 1e-09***********\n",
      "Training: loss = 153.7707, accuracy = 0.560000\n",
      "Validation: loss = 968.7275, accuracy = 0.510000\n",
      "***********lambda = 1e-08***********\n",
      "Training: loss = 209.4991, accuracy = 0.560000\n",
      "Validation: loss = 968.7273, accuracy = 0.510000\n",
      "***********lambda = 1e-07***********\n",
      "Training: loss = 210.0471, accuracy = 0.560000\n",
      "Validation: loss = 968.7272, accuracy = 0.510000\n",
      "***********lambda = 1e-06***********\n",
      "Training: loss = 210.0526, accuracy = 0.560000\n",
      "Validation: loss = 968.7261, accuracy = 0.510000\n",
      "***********lambda = 1e-05***********\n",
      "Training: loss = 210.0514, accuracy = 0.560000\n",
      "Validation: loss = 968.7147, accuracy = 0.510000\n",
      "***********lambda = 0.0001***********\n",
      "Training: loss = 210.0386, accuracy = 0.560000\n",
      "Validation: loss = 968.6013, accuracy = 0.510000\n",
      "***********lambda = 0.001***********\n",
      "Training: loss = 209.9122, accuracy = 0.560000\n",
      "Validation: loss = 967.4861, accuracy = 0.510000\n",
      "***********lambda = 0.01***********\n",
      "Training: loss = 208.7851, accuracy = 0.557500\n",
      "Validation: loss = 957.8350, accuracy = 0.508125\n",
      "***********lambda = 0.1***********\n",
      "Training: loss = 202.5222, accuracy = 0.545000\n",
      "Validation: loss = 911.2295, accuracy = 0.498750\n",
      "***********lambda = 1***********\n",
      "Training: loss = 190.0886, accuracy = 0.540000\n",
      "Validation: loss = 841.2765, accuracy = 0.489375\n",
      "***********lambda = 10***********\n",
      "Training: loss = 184.4703, accuracy = 0.540000\n",
      "Validation: loss = 817.9388, accuracy = 0.489375\n",
      "************* KRR for dataset 2*************\n",
      "\n",
      "***********lambda = 0***********\n",
      "Training: loss = -52485565679132528.0000, accuracy = 0.470000\n",
      "Validation: loss = 1235030.7493, accuracy = 0.456875\n",
      "***********lambda = 1e-10***********\n",
      "Training: loss = -23594.2109, accuracy = 0.562500\n",
      "Validation: loss = 985.0991, accuracy = 0.561875\n",
      "***********lambda = 1e-09***********\n",
      "Training: loss = 3.1970, accuracy = 0.562500\n",
      "Validation: loss = 985.0914, accuracy = 0.561875\n",
      "***********lambda = 1e-08***********\n",
      "Training: loss = 244.7822, accuracy = 0.562500\n",
      "Validation: loss = 985.0906, accuracy = 0.561875\n",
      "***********lambda = 1e-07***********\n",
      "Training: loss = 247.2356, accuracy = 0.562500\n",
      "Validation: loss = 985.0905, accuracy = 0.561875\n",
      "***********lambda = 1e-06***********\n",
      "Training: loss = 247.2593, accuracy = 0.562500\n",
      "Validation: loss = 985.0899, accuracy = 0.561875\n",
      "***********lambda = 1e-05***********\n",
      "Training: loss = 247.2586, accuracy = 0.562500\n",
      "Validation: loss = 985.0840, accuracy = 0.561875\n",
      "***********lambda = 0.0001***********\n",
      "Training: loss = 247.2493, accuracy = 0.562500\n",
      "Validation: loss = 985.0251, accuracy = 0.561875\n",
      "***********lambda = 0.001***********\n",
      "Training: loss = 247.1568, accuracy = 0.562500\n",
      "Validation: loss = 984.4395, accuracy = 0.561875\n",
      "***********lambda = 0.01***********\n",
      "Training: loss = 246.2712, accuracy = 0.560000\n",
      "Validation: loss = 978.8961, accuracy = 0.558750\n",
      "***********lambda = 0.1***********\n",
      "Training: loss = 239.9788, accuracy = 0.545000\n",
      "Validation: loss = 942.7331, accuracy = 0.541250\n",
      "***********lambda = 1***********\n",
      "Training: loss = 222.3441, accuracy = 0.490000\n",
      "Validation: loss = 866.0566, accuracy = 0.511875\n",
      "***********lambda = 10***********\n",
      "Training: loss = 206.9852, accuracy = 0.487500\n",
      "Validation: loss = 804.6139, accuracy = 0.505000\n"
     ]
    }
   ],
   "source": [
    "lambdas = [0] + [10**i for i in range(-10,2)]\n",
    "print(\"************* KRR for dataset 0*************\\n\")\n",
    "alphas_tr0, loss_tr0, acc_0, loss_val0, acc_val0 = KRR(K_tr0_ln, ytr0[:,1], K_val0_ln, yval0[:,1], lambdas)\n",
    "print(\"************* KRR for dataset 1*************\\n\")\n",
    "alphas_tr1, loss_tr1, acc_1, loss_val1, acc_val1 = KRR(K_tr1_ln, ytr1[:,1], K_val1_ln, yval1[:,1],lambdas)\n",
    "print(\"************* KRR for dataset 2*************\\n\")\n",
    "alphas_tr2, loss_tr2, acc_2, loss_val2, acc_val2 = KRR(K_tr2_ln, ytr2[:,1], K_val2_ln, yval2[:,1],lambdas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "photographic-parks",
   "metadata": {},
   "source": [
    "### Polynomial Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "complimentary-nickel",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************* KRR for dataset 0*************\n",
      "\n",
      "***********lambda = 0***********\n",
      "Training: loss = 199.1552, accuracy = 1.000000\n",
      "Validation: loss = 642.6999, accuracy = 0.500625\n",
      "***********lambda = 1e-10***********\n",
      "Training: loss = 199.1552, accuracy = 1.000000\n",
      "Validation: loss = 642.6999, accuracy = 0.500625\n",
      "***********lambda = 1e-09***********\n",
      "Training: loss = 199.1552, accuracy = 1.000000\n",
      "Validation: loss = 642.6999, accuracy = 0.500625\n",
      "***********lambda = 1e-08***********\n",
      "Training: loss = 199.1552, accuracy = 1.000000\n",
      "Validation: loss = 642.6999, accuracy = 0.500625\n",
      "***********lambda = 1e-07***********\n",
      "Training: loss = 199.1552, accuracy = 1.000000\n",
      "Validation: loss = 642.6999, accuracy = 0.500625\n",
      "***********lambda = 1e-06***********\n",
      "Training: loss = 199.1552, accuracy = 1.000000\n",
      "Validation: loss = 642.6999, accuracy = 0.500625\n",
      "***********lambda = 1e-05***********\n",
      "Training: loss = 199.1552, accuracy = 1.000000\n",
      "Validation: loss = 642.6999, accuracy = 0.500625\n",
      "***********lambda = 0.0001***********\n",
      "Training: loss = 199.1552, accuracy = 1.000000\n",
      "Validation: loss = 642.6999, accuracy = 0.500625\n",
      "***********lambda = 0.001***********\n",
      "Training: loss = 199.1551, accuracy = 1.000000\n",
      "Validation: loss = 642.6999, accuracy = 0.500625\n",
      "***********lambda = 0.01***********\n",
      "Training: loss = 199.1541, accuracy = 1.000000\n",
      "Validation: loss = 642.6999, accuracy = 0.500625\n",
      "***********lambda = 0.1***********\n",
      "Training: loss = 199.1441, accuracy = 1.000000\n",
      "Validation: loss = 642.7006, accuracy = 0.500625\n",
      "***********lambda = 1***********\n",
      "Training: loss = 199.0439, accuracy = 1.000000\n",
      "Validation: loss = 642.7073, accuracy = 0.500625\n",
      "***********lambda = 10***********\n",
      "Training: loss = 198.0569, accuracy = 1.000000\n",
      "Validation: loss = 642.7742, accuracy = 0.501875\n",
      "************* KRR for dataset 1*************\n",
      "\n",
      "***********lambda = 0***********\n",
      "Training: loss = 198.7203, accuracy = 1.000000\n",
      "Validation: loss = 653.4781, accuracy = 0.493750\n",
      "***********lambda = 1e-10***********\n",
      "Training: loss = 198.7203, accuracy = 1.000000\n",
      "Validation: loss = 653.4781, accuracy = 0.493750\n",
      "***********lambda = 1e-09***********\n",
      "Training: loss = 198.7203, accuracy = 1.000000\n",
      "Validation: loss = 653.4781, accuracy = 0.493750\n",
      "***********lambda = 1e-08***********\n",
      "Training: loss = 198.7203, accuracy = 1.000000\n",
      "Validation: loss = 653.4781, accuracy = 0.493750\n",
      "***********lambda = 1e-07***********\n",
      "Training: loss = 198.7203, accuracy = 1.000000\n",
      "Validation: loss = 653.4781, accuracy = 0.493750\n",
      "***********lambda = 1e-06***********\n",
      "Training: loss = 198.7203, accuracy = 1.000000\n",
      "Validation: loss = 653.4781, accuracy = 0.493750\n",
      "***********lambda = 1e-05***********\n",
      "Training: loss = 198.7203, accuracy = 1.000000\n",
      "Validation: loss = 653.4781, accuracy = 0.493750\n",
      "***********lambda = 0.0001***********\n",
      "Training: loss = 198.7202, accuracy = 1.000000\n",
      "Validation: loss = 653.4781, accuracy = 0.493750\n",
      "***********lambda = 0.001***********\n",
      "Training: loss = 198.7201, accuracy = 1.000000\n",
      "Validation: loss = 653.4781, accuracy = 0.493750\n",
      "***********lambda = 0.01***********\n",
      "Training: loss = 198.7190, accuracy = 1.000000\n",
      "Validation: loss = 653.4781, accuracy = 0.493750\n",
      "***********lambda = 0.1***********\n",
      "Training: loss = 198.7078, accuracy = 1.000000\n",
      "Validation: loss = 653.4779, accuracy = 0.493750\n",
      "***********lambda = 1***********\n",
      "Training: loss = 198.5958, accuracy = 1.000000\n",
      "Validation: loss = 653.4755, accuracy = 0.493750\n",
      "***********lambda = 10***********\n",
      "Training: loss = 197.4926, accuracy = 1.000000\n",
      "Validation: loss = 653.4569, accuracy = 0.493750\n",
      "************* KRR for dataset 2*************\n",
      "\n",
      "***********lambda = 0***********\n",
      "Training: loss = 199.8752, accuracy = 1.000000\n",
      "Validation: loss = 617.9111, accuracy = 0.546875\n",
      "***********lambda = 1e-10***********\n",
      "Training: loss = 199.8752, accuracy = 1.000000\n",
      "Validation: loss = 617.9111, accuracy = 0.546875\n",
      "***********lambda = 1e-09***********\n",
      "Training: loss = 199.8752, accuracy = 1.000000\n",
      "Validation: loss = 617.9111, accuracy = 0.546875\n",
      "***********lambda = 1e-08***********\n",
      "Training: loss = 199.8752, accuracy = 1.000000\n",
      "Validation: loss = 617.9111, accuracy = 0.546875\n",
      "***********lambda = 1e-07***********\n",
      "Training: loss = 199.8752, accuracy = 1.000000\n",
      "Validation: loss = 617.9111, accuracy = 0.546875\n",
      "***********lambda = 1e-06***********\n",
      "Training: loss = 199.8752, accuracy = 1.000000\n",
      "Validation: loss = 617.9111, accuracy = 0.546875\n",
      "***********lambda = 1e-05***********\n",
      "Training: loss = 199.8752, accuracy = 1.000000\n",
      "Validation: loss = 617.9111, accuracy = 0.546875\n",
      "***********lambda = 0.0001***********\n",
      "Training: loss = 199.8752, accuracy = 1.000000\n",
      "Validation: loss = 617.9111, accuracy = 0.546875\n",
      "***********lambda = 0.001***********\n",
      "Training: loss = 199.8751, accuracy = 1.000000\n",
      "Validation: loss = 617.9111, accuracy = 0.546875\n",
      "***********lambda = 0.01***********\n",
      "Training: loss = 199.8742, accuracy = 1.000000\n",
      "Validation: loss = 617.9113, accuracy = 0.546875\n",
      "***********lambda = 0.1***********\n",
      "Training: loss = 199.8655, accuracy = 1.000000\n",
      "Validation: loss = 617.9129, accuracy = 0.546875\n",
      "***********lambda = 1***********\n",
      "Training: loss = 199.7781, accuracy = 1.000000\n",
      "Validation: loss = 617.9294, accuracy = 0.546875\n",
      "***********lambda = 10***********\n",
      "Training: loss = 198.9152, accuracy = 1.000000\n",
      "Validation: loss = 618.0821, accuracy = 0.546250\n"
     ]
    }
   ],
   "source": [
    "lambdas = [0] + [10**i for i in range(-10,2)]\n",
    "print(\"************* KRR for dataset 0*************\\n\")\n",
    "alphas_tr0, loss_tr0, acc_0, loss_val0, acc_val0 = KRR(K_tr0_poly, ytr0[:,1], K_val0_poly, yval0[:,1], lambdas)\n",
    "print(\"************* KRR for dataset 1*************\\n\")\n",
    "alphas_tr1, loss_tr1, acc_1, loss_val1, acc_val1 = KRR(K_tr1_poly, ytr1[:,1], K_val1_poly, yval1[:,1],lambdas)\n",
    "print(\"************* KRR for dataset 2*************\\n\")\n",
    "alphas_tr2, loss_tr2, acc_2, loss_val2, acc_val2 = KRR(K_tr2_poly, ytr2[:,1], K_val2_poly, yval2[:,1],lambdas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brief-arkansas",
   "metadata": {},
   "source": [
    "### Conclusion : gaussian is better"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "modular-basics",
   "metadata": {},
   "source": [
    "## Testing KLR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "excited-association",
   "metadata": {},
   "source": [
    "### Gaussian Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "irish-rover",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*************KLR for dataset 0*************\n",
      "\n",
      "0.0001 10\n",
      "***********lambda = 0.0001***********\n",
      "Training: loss = 0.4589, accuracy = 0.995625\n",
      "Validation: loss = 0.6579, accuracy = 0.585000\n",
      "***********lambda = 0.001***********\n",
      "Training: loss = 0.6184, accuracy = 0.878125\n",
      "Validation: loss = 0.6681, accuracy = 0.590000\n",
      "***********lambda = 0.01***********\n",
      "Training: loss = 0.6791, accuracy = 0.726875\n",
      "Validation: loss = 0.6865, accuracy = 0.575000\n",
      "***********lambda = 0.1***********\n",
      "Training: loss = 0.6914, accuracy = 0.618750\n",
      "Validation: loss = 0.6923, accuracy = 0.550000\n",
      "***********lambda = 1***********\n",
      "Training: loss = 0.6930, accuracy = 0.584375\n",
      "Validation: loss = 0.6931, accuracy = 0.532500\n",
      "*************KLR for dataset 1*************\n",
      "\n",
      "0.0001 10\n",
      "***********lambda = 0.0001***********\n",
      "Training: loss = 0.4651, accuracy = 0.998750\n",
      "Validation: loss = 0.6645, accuracy = 0.620000\n",
      "***********lambda = 0.001***********\n",
      "Training: loss = 0.6312, accuracy = 0.895625\n",
      "Validation: loss = 0.6788, accuracy = 0.617500\n",
      "***********lambda = 0.01***********\n",
      "Training: loss = 0.6845, accuracy = 0.761875\n",
      "Validation: loss = 0.6903, accuracy = 0.592500\n",
      "***********lambda = 0.1***********\n",
      "Training: loss = 0.6922, accuracy = 0.706250\n",
      "Validation: loss = 0.6928, accuracy = 0.570000\n",
      "***********lambda = 1***********\n",
      "Training: loss = 0.6931, accuracy = 0.691875\n",
      "Validation: loss = 0.6931, accuracy = 0.570000\n",
      "*************KLR for dataset 2*************\n",
      "\n",
      "0.0001 10\n",
      "***********lambda = 0.0001***********\n",
      "Training: loss = 0.4384, accuracy = 0.998750\n",
      "Validation: loss = 0.6046, accuracy = 0.707500\n",
      "***********lambda = 0.001***********\n",
      "Training: loss = 0.5886, accuracy = 0.858750\n",
      "Validation: loss = 0.6296, accuracy = 0.702500\n",
      "***********lambda = 0.01***********\n",
      "Training: loss = 0.6677, accuracy = 0.760625\n",
      "Validation: loss = 0.6725, accuracy = 0.685000\n",
      "***********lambda = 0.1***********\n",
      "Training: loss = 0.6900, accuracy = 0.738750\n",
      "Validation: loss = 0.6904, accuracy = 0.672500\n",
      "***********lambda = 1***********\n",
      "Training: loss = 0.6928, accuracy = 0.735625\n",
      "Validation: loss = 0.6929, accuracy = 0.675000\n"
     ]
    }
   ],
   "source": [
    "lambdas = [10**i for i in range(-4,1)]\n",
    "\n",
    "print(\"*************KLR for dataset 0*************\\n\")\n",
    "alphas_tr0_klr, loss_tr0_klr, acc_0_klr, loss_val0_klr, acc_val0_klr = KLR(K_tr0, ytr0[:,1], K_val0, yval0[:,1], lambdas, tresh=1e-8)\n",
    "print(\"*************KLR for dataset 1*************\\n\")\n",
    "alphas_tr1_klr, loss_tr1_klr, acc_1_klr, loss_val1_klr, acc_val1_klr = KLR(K_tr1, ytr1[:,1], K_val1, yval1[:,1], lambdas, tresh=1e-8)\n",
    "print(\"*************KLR for dataset 2*************\\n\")\n",
    "alphas_tr2_klr, loss_tr2_klr, acc_2_klr, loss_val2_klr, acc_val2_klr = KLR(K_tr2, ytr2[:,1], K_val2, yval2[:,1], lambdas, tresh=1e-8)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exciting-humanity",
   "metadata": {},
   "source": [
    "### Linear Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "turkish-legend",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*************KLR for dataset 0*************\n",
      "\n",
      "0.0001 10\n",
      "***********lambda = 0.0001***********\n",
      "Training: loss = 0.5748, accuracy = 0.742500\n",
      "Validation: loss = 0.7005, accuracy = 0.556250\n",
      "0.001 10\n",
      "***********lambda = 0.001***********\n",
      "Training: loss = 0.5755, accuracy = 0.742500\n",
      "Validation: loss = 0.6997, accuracy = 0.555625\n",
      "***********lambda = 0.01***********\n",
      "Training: loss = 0.5813, accuracy = 0.745000\n",
      "Validation: loss = 0.6938, accuracy = 0.563125\n",
      "***********lambda = 0.1***********\n",
      "Training: loss = 0.6102, accuracy = 0.730000\n",
      "Validation: loss = 0.6806, accuracy = 0.566875\n",
      "***********lambda = 1***********\n",
      "Training: loss = 0.6607, accuracy = 0.680000\n",
      "Validation: loss = 0.6834, accuracy = 0.558125\n",
      "*************KLR for dataset 1*************\n",
      "\n",
      "0.0001 10\n",
      "***********lambda = 0.0001***********\n",
      "Training: loss = 0.5937, accuracy = 0.720000\n",
      "Validation: loss = 0.7062, accuracy = 0.543125\n",
      "***********lambda = 0.001***********\n",
      "Training: loss = 0.5945, accuracy = 0.720000\n",
      "Validation: loss = 0.7049, accuracy = 0.541875\n",
      "***********lambda = 0.01***********\n",
      "Training: loss = 0.6011, accuracy = 0.705000\n",
      "Validation: loss = 0.6978, accuracy = 0.541875\n",
      "***********lambda = 0.1***********\n",
      "Training: loss = 0.6281, accuracy = 0.695000\n",
      "Validation: loss = 0.6862, accuracy = 0.538750\n",
      "***********lambda = 1***********\n",
      "Training: loss = 0.6700, accuracy = 0.685000\n",
      "Validation: loss = 0.6879, accuracy = 0.541250\n",
      "*************KLR for dataset 2*************\n",
      "\n",
      "0.0001 10\n",
      "0.0001 20\n",
      "0.0001 30\n",
      "0.0001 40\n",
      "***********lambda = 0.0001***********\n",
      "Training: loss = 0.5256, accuracy = 0.805000\n",
      "Validation: loss = 0.6298, accuracy = 0.661250\n",
      "0.001 10\n",
      "0.001 20\n",
      "0.001 30\n",
      "***********lambda = 0.001***********\n",
      "Training: loss = 0.5265, accuracy = 0.810000\n",
      "Validation: loss = 0.6291, accuracy = 0.661250\n",
      "0.01 10\n",
      "***********lambda = 0.01***********\n",
      "Training: loss = 0.5337, accuracy = 0.802500\n",
      "Validation: loss = 0.6250, accuracy = 0.666250\n",
      "***********lambda = 0.1***********\n",
      "Training: loss = 0.5669, accuracy = 0.782500\n",
      "Validation: loss = 0.6224, accuracy = 0.685625\n",
      "***********lambda = 1***********\n",
      "Training: loss = 0.6323, accuracy = 0.730000\n",
      "Validation: loss = 0.6500, accuracy = 0.690625\n"
     ]
    }
   ],
   "source": [
    "lambdas = [10**i for i in range(-4,1)]\n",
    "\n",
    "print(\"*************KLR for dataset 0*************\\n\")\n",
    "alphas_tr0_klr, loss_tr0_klr, acc_0_klr, loss_val0_klr, acc_val0_klr = KLR(K_tr0_ln, ytr0[:,1], K_val0_ln, yval0[:,1], lambdas, tresh=1e-5)\n",
    "print(\"*************KLR for dataset 1*************\\n\")\n",
    "alphas_tr1_klr, loss_tr1_klr, acc_1_klr, loss_val1_klr, acc_val1_klr = KLR(K_tr1_ln, ytr1[:,1], K_val1_ln, yval1[:,1], lambdas, tresh=1e-5)\n",
    "print(\"*************KLR for dataset 2*************\\n\")\n",
    "alphas_tr2_klr, loss_tr2_klr, acc_2_klr, loss_val2_klr, acc_val2_klr = KLR(K_tr2_ln, ytr2[:,1], K_val2_ln, yval2[:,1], lambdas, tresh=1e-5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abstract-institute",
   "metadata": {},
   "source": [
    "### Polynomial kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "framed-black",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*************KLR for dataset 0*************\n",
      "\n",
      "***********lambda = 0.0001***********\n",
      "Training: loss = 0.3133, accuracy = 1.000000\n",
      "Validation: loss = 0.6741, accuracy = 0.567500\n",
      "***********lambda = 0.001***********\n",
      "Training: loss = 0.3133, accuracy = 1.000000\n",
      "Validation: loss = 0.6741, accuracy = 0.567500\n",
      "***********lambda = 0.01***********\n",
      "Training: loss = 0.3133, accuracy = 1.000000\n",
      "Validation: loss = 0.6741, accuracy = 0.567500\n",
      "***********lambda = 0.1***********\n",
      "Training: loss = 0.3133, accuracy = 1.000000\n",
      "Validation: loss = 0.6741, accuracy = 0.567500\n",
      "***********lambda = 1***********\n",
      "Training: loss = 0.3140, accuracy = 1.000000\n",
      "Validation: loss = 0.6741, accuracy = 0.567500\n",
      "*************KLR for dataset 1*************\n",
      "\n",
      "***********lambda = 0.0001***********\n",
      "Training: loss = 0.3133, accuracy = 1.000000\n",
      "Validation: loss = 0.6826, accuracy = 0.523125\n",
      "***********lambda = 0.001***********\n",
      "Training: loss = 0.3133, accuracy = 1.000000\n",
      "Validation: loss = 0.6826, accuracy = 0.523125\n",
      "***********lambda = 0.01***********\n",
      "Training: loss = 0.3133, accuracy = 1.000000\n",
      "Validation: loss = 0.6826, accuracy = 0.523125\n",
      "***********lambda = 0.1***********\n",
      "Training: loss = 0.3133, accuracy = 1.000000\n",
      "Validation: loss = 0.6826, accuracy = 0.523125\n",
      "***********lambda = 1***********\n",
      "Training: loss = 0.3141, accuracy = 1.000000\n",
      "Validation: loss = 0.6826, accuracy = 0.523125\n",
      "*************KLR for dataset 2*************\n",
      "\n",
      "***********lambda = 0.0001***********\n",
      "Training: loss = 0.3133, accuracy = 1.000000\n",
      "Validation: loss = 0.6312, accuracy = 0.668750\n",
      "***********lambda = 0.001***********\n",
      "Training: loss = 0.3133, accuracy = 1.000000\n",
      "Validation: loss = 0.6312, accuracy = 0.668750\n",
      "***********lambda = 0.01***********\n",
      "Training: loss = 0.3133, accuracy = 1.000000\n",
      "Validation: loss = 0.6312, accuracy = 0.668750\n",
      "***********lambda = 0.1***********\n",
      "Training: loss = 0.3133, accuracy = 1.000000\n",
      "Validation: loss = 0.6312, accuracy = 0.668750\n",
      "***********lambda = 1***********\n",
      "Training: loss = 0.3139, accuracy = 1.000000\n",
      "Validation: loss = 0.6313, accuracy = 0.668750\n"
     ]
    }
   ],
   "source": [
    "lambdas = [10**i for i in range(-4,1)]\n",
    "\n",
    "print(\"*************KLR for dataset 0*************\\n\")\n",
    "alphas_tr0_klr, loss_tr0_klr, acc_0_klr, loss_val0_klr, acc_val0_klr = KLR(K_tr0_poly, ytr0[:,1], K_val0_poly, yval0[:,1], lambdas, tresh=1e-5)\n",
    "print(\"*************KLR for dataset 1*************\\n\")\n",
    "alphas_tr1_klr, loss_tr1_klr, acc_1_klr, loss_val1_klr, acc_val1_klr = KLR(K_tr1_poly, ytr1[:,1], K_val1_poly, yval1[:,1], lambdas, tresh=1e-5)\n",
    "print(\"*************KLR for dataset 2*************\\n\")\n",
    "alphas_tr2_klr, loss_tr2_klr, acc_2_klr, loss_val2_klr, acc_val2_klr = KLR(K_tr2_poly, ytr2[:,1], K_val2_poly, yval2[:,1], lambdas, tresh=1e-5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "involved-momentum",
   "metadata": {},
   "source": [
    "## Testing SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "protecting-recall",
   "metadata": {},
   "source": [
    "### Gaussian Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "compliant-mailman",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************* SVM for dataset 0 *************\n",
      "\n",
      "---------------  lambda = 1e-10  ---------------\n",
      "Training: loss = 0.000000, accuracy = 1.000000\n",
      "Validation: loss = 0.905722, accuracy = 0.572500\n",
      "---------------  lambda = 1e-09  ---------------\n",
      "Training: loss = 0.000000, accuracy = 1.000000\n",
      "Validation: loss = 0.905723, accuracy = 0.572500\n",
      "---------------  lambda = 1e-08  ---------------\n",
      "Training: loss = 0.000000, accuracy = 1.000000\n",
      "Validation: loss = 0.905724, accuracy = 0.572500\n",
      "---------------  lambda = 1e-07  ---------------\n",
      "Training: loss = 0.000000, accuracy = 1.000000\n",
      "Validation: loss = 0.905722, accuracy = 0.572500\n",
      "---------------  lambda = 1e-06  ---------------\n",
      "Training: loss = 0.000000, accuracy = 1.000000\n",
      "Validation: loss = 0.905723, accuracy = 0.572500\n",
      "---------------  lambda = 1e-05  ---------------\n",
      "Training: loss = 0.000000, accuracy = 1.000000\n",
      "Validation: loss = 0.905724, accuracy = 0.572500\n",
      "---------------  lambda = 0.0001  ---------------\n",
      "Training: loss = 0.000000, accuracy = 1.000000\n",
      "Validation: loss = 0.905722, accuracy = 0.572500\n",
      "---------------  lambda = 0.001  ---------------\n",
      "Training: loss = 0.216277, accuracy = 0.960000\n",
      "Validation: loss = 0.895915, accuracy = 0.571250\n",
      "---------------  lambda = 0.01  ---------------\n",
      "Training: loss = 0.821902, accuracy = 0.562500\n",
      "Validation: loss = 0.947779, accuracy = 0.517500\n",
      "---------------  lambda = 0.1  ---------------\n",
      "Training: loss = 0.980732, accuracy = 0.545000\n",
      "Validation: loss = 0.993990, accuracy = 0.517500\n",
      "\n",
      "\n",
      "************* SVM for dataset 1 *************\n",
      "\n",
      "---------------  lambda = 1e-10  ---------------\n",
      "Training: loss = 0.000000, accuracy = 1.000000\n",
      "Validation: loss = 0.953898, accuracy = 0.547500\n",
      "---------------  lambda = 1e-09  ---------------\n",
      "Training: loss = 0.000000, accuracy = 1.000000\n",
      "Validation: loss = 0.953899, accuracy = 0.547500\n",
      "---------------  lambda = 1e-08  ---------------\n",
      "Training: loss = 0.000000, accuracy = 1.000000\n",
      "Validation: loss = 0.953896, accuracy = 0.547500\n",
      "---------------  lambda = 1e-07  ---------------\n",
      "Training: loss = 0.000000, accuracy = 1.000000\n",
      "Validation: loss = 0.953898, accuracy = 0.547500\n",
      "---------------  lambda = 1e-06  ---------------\n",
      "Training: loss = 0.000000, accuracy = 1.000000\n",
      "Validation: loss = 0.953899, accuracy = 0.547500\n",
      "---------------  lambda = 1e-05  ---------------\n",
      "Training: loss = 0.000000, accuracy = 1.000000\n",
      "Validation: loss = 0.953897, accuracy = 0.547500\n",
      "---------------  lambda = 0.0001  ---------------\n",
      "Training: loss = 0.000000, accuracy = 1.000000\n",
      "Validation: loss = 0.953899, accuracy = 0.547500\n",
      "---------------  lambda = 0.001  ---------------\n",
      "Training: loss = 0.267926, accuracy = 0.950000\n",
      "Validation: loss = 0.951636, accuracy = 0.536875\n",
      "---------------  lambda = 0.01  ---------------\n",
      "Training: loss = 0.850585, accuracy = 0.547500\n",
      "Validation: loss = 1.011236, accuracy = 0.489375\n",
      "---------------  lambda = 0.1  ---------------\n",
      "Training: loss = 0.984859, accuracy = 0.547500\n",
      "Validation: loss = 1.001194, accuracy = 0.489375\n",
      "\n",
      "\n",
      "************* SVM for dataset 2 *************\n",
      "\n",
      "---------------  lambda = 1e-10  ---------------\n",
      "Training: loss = 0.000000, accuracy = 1.000000\n",
      "Validation: loss = 0.780078, accuracy = 0.680625\n",
      "---------------  lambda = 1e-09  ---------------\n",
      "Training: loss = 0.000000, accuracy = 1.000000\n",
      "Validation: loss = 0.780083, accuracy = 0.680625\n",
      "---------------  lambda = 1e-08  ---------------\n",
      "Training: loss = 0.000000, accuracy = 1.000000\n",
      "Validation: loss = 0.780074, accuracy = 0.680625\n",
      "---------------  lambda = 1e-07  ---------------\n",
      "Training: loss = 0.000000, accuracy = 1.000000\n",
      "Validation: loss = 0.780080, accuracy = 0.680625\n",
      "---------------  lambda = 1e-06  ---------------\n",
      "Training: loss = 0.000000, accuracy = 1.000000\n",
      "Validation: loss = 0.780085, accuracy = 0.680625\n",
      "---------------  lambda = 1e-05  ---------------\n",
      "Training: loss = 0.000000, accuracy = 1.000000\n",
      "Validation: loss = 0.780075, accuracy = 0.680625\n",
      "---------------  lambda = 0.0001  ---------------\n",
      "Training: loss = 0.000000, accuracy = 1.000000\n",
      "Validation: loss = 0.780078, accuracy = 0.680625\n",
      "---------------  lambda = 0.001  ---------------\n",
      "Training: loss = 0.181552, accuracy = 0.967500\n",
      "Validation: loss = 0.759867, accuracy = 0.684375\n",
      "---------------  lambda = 0.01  ---------------\n",
      "Training: loss = 0.751147, accuracy = 0.747500\n",
      "Validation: loss = 0.855680, accuracy = 0.658125\n",
      "---------------  lambda = 0.1  ---------------\n",
      "Training: loss = 0.974904, accuracy = 0.755000\n",
      "Validation: loss = 0.985371, accuracy = 0.658750\n"
     ]
    }
   ],
   "source": [
    "lambdas = [10**i for i in range(-10, 0)]\n",
    "print(\"************* SVM for dataset 0 *************\\n\")\n",
    "alphas_tr0, loss_tr0, acc_0, loss_val0, acc_val0 = SVM(K_tr0, ytr0[:,1], K_val0, yval0[:,1], lambdas)\n",
    "\n",
    "print(\"\")\n",
    "print(\"\")\n",
    "print(\"************* SVM for dataset 1 *************\\n\")\n",
    "alphas_tr1, loss_tr1, acc_1, loss_val1, acc_val1 = SVM(K_tr1, ytr1[:,1], K_val1, yval1[:,1],lambdas)\n",
    "\n",
    "print(\"\")\n",
    "print(\"\")\n",
    "print(\"************* SVM for dataset 2 *************\\n\")\n",
    "alphas_tr2, loss_tr2, acc_2, loss_val2, acc_val2 = SVM(K_tr2, ytr2[:,1], K_val2, yval2[:,1],lambdas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hispanic-advance",
   "metadata": {},
   "source": [
    "### Linear Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "abstract-yellow",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************* SVM for dataset 0 *************\n",
      "\n",
      "---------------  lambda = 1e-10  ---------------\n",
      "Training: loss = 0.554536, accuracy = 0.775000\n",
      "Validation: loss = 1.192354, accuracy = 0.556875\n",
      "---------------  lambda = 1e-09  ---------------\n",
      "Training: loss = 0.554536, accuracy = 0.775000\n",
      "Validation: loss = 1.192354, accuracy = 0.556875\n",
      "---------------  lambda = 1e-08  ---------------\n",
      "Training: loss = 0.554536, accuracy = 0.775000\n",
      "Validation: loss = 1.192353, accuracy = 0.556875\n",
      "---------------  lambda = 1e-07  ---------------\n",
      "Training: loss = 0.554536, accuracy = 0.775000\n",
      "Validation: loss = 1.192353, accuracy = 0.556875\n",
      "---------------  lambda = 1e-06  ---------------\n",
      "Training: loss = 0.554536, accuracy = 0.775000\n",
      "Validation: loss = 1.192354, accuracy = 0.556875\n",
      "---------------  lambda = 1e-05  ---------------\n",
      "Training: loss = 0.554536, accuracy = 0.775000\n",
      "Validation: loss = 1.192354, accuracy = 0.556875\n",
      "---------------  lambda = 0.0001  ---------------\n",
      "Training: loss = 0.554537, accuracy = 0.777500\n",
      "Validation: loss = 1.191654, accuracy = 0.558125\n",
      "---------------  lambda = 0.001  ---------------\n",
      "Training: loss = 0.554692, accuracy = 0.770000\n",
      "Validation: loss = 1.174117, accuracy = 0.556250\n",
      "---------------  lambda = 0.01  ---------------\n",
      "Training: loss = 0.563110, accuracy = 0.765000\n",
      "Validation: loss = 1.038654, accuracy = 0.562500\n",
      "---------------  lambda = 0.1  ---------------\n",
      "Training: loss = 0.619140, accuracy = 0.730000\n",
      "Validation: loss = 0.926036, accuracy = 0.573750\n",
      "\n",
      "\n",
      "************* SVM for dataset 1 *************\n",
      "\n",
      "---------------  lambda = 1e-10  ---------------\n",
      "Training: loss = 0.610000, accuracy = 0.737500\n",
      "Validation: loss = 1.162969, accuracy = 0.533750\n",
      "---------------  lambda = 1e-09  ---------------\n",
      "Training: loss = 0.610000, accuracy = 0.737500\n",
      "Validation: loss = 1.162969, accuracy = 0.533750\n",
      "---------------  lambda = 1e-08  ---------------\n",
      "Training: loss = 0.610000, accuracy = 0.737500\n",
      "Validation: loss = 1.162973, accuracy = 0.533750\n",
      "---------------  lambda = 1e-07  ---------------\n",
      "Training: loss = 0.610000, accuracy = 0.737500\n",
      "Validation: loss = 1.162969, accuracy = 0.533750\n",
      "---------------  lambda = 1e-06  ---------------\n",
      "Training: loss = 0.610000, accuracy = 0.737500\n",
      "Validation: loss = 1.163087, accuracy = 0.533750\n",
      "---------------  lambda = 1e-05  ---------------\n",
      "Training: loss = 0.610000, accuracy = 0.737500\n",
      "Validation: loss = 1.162968, accuracy = 0.533750\n",
      "---------------  lambda = 0.0001  ---------------\n",
      "Training: loss = 0.610008, accuracy = 0.735000\n",
      "Validation: loss = 1.162070, accuracy = 0.532500\n",
      "---------------  lambda = 0.001  ---------------\n",
      "Training: loss = 0.610234, accuracy = 0.730000\n",
      "Validation: loss = 1.154368, accuracy = 0.533750\n",
      "---------------  lambda = 0.01  ---------------\n",
      "Training: loss = 0.620763, accuracy = 0.745000\n",
      "Validation: loss = 1.048448, accuracy = 0.540000\n",
      "---------------  lambda = 0.1  ---------------\n",
      "Training: loss = 0.678893, accuracy = 0.705000\n",
      "Validation: loss = 0.959973, accuracy = 0.535625\n",
      "\n",
      "\n",
      "************* SVM for dataset 2 *************\n",
      "\n",
      "---------------  lambda = 1e-10  ---------------\n",
      "Training: loss = 0.424575, accuracy = 0.832500\n",
      "Validation: loss = 1.073543, accuracy = 0.636250\n",
      "---------------  lambda = 1e-09  ---------------\n",
      "Training: loss = 0.424575, accuracy = 0.832500\n",
      "Validation: loss = 1.073543, accuracy = 0.636250\n",
      "---------------  lambda = 1e-08  ---------------\n",
      "Training: loss = 0.424575, accuracy = 0.832500\n",
      "Validation: loss = 1.073542, accuracy = 0.636250\n",
      "---------------  lambda = 1e-07  ---------------\n",
      "Training: loss = 0.424575, accuracy = 0.832500\n",
      "Validation: loss = 1.073543, accuracy = 0.636250\n",
      "---------------  lambda = 1e-06  ---------------\n",
      "Training: loss = 0.424575, accuracy = 0.832500\n",
      "Validation: loss = 1.073544, accuracy = 0.636250\n",
      "---------------  lambda = 1e-05  ---------------\n",
      "Training: loss = 0.424575, accuracy = 0.832500\n",
      "Validation: loss = 1.073542, accuracy = 0.636250\n",
      "---------------  lambda = 0.0001  ---------------\n",
      "Training: loss = 0.424580, accuracy = 0.835000\n",
      "Validation: loss = 1.068365, accuracy = 0.635000\n",
      "---------------  lambda = 0.001  ---------------\n",
      "Training: loss = 0.425517, accuracy = 0.835000\n",
      "Validation: loss = 0.997615, accuracy = 0.633750\n",
      "---------------  lambda = 0.01  ---------------\n",
      "Training: loss = 0.436597, accuracy = 0.812500\n",
      "Validation: loss = 0.852775, accuracy = 0.655000\n",
      "---------------  lambda = 0.1  ---------------\n",
      "Training: loss = 0.505367, accuracy = 0.800000\n",
      "Validation: loss = 0.746142, accuracy = 0.676250\n"
     ]
    }
   ],
   "source": [
    "lambdas = [10**i for i in range(-10, 0)]\n",
    "print(\"************* SVM for dataset 0 *************\\n\")\n",
    "alphas_tr0, loss_tr0, acc_0, loss_val0, acc_val0 = SVM(K_tr0_ln, ytr0[:,1], K_val0_ln, yval0[:,1], lambdas)\n",
    "\n",
    "print(\"\")\n",
    "print(\"\")\n",
    "print(\"************* SVM for dataset 1 *************\\n\")\n",
    "alphas_tr1, loss_tr1, acc_1, loss_val1, acc_val1 = SVM(K_tr1_ln, ytr1[:,1], K_val1_ln, yval1[:,1],lambdas)\n",
    "\n",
    "print(\"\")\n",
    "print(\"\")\n",
    "print(\"************* SVM for dataset 2 *************\\n\")\n",
    "alphas_tr2, loss_tr2, acc_2, loss_val2, acc_val2 = SVM(K_tr2_ln, ytr2[:,1], K_val2_ln, yval2[:,1],lambdas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "attempted-nitrogen",
   "metadata": {},
   "source": [
    "### Polynomial Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "acoustic-savage",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************* SVM for dataset 0 *************\n",
      "\n",
      "---------------  lambda = 1e-10  ---------------\n",
      "Training: loss = 0.000000, accuracy = 1.000000\n",
      "Validation: loss = 0.934044, accuracy = 0.565625\n",
      "---------------  lambda = 1e-09  ---------------\n",
      "Training: loss = 0.000000, accuracy = 1.000000\n",
      "Validation: loss = 0.934020, accuracy = 0.565625\n",
      "---------------  lambda = 1e-08  ---------------\n",
      "Training: loss = 0.000000, accuracy = 1.000000\n",
      "Validation: loss = 0.934041, accuracy = 0.565625\n",
      "---------------  lambda = 1e-07  ---------------\n",
      "Training: loss = 0.000000, accuracy = 1.000000\n",
      "Validation: loss = 0.934037, accuracy = 0.565625\n",
      "---------------  lambda = 1e-06  ---------------\n",
      "Training: loss = 0.000000, accuracy = 1.000000\n",
      "Validation: loss = 0.934035, accuracy = 0.565625\n",
      "---------------  lambda = 1e-05  ---------------\n",
      "Training: loss = 0.000000, accuracy = 1.000000\n",
      "Validation: loss = 0.934030, accuracy = 0.565625\n",
      "---------------  lambda = 0.0001  ---------------\n",
      "Training: loss = 0.000000, accuracy = 1.000000\n",
      "Validation: loss = 0.934017, accuracy = 0.565625\n",
      "---------------  lambda = 0.001  ---------------\n",
      "Training: loss = 0.000000, accuracy = 1.000000\n",
      "Validation: loss = 0.934020, accuracy = 0.565625\n",
      "---------------  lambda = 0.01  ---------------\n",
      "Training: loss = 0.000000, accuracy = 1.000000\n",
      "Validation: loss = 0.934029, accuracy = 0.565625\n",
      "---------------  lambda = 0.1  ---------------\n",
      "Training: loss = 0.000000, accuracy = 1.000000\n",
      "Validation: loss = 0.934025, accuracy = 0.565625\n",
      "\n",
      "\n",
      "************* SVM for dataset 1 *************\n",
      "\n",
      "---------------  lambda = 1e-10  ---------------\n",
      "Training: loss = 0.000000, accuracy = 1.000000\n",
      "Validation: loss = 0.963927, accuracy = 0.522500\n",
      "---------------  lambda = 1e-09  ---------------\n",
      "Training: loss = 0.000000, accuracy = 1.000000\n",
      "Validation: loss = 0.963959, accuracy = 0.522500\n",
      "---------------  lambda = 1e-08  ---------------\n",
      "Training: loss = 0.000000, accuracy = 1.000000\n",
      "Validation: loss = 0.963958, accuracy = 0.522500\n",
      "---------------  lambda = 1e-07  ---------------\n",
      "Training: loss = 0.000000, accuracy = 1.000000\n",
      "Validation: loss = 0.963950, accuracy = 0.522500\n",
      "---------------  lambda = 1e-06  ---------------\n",
      "Training: loss = 0.000000, accuracy = 1.000000\n",
      "Validation: loss = 0.963951, accuracy = 0.522500\n",
      "---------------  lambda = 1e-05  ---------------\n",
      "Training: loss = 0.000000, accuracy = 1.000000\n",
      "Validation: loss = 0.963945, accuracy = 0.522500\n",
      "---------------  lambda = 0.0001  ---------------\n",
      "Training: loss = 0.000000, accuracy = 1.000000\n",
      "Validation: loss = 0.963957, accuracy = 0.522500\n",
      "---------------  lambda = 0.001  ---------------\n",
      "Training: loss = 0.000000, accuracy = 1.000000\n",
      "Validation: loss = 0.963996, accuracy = 0.522500\n",
      "---------------  lambda = 0.01  ---------------\n",
      "Training: loss = 0.000000, accuracy = 1.000000\n",
      "Validation: loss = 0.964028, accuracy = 0.521875\n",
      "---------------  lambda = 0.1  ---------------\n",
      "Training: loss = 0.000000, accuracy = 1.000000\n",
      "Validation: loss = 0.964029, accuracy = 0.521875\n",
      "\n",
      "\n",
      "************* SVM for dataset 2 *************\n",
      "\n",
      "---------------  lambda = 1e-10  ---------------\n",
      "Training: loss = 0.000000, accuracy = 1.000000\n",
      "Validation: loss = 0.841448, accuracy = 0.666875\n",
      "---------------  lambda = 1e-09  ---------------\n",
      "Training: loss = 0.000000, accuracy = 1.000000\n",
      "Validation: loss = 0.841521, accuracy = 0.667500\n",
      "---------------  lambda = 1e-08  ---------------\n",
      "Training: loss = 0.000000, accuracy = 1.000000\n",
      "Validation: loss = 0.841486, accuracy = 0.666875\n",
      "---------------  lambda = 1e-07  ---------------\n",
      "Training: loss = 0.000000, accuracy = 1.000000\n",
      "Validation: loss = 0.841503, accuracy = 0.666875\n",
      "---------------  lambda = 1e-06  ---------------\n",
      "Training: loss = 0.000000, accuracy = 1.000000\n",
      "Validation: loss = 0.841431, accuracy = 0.666875\n",
      "---------------  lambda = 1e-05  ---------------\n",
      "Training: loss = 0.000000, accuracy = 1.000000\n",
      "Validation: loss = 0.841451, accuracy = 0.666875\n",
      "---------------  lambda = 0.0001  ---------------\n",
      "Training: loss = 0.000000, accuracy = 1.000000\n",
      "Validation: loss = 0.841333, accuracy = 0.666875\n",
      "---------------  lambda = 0.001  ---------------\n",
      "Training: loss = 0.000000, accuracy = 1.000000\n",
      "Validation: loss = 0.841447, accuracy = 0.666875\n",
      "---------------  lambda = 0.01  ---------------\n",
      "Training: loss = 0.000000, accuracy = 1.000000\n",
      "Validation: loss = 0.841543, accuracy = 0.667500\n",
      "---------------  lambda = 0.1  ---------------\n",
      "Training: loss = 0.000000, accuracy = 1.000000\n",
      "Validation: loss = 0.841534, accuracy = 0.667500\n"
     ]
    }
   ],
   "source": [
    "lambdas = [10**i for i in range(-10, 0)]\n",
    "print(\"************* SVM for dataset 0 *************\\n\")\n",
    "alphas_tr0, loss_tr0, acc_0, loss_val0, acc_val0 = SVM(K_tr0_poly, ytr0[:,1], K_val0_poly, yval0[:,1], lambdas)\n",
    "\n",
    "print(\"\")\n",
    "print(\"\")\n",
    "print(\"************* SVM for dataset 1 *************\\n\")\n",
    "alphas_tr1, loss_tr1, acc_1, loss_val1, acc_val1 = SVM(K_tr1_poly, ytr1[:,1], K_val1_poly, yval1[:,1],lambdas)\n",
    "\n",
    "print(\"\")\n",
    "print(\"\")\n",
    "print(\"************* SVM for dataset 2 *************\\n\")\n",
    "alphas_tr2, loss_tr2, acc_2, loss_val2, acc_val2 = SVM(K_tr2_poly, ytr2[:,1], K_val2_poly, yval2[:,1],lambdas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coastal-mambo",
   "metadata": {},
   "source": [
    "# Testing the accuracy on sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "necessary-image",
   "metadata": {},
   "outputs": [],
   "source": [
    "from csv import reader\n",
    "\n",
    "def features_into_array(path):\n",
    "    with open(path, 'r') as read_obj:\n",
    "        csv_reader = reader(read_obj)\n",
    "        header = next(csv_reader)\n",
    "        X = list()\n",
    "        if header != None:\n",
    "            for row in csv_reader:\n",
    "                # row variable is a list that represents a row in csv\n",
    "                X.append(np.array(row[1]))\n",
    "                \n",
    "    X = np.array(X) ## dtype might be changed in something more convenient. For now, dtype = \"<U1\"\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "dangerous-lindsay",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtr0_seq = features_into_array(\"data/Xtr0.csv\")\n",
    "Ytr0 = np.genfromtxt(\"data/Ytr0.csv\", delimiter=',', skip_header=1)\n",
    "\n",
    "Xtr1_seq = features_into_array(\"data/Xtr1.csv\")\n",
    "Ytr1 = np.genfromtxt(\"data/Ytr1.csv\", delimiter=',', skip_header=1)\n",
    "\n",
    "Xtr2_seq = features_into_array(\"data/Xtr2.csv\")\n",
    "Ytr2 = np.genfromtxt(\"data/Ytr2.csv\", delimiter=',', skip_header=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "headed-genome",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "Xtr0, Xval0, ytr0, yval0 = train_test_split(Xtr0_seq, Ytr0, test_size=0.5, random_state=42)\n",
    "Xtr1, Xval1, ytr1, yval1 = train_test_split(Xtr1_seq, Ytr1, test_size=0.5, random_state=42)\n",
    "Xtr2, Xval2, ytr2, yval2 = train_test_split(Xtr2_seq, Ytr2, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "radical-airport",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "systematic-violin",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "impressive-blocking",
   "metadata": {},
   "source": [
    "## Predictions on the testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "satellite-publicity",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xte0_seq = features_into_array(\"data/Xte0.csv\")\n",
    "Xte1_seq = features_into_array(\"data/Xte1.csv\")\n",
    "Xte2_seq = features_into_array(\"data/Xte2.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "written-dakota",
   "metadata": {},
   "source": [
    "## Making predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lovely-adaptation",
   "metadata": {},
   "source": [
    "### First create the kernels for each testing set with the chosen parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "recognized-congo",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xte0 = np.genfromtxt(\"data/Xte0_mat100.csv\", delimiter='')\n",
    "Xte1 = np.genfromtxt(\"data/Xte1_mat100.csv\", delimiter='')\n",
    "Xte2 = np.genfromtxt(\"data/Xte2_mat100.csv\", delimiter='')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prompt-alaska",
   "metadata": {},
   "source": [
    "Please make sure to use the same parameters as those that were used to create the initial kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "experimental-detective",
   "metadata": {},
   "outputs": [],
   "source": [
    "K_te0 = gaussian_kernel(Xtr0, Xte0, mode=\"test\")\n",
    "K_te1 = gaussian_kernel(Xtr1, Xte1, mode=\"test\")\n",
    "K_te2 = gaussian_kernel(Xtr2, Xte2, mode=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "civic-parks",
   "metadata": {},
   "outputs": [],
   "source": [
    "K_te0_ln = linear_kernel(Xtr0, Xte0, scale=True, mode=\"test\")\n",
    "K_te1_ln = linear_kernel(Xtr1, Xte1, scale=True, mode=\"test\")\n",
    "K_te2_ln = linear_kernel(Xtr2, Xte2, scale=True, mode=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "mounted-diary",
   "metadata": {},
   "outputs": [],
   "source": [
    "K_te0_poly = polynomial_kernel(Xtr0, Xte0, d=3, c=1, mode=\"test\")\n",
    "K_te1_poly = polynomial_kernel(Xtr1, Xte1, d=3, c=1, mode=\"test\")\n",
    "K_te2_poly = polynomial_kernel(Xtr2, Xte2, d=3, c=1, mode=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "cooperative-schema",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_predictions_csv(test_kernels, test_alphas, path):\n",
    "    \n",
    "    predictions = np.zeros(3000, dtype=int)\n",
    "    \n",
    "    for i in range(3):\n",
    "        y_pred = test_kernels[i] @ test_alphas[i]\n",
    "        y_pred[y_pred >= 0.5] = 1\n",
    "        y_pred[y_pred < 0.5] = 0\n",
    "        \n",
    "        predictions[1000*i:1000*(i+1)] = y_pred\n",
    "    \n",
    "    #predictions = predictions.astype(int)\n",
    "    pred = pd.DataFrame({\"Bound\" : predictions})\n",
    "    pred.to_csv(path, index=True,index_label=\"Id\")\n",
    "    print(\"saving predictions\")\n",
    "    #np.savetxt(\"data/Ytest_KRR.csv\", predictions, header = \"Id, Bound\", delimiter =\",\")\n",
    "    print(\"saved predictions\")\n",
    "    return(predictions)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cubic-response",
   "metadata": {},
   "source": [
    "Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "major-collapse",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving predictions\n",
      "saved predictions\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_kernels = [K_te0, K_te1, K_te2]\n",
    "#test_alphas = [alphas_tr0[-4], alphas_tr1[-4], alphas_tr2[-3]] # il faut choisir l'alpha associé à un bon lambda!\n",
    "test_alphas = [alphas_tr0_klr[0], alphas_tr1_klr[0], alphas_tr2_klr[0]]\n",
    "write_predictions_csv(test_kernels, test_alphas, path =\"data/Ytest_KLR.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "visible-proceeding",
   "metadata": {},
   "outputs": [],
   "source": [
    "ex1 = Xtr0_mat100[:10]\n",
    "ex2 = Xtr0_mat100[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "animal-lafayette",
   "metadata": {},
   "outputs": [],
   "source": [
    "M = np.zeros((5,10))\n",
    "for i in range(5):\n",
    "    for j in range(10):\n",
    "        M[i,j] = np.dot(ex2[i], ex1[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ready-barbados",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ True False  True False False  True  True  True  True  True]\n",
      " [False False  True False  True  True  True  True False  True]\n",
      " [ True  True False False False False False False  True  True]\n",
      " [False False False False  True False False  True False False]\n",
      " [False  True False  True  True  True  True  True False False]]\n",
      "1.9081958235744878e-17\n"
     ]
    }
   ],
   "source": [
    "print(M == np.inner(ex2,ex1))\n",
    "print(np.linalg.norm(M - np.inner(ex2,ex1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "twelve-moisture",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.01913989, 0.0094518 , 0.00980624, 0.01051512, 0.00756144,\n",
       "        0.0114603 , 0.00968809, 0.00862476, 0.01110586, 0.01063327],\n",
       "       [0.0094518 , 0.03048204, 0.0116966 , 0.01016068, 0.00791588,\n",
       "        0.00815217, 0.0044896 , 0.01512287, 0.01240548, 0.00378072],\n",
       "       [0.00980624, 0.0116966 , 0.03331758, 0.01772212, 0.01134216,\n",
       "        0.00508034, 0.00968809, 0.01429584, 0.0137051 , 0.00531664],\n",
       "       [0.01051512, 0.01016068, 0.01772212, 0.02457467, 0.01181474,\n",
       "        0.0067344 , 0.01051512, 0.01488658, 0.01465028, 0.00732514],\n",
       "       [0.00756144, 0.00791588, 0.01134216, 0.01181474, 0.02079395,\n",
       "        0.00756144, 0.01228733, 0.01228733, 0.01075142, 0.00850662]])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.inner(ex2, ex1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "micro-stanley",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.01913989, 0.0094518 , 0.00980624, 0.01051512, 0.00756144,\n",
       "        0.0114603 , 0.00968809, 0.00862476, 0.01110586, 0.01063327],\n",
       "       [0.0094518 , 0.03048204, 0.0116966 , 0.01016068, 0.00791588,\n",
       "        0.00815217, 0.0044896 , 0.01512287, 0.01240548, 0.00378072],\n",
       "       [0.00980624, 0.0116966 , 0.03331758, 0.01772212, 0.01134216,\n",
       "        0.00508034, 0.00968809, 0.01429584, 0.0137051 , 0.00531664],\n",
       "       [0.01051512, 0.01016068, 0.01772212, 0.02457467, 0.01181474,\n",
       "        0.0067344 , 0.01051512, 0.01488658, 0.01465028, 0.00732514],\n",
       "       [0.00756144, 0.00791588, 0.01134216, 0.01181474, 0.02079395,\n",
       "        0.00756144, 0.01228733, 0.01228733, 0.01075142, 0.00850662]])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "romantic-calvin",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0005"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1/(2*1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "structural-person",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
