{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "utility-italian",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "noted-driver",
   "metadata": {},
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "diagnostic-heaven",
   "metadata": {},
   "source": [
    "For $k = 0, 1, 2$ we have the following files:\n",
    "* Xtrk.csv - the training sequences.\n",
    "* Xtek.csv - the test sequences.\n",
    "* Ytrk.csv - labels for the training sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "reverse-sociology",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtr0_mat100 = np.genfromtxt(\"data/Xtr0_mat100.csv\", delimiter='')\n",
    "Ytr0 = np.genfromtxt(\"data/Ytr0.csv\", delimiter=',', skip_header=1)\n",
    "\n",
    "Xtr1_mat100 = np.genfromtxt(\"data/Xtr1_mat100.csv\", delimiter='')\n",
    "Ytr1 = np.genfromtxt(\"data/Ytr1.csv\", delimiter=',', skip_header=1)\n",
    "\n",
    "Xtr2_mat100 = np.genfromtxt(\"data/Xtr2_mat100.csv\", delimiter='')\n",
    "Ytr2 = np.genfromtxt(\"data/Ytr2.csv\", delimiter=',', skip_header=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "coordinate-chassis",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_true,y_pred):\n",
    "    n = y_true.shape[0]\n",
    "    predictions = np.zeros(n)\n",
    "    predictions[y_pred >= 0.5] = 1\n",
    "    return np.sum(y_true == predictions) / n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "invalid-present",
   "metadata": {},
   "source": [
    "# Implementing some kernels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "white-lounge",
   "metadata": {},
   "source": [
    "## Linear Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "congressional-authority",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_kernel(X):\n",
    "    return(X @ X.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adequate-momentum",
   "metadata": {},
   "source": [
    "## Gaussian Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "proprietary-costa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian(x,y, sigma):\n",
    "    exp_term = np.linalg.norm(x-y)**2 /(2*sigma)\n",
    "    return(np.exp(-exp_term))\n",
    "\n",
    "# Naive computation of the gaussian kernel that can be easily improved\n",
    "def gaussian_kernel(X,sigma):\n",
    "    n = X.shape[0]\n",
    "    K = np.eye(n) # One along the diagonals because K(x,x) = exp(0) = 1\n",
    "    for i in range(n):\n",
    "        for j in range(i+1,n):\n",
    "            val = gaussian(X[i], X[j], sigma)\n",
    "            K[i,j] = val\n",
    "            K[j,i] = val\n",
    "    return(K)\n",
    "\n",
    "# Idea : save some time by dividing by 2*sigma and applying the exponential to the matrix of pairwise distances\n",
    "# (computed on a loop)\n",
    "def gaussian_kernel2(X,sigma):\n",
    "    n = X.shape[0]\n",
    "    K = np.zeros((n,n)) # zero along the diagonals \n",
    "    for i in range(n):\n",
    "        for j in range(i+1,n):\n",
    "            val = np.linalg.norm(X[i]-X[j])\n",
    "            K[i,j] = val\n",
    "            K[j,i] = val\n",
    "    K = np.exp((-K**2)/(2*sigma))\n",
    "    return(K)\n",
    "\n",
    "# Idea : efficient computation of the pairwise distances\n",
    "def gaussian_kernel3(X,sigma):\n",
    "    K = ((X[:, :, None] - X[:, :, None].T) ** 2).sum(1)\n",
    "    K = np.exp((-K)/(2*sigma))\n",
    "    return(K)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dental-dialogue",
   "metadata": {},
   "source": [
    "### Check that all three methods have the same output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "broken-tanzania",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = Xtr0_mat100[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "expressed-conducting",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 9.411180019378662 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "K1 = gaussian_kernel(X, 1)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "conditional-research",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 7.064807891845703 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "K2 = gaussian_kernel2(X, 1)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "median-laser",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 0.8858790397644043 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "K3 = gaussian_kernel3(X,1)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "collect-dependence",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.all(K1 == K2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "immune-bulgaria",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.all(K1 == K3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "universal-authority",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.62449569005189e-14\n"
     ]
    }
   ],
   "source": [
    "K_diff = K1 - K3\n",
    "print(np.linalg.norm(K_diff))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "occupied-nothing",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value for K1 : 0.9907097748987076, Value for K3: 0.9907097748987077, value difference : -1.1102230246251565e-16\n",
      "Value for K1 : 0.983944092235294, Value for K3: 0.9839440922352941, value difference : -1.1102230246251565e-16\n",
      "Value for K1 : 0.9870878669144311, Value for K3: 0.9870878669144312, value difference : -1.1102230246251565e-16\n",
      "Value for K1 : 0.9883715419167118, Value for K3: 0.9883715419167117, value difference : 1.1102230246251565e-16\n",
      "Value for K1 : 0.9904757028915513, Value for K3: 0.9904757028915512, value difference : 1.1102230246251565e-16\n",
      "Value for K1 : 0.9907097748987076, Value for K3: 0.9907097748987077, value difference : -1.1102230246251565e-16\n",
      "Value for K1 : 0.9904757028915512, Value for K3: 0.9904757028915513, value difference : -1.1102230246251565e-16\n",
      "Value for K1 : 0.9904757028915512, Value for K3: 0.9904757028915513, value difference : -1.1102230246251565e-16\n",
      "Value for K1 : 0.9904757028915513, Value for K3: 0.9904757028915512, value difference : 1.1102230246251565e-16\n",
      "Value for K1 : 0.9904757028915513, Value for K3: 0.9904757028915512, value difference : 1.1102230246251565e-16\n",
      "Value for K1 : 0.9858058591207269, Value for K3: 0.985805859120727, value difference : -1.1102230246251565e-16\n",
      "Value for K1 : 0.9907097748987076, Value for K3: 0.9907097748987077, value difference : -1.1102230246251565e-16\n",
      "Value for K1 : 0.9858058591207269, Value for K3: 0.985805859120727, value difference : -1.1102230246251565e-16\n",
      "Value for K1 : 0.9904757028915513, Value for K3: 0.9904757028915512, value difference : 1.1102230246251565e-16\n",
      "Value for K1 : 0.9870878669144311, Value for K3: 0.9870878669144312, value difference : -1.1102230246251565e-16\n",
      "Value for K1 : 0.9907097748987076, Value for K3: 0.9907097748987077, value difference : -1.1102230246251565e-16\n",
      "Value for K1 : 0.9883715419167117, Value for K3: 0.9883715419167118, value difference : -1.1102230246251565e-16\n"
     ]
    }
   ],
   "source": [
    "for i in range(1000):\n",
    "    if K_diff[0,i] != 0:\n",
    "        print(f\"Value for K1 : {K1[0,i]}, Value for K3: {K3[0,i]}, value difference : {K_diff[0,i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "identical-halloween",
   "metadata": {},
   "source": [
    "We can see that the differnce comes at 1e-16. \n",
    "Given the norm of the difference of K1 and K3, we can consider that both matrices are equal so it is okay to use the third method which is considerably faster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "handmade-preview",
   "metadata": {},
   "source": [
    "## Polynomial Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "shaped-mumbai",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "common-exclusion",
   "metadata": {},
   "source": [
    "## Kernel Ridge Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "appreciated-november",
   "metadata": {},
   "source": [
    "* Consider RKHS $\\mathcal H$, associated to a p.d. kernel K on $\\mathcal X$\n",
    "* Let $y = (y_1, \\dots, y_n)^T \\in \\mathbb R ^n$\n",
    "* Let $\\alpha = (\\alpha_1, \\dots, \\alpha_n)^T \\in \\mathbb R ^n$\n",
    "* Let $K$ be the $n\\times n$ Gram Matrix such that $K_{i,j} = K(x_i, x_j)$\n",
    "* We can then write\n",
    "$$\n",
    "(\\hat f(x_1), \\dots, \\hat f(x_n))^T = K\\alpha\n",
    "$$\n",
    "* The norm is $||\\hat f||^2_{\\mathcal H} = \\alpha^T K \\alpha$\n",
    "* KRR $\\leftrightarrow \\text{argmin}_{\\alpha \\in \\mathbb R^n} \\frac{1}{n} (K\\alpha - y)^T(K\\alpha - y) + \\lambda \\alpha^T K \\alpha$\n",
    "* Solution for $\\lambda > 0$:\n",
    "$$\n",
    "\\alpha = (K+\\lambda nI)^{-1}y\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "scientific-andrews",
   "metadata": {},
   "outputs": [],
   "source": [
    "def KRR(K, y, Kval, yval, lambd):\n",
    "    \"\"\"\n",
    "    takes the kernel matrix as an input and computes the MSE and the predictions for each value in lambd (list)\n",
    "    \"\"\"\n",
    "    assert K.shape[0] == y.shape[0]\n",
    "    assert len(lambd) > 0\n",
    "    n = K.shape[0]\n",
    "    \n",
    "    loss = []\n",
    "    acc = []\n",
    "    \n",
    "    loss_val = []\n",
    "    acc_val = []\n",
    "    alphas = []\n",
    "    \n",
    "    for l in lambd:\n",
    "        \n",
    "        assert l >= 0\n",
    "        # find the parameter alpha\n",
    "        alpha = np.linalg.solve((K + l*n*np.eye(n)), y)\n",
    "        # predict\n",
    "        \n",
    "        loss_lambda = MSE(K, y, l, alpha)\n",
    "        acc_lambda = accuracy(y,K@alpha)\n",
    "        \n",
    "        loss_lambdaval = MSE(Kval, yval, l, alpha)\n",
    "        acc_lambdaval = accuracy(yval,Kval@alpha)\n",
    "\n",
    "        print(f\"***********lambda = {l}***********\")\n",
    "        print(f\"Training: loss = {loss_lambda:.4f}, accuracy = {acc_lambda:.6f}\")\n",
    "        print(f\"Validation: loss = {loss_lambdaval:.4f}, accuracy = {acc_lambdaval:.6f}\")\n",
    "        \n",
    "        loss += [loss_lambda]\n",
    "        acc += [acc_lambda]\n",
    "        \n",
    "        loss_val += [loss_lambdaval]\n",
    "        acc_val += [acc_lambdaval]\n",
    "        \n",
    "        \n",
    "        alphas +=[alpha]\n",
    "        \n",
    "    return(alphas, loss, acc, loss_val, acc_val)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "progressive-respect",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MSE(K, y, lambd, alpha):\n",
    "    n = y.shape[0]\n",
    "    data_term = (np.linalg.norm(np.dot(K, alpha.reshape(-1,1)) - y)**2)/n\n",
    "    reg_term = alpha @ K @ alpha\n",
    "    return(data_term + lambd * reg_term)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spatial-purple",
   "metadata": {},
   "source": [
    "## Kernel Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "marine-association",
   "metadata": {},
   "source": [
    "- Binary Classificaiton setup: $\\mathcal Y = \\{-1, 1\\}$\n",
    "- $\\mathcal l_{\\text{logistic}}(f(x),y) = -\\log p(y|f(x)) = \\log(1 + e^{-yf(x)})$ where $p(y|f(x)) = \\sigma(y(f(x))$\n",
    "\n",
    "Objective:\n",
    "\\begin{align*}\n",
    "\\hat f &= \\text{argmin}_{f\\in \\mathcal H} \\frac{1}{n} \\sum_{i=1}^n \\log(1+e^{-y_if(x_i)}) + \\frac{\\lambda}{2}||f||^2_{\\mathcal H}\\\\\n",
    "\\alpha &= \\text{argmin}_{\\alpha \\in \\mathbb R^n} \\frac{1}{n} \\sum_{i=1}^n \\log(1+e^{-y_i[K\\alpha]_i}) + \\frac{\\lambda}{2} \\alpha^T K \\alpha\n",
    "\\end{align*}\n",
    "\n",
    "We define the following fonctions and vectors:\n",
    "* $\\mathcal l _\\text{logistic}(u) = \\log(1+e^{-u})$\n",
    "* $\\mathcal l' _\\text{logistic}(u) = -\\sigma(-u)$\n",
    "* $\\mathcal l'' _\\text{logistic}(u) = \\sigma(u)\\sigma(-u)$\n",
    "\n",
    "* for $i = 1, \\dots, n$, $P_i(\\alpha) = \\mathcal l' _\\text{logistic}(y_i[K\\alpha]_i)$\n",
    "* for $i = 1, \\dots, n$, $W_i(\\alpha) = \\mathcal l'' _\\text{logistic}(y_i[K\\alpha]_i)$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\\begin{align*}\n",
    "J(\\alpha) &= \\frac{1}{n} \\sum_{i=1}^n \\log(1+e^{-y_i[K\\alpha]_i}) + \\frac{\\lambda}{2} \\alpha^T K \\alpha\\\\\n",
    "\\nabla J(\\alpha) &= \\frac{1}{n} KP(\\alpha) y + \\lambda K \\alpha \\quad \\text{where } P(\\alpha) = \\text{diag}(P_1(\\alpha), \\dots, P_n(\\alpha))\\\\\n",
    "\\nabla^2 J(\\alpha) &= \\frac{1}{n}KW(\\alpha)K+\\lambda K \\quad \\text{where } W(\\alpha) = \\text{diag}(W_1(\\alpha), \\dots, W_n(\\alpha))\n",
    "\\end{align*}\n",
    "\n",
    "We are interested in the quadratic approximation of $J$ near a point $\\alpha_0$:\n",
    "\\begin{align*}\n",
    "J_q(\\alpha) &= J(\\alpha_0) + (\\alpha - \\alpha_0)^T \\nabla J(\\alpha_0) + \\frac{1}{2} (\\alpha - \\alpha_0)^T \\nabla^2 J(\\alpha_0)(\\alpha - \\alpha_0)\\\\\n",
    "2J_q(\\alpha) &= -\\frac{2}{n} \\alpha^T KW(K\\alpha_0-W^{-1}Py)+\\frac{1}{n}\\alpha^TKWK\\alpha+ \\lambda\\alpha^TK\\alpha +C\\\\\n",
    "&= \\frac{1}{n} (K\\alpha - z)^TW(K\\alpha - z) + \\lambda\\alpha^TK\\alpha + C \\quad \\text{where} z = K\\alpha_0 - W^{-1} P y\n",
    "\\end{align*}\n",
    "\n",
    "The WKRR problem is presented as:\n",
    "$$\n",
    "\\text{argmin}_{\\alpha \\in \\mathbb R^n} \\frac{1}{n}(K\\alpha - y)^TW(K\\alpha - y) + \\lambda \\alpha^TK\\alpha\n",
    "$$\n",
    "and has as :\n",
    "$$\n",
    "\\alpha = W^{1/2} (W^{1/2}KW^{1/2}+n\\lambda I)^{-1} W^{1/2}y\n",
    "$$\n",
    "\n",
    "So, in order to solve KRL, we use IRLS on a WKRR problem until convergence:\n",
    "$$\\alpha^{t+1} \\gets \\text{solveWKRR}(K, W^t, z^t)$$\n",
    "With the updates for $W^t$ and $z^t$ from $\\alpha^t$ are:\n",
    "- $m_i \\gets [K\\alpha^t]_i$\n",
    "- $P_i^t \\gets -\\sigma(-y_im_i)$\n",
    "- $W_i^t \\gets \\sigma(m_i)\\sigma(-m_i)$\n",
    "- $z_i^t \\gets m_i + y_i / \\sigma(-y_im_i)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "german-roman",
   "metadata": {},
   "outputs": [],
   "source": [
    "def solveWKRR(K,W,z,y,lambd):\n",
    "    \n",
    "    assert np.all(W >= 0)\n",
    "    \n",
    "    W_sq = np.sqrt(W)\n",
    "    n = K.shape[0]\n",
    "    inv_matrix = np.linalg.solve((W_sq @ K @ W_sq + n * lambd * np.eye(n)), W_sq @ y)\n",
    "    alpha = W_sq @ inv_matrix\n",
    "    \n",
    "    return alpha\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "vocational-logistics",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "def logistic_loss(y_true, y_pred):\n",
    "    n = y_true.shape[0]\n",
    "    log_term = np.log(sigmoid(y_true*y_pred))\n",
    "    return(-np.sum(log_term)/n)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "marine-trinidad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def KLR(K, y, Kval, yval, lambd, maxIter = 100, tresh = 1e-8):\n",
    "    \n",
    "    # initialize the values\n",
    "    assert K.shape[0] == y.shape[0]\n",
    "    n = K.shape[0]\n",
    "    \n",
    "    loss = []\n",
    "    acc = []\n",
    "    \n",
    "    loss_val = []\n",
    "    acc_val = []\n",
    "    \n",
    "    \n",
    "    alphas = []\n",
    "    \n",
    "    for l in lambd :\n",
    "        cnt = 0\n",
    "        \n",
    "        P_t, W_t = np.eye(n), np.eye(n)\n",
    "        z_t = K@ np.ones(n) - y\n",
    "        alpha_t = np.ones(n)\n",
    "        diff_alpha = np.inf\n",
    "\n",
    "\n",
    "        while (diff_alpha > tresh) and (cnt < maxIter):\n",
    "\n",
    "            old_alpha = alpha_t\n",
    "            alpha_t = solveWKRR(K, W_t, z_t, y, l)\n",
    "\n",
    "            m_t = K@alpha_t\n",
    "            sigma_m = sigmoid(m_t)\n",
    "            sigma_my = sigmoid(-y*m_t)\n",
    "\n",
    "            P_t = - np.diag(sigma_my)\n",
    "            W_t = np.diag(sigma_m * (1-sigma_m))\n",
    "\n",
    "            z_t = m_t - (P_t@y)/(sigma_m * (1-sigma_m))\n",
    "\n",
    "            diff_alpha = np.linalg.norm(alpha_t - old_alpha)\n",
    "            cnt+=1\n",
    "            if cnt % 10 == 0:\n",
    "                print(l, cnt)\n",
    "        \n",
    "        loss_lambda = logistic_loss(y, K@alpha_t)\n",
    "        acc_lambda = accuracy(y,K@alpha_t)\n",
    "        \n",
    "        loss_lambdaval = logistic_loss(yval, Kval@alpha_t)\n",
    "        acc_lambdaval = accuracy(yval,Kval@alpha_t)\n",
    "\n",
    "        \n",
    "        print(f\"***********lambda = {l}***********\")\n",
    "        print(f\"Training: loss = {loss_lambda:.4f}, accuracy = {acc_lambda:.6f}\")\n",
    "        print(f\"Validation: loss = {loss_lambdaval:.4f}, accuracy = {acc_lambdaval:.6f}\")\n",
    "        \n",
    "        \n",
    "        loss += [loss_lambda]\n",
    "        acc += [acc_lambda]\n",
    "        \n",
    "        loss_val += [loss_lambdaval]\n",
    "        acc_val += [acc_lambdaval]\n",
    "        \n",
    "        alphas +=[alpha_t]\n",
    "        \n",
    "    return(alphas, loss, acc, loss_val, acc_val)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "light-recall",
   "metadata": {},
   "source": [
    "## Support Vector Machine approach (SVM)\n",
    "\n",
    "- Binary Classificaiton setup: $\\mathcal Y = \\{-1, 1\\}$\n",
    "- $\\mathcal l_{\\text{hinge}}(f(x),y) = \\max(1- y f(x), 0)$\n",
    "\n",
    "Objective:\n",
    "\\begin{align*}\n",
    "\\hat f &= \\text{argmin}_{f\\in \\mathcal H} \\frac{1}{n} \\sum_{i=1}^n \\max(1- y_i f(x_i), 0) + \\lambda||f||^2_{\\mathcal H}\\\\\n",
    "\\alpha &= \\text{argmin}_{\\alpha \\in \\mathbb R^n} \\frac{1}{n} \\sum_{i=1}^n \\max(y_i[K\\alpha]_i, 0) + \\lambda \\alpha^T K \\alpha\n",
    "\\end{align*}\n",
    "\n",
    "It is a convex optimization problem but the objective is not smooth.\n",
    "\n",
    "By introducing additional slack variables $\\xi_i$, the problem's objective becomes smooth but it is not the case for the constraints anymore. Let us solve the dual formulation instead (which is sparse, leading to faster algorithms). \n",
    "\n",
    "The dual can be rewritten as a quadratic minimization under box constraints : \n",
    "\n",
    "\\begin{align*}\n",
    "\\min_{\\alpha \\in \\mathbb R^n} \\frac{1}{2} \\alpha^TK\\alpha - \\alpha^T y\\\\\n",
    "\\text{s.t. }  0\\leq y_i\\alpha_i\\leq \\frac{1}{2\\lambda n}\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "We will solve it using CVXOpt tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "defensive-harbor",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hinge_loss(y_true, y_pred):\n",
    "    n = y_true.shape[0]\n",
    "    term = np.maximum(1-y_true*y_pred, 0)\n",
    "    return(np.sum(term)/n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "representative-revolution",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Je pense qu'une plus belle façon de faire serait de créer des fonctions de \n",
    "## kernel(X, sigma) et de les appeler avec en paramètres (X_train ou X_val) selon si on \n",
    "## fait le training ou la validation, pour pas avoir à garder en mémoire les kernels train/val\n",
    "## comme on le fait jusqu'à maintenant\n",
    "## Mais bon, là il est 2h47 du matin, j'ai un peu la flemme et j'imagine que toi aussi,\n",
    "## ça marche déjà bien comme ça ^^\n",
    "\n",
    "def _gaussian_kernel(sigma=1):\n",
    "    \"\"\"\n",
    "    Prepares a Gaussian RBF kernel using the provided sigma.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    kernel_function: A callable to the Gaussian RBF kernel function.\n",
    "\n",
    "    \"\"\"\n",
    "    gamma = -1 / (2 * sigma ** 2)\n",
    "    kernel_function = lambda X, y: np.exp(gamma * np.square(X[:, np.newaxis] - y).sum(axis=2))\n",
    "    return kernel_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "norman-firewall",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cvxopt import matrix, solvers\n",
    "\n",
    "def SVM(K, y, K_val, y_val, lambd):\n",
    "    # initialize the values\n",
    "    assert K.shape[0] == y.shape[0]\n",
    "    n = K.shape[0]\n",
    "    \n",
    "    y_preds, y_preds_val = [], []\n",
    "    losses, losses_val = [], []\n",
    "    accuracies, accuracies_val = [], []\n",
    "    alphas = []\n",
    "    \n",
    "    \n",
    "    for l in lambd :\n",
    "\n",
    "            ## Solving dual using CVXOpt\n",
    "        P = matrix(K)\n",
    "        q = matrix(-y)\n",
    "        D = np.diag(-y)\n",
    "        G = matrix(np.vstack((D,-D)))\n",
    "        h = matrix(np.concatenate((np.zeros(n), 1/(2*l) * np.ones(n)), axis=0))\n",
    "        solvers.options['show_progress'] = False\n",
    "        sol=solvers.qp(P, q, G, h)\n",
    "        alpha = sol['x']\n",
    "        alpha = np.reshape(alpha,-1)               \n",
    "\n",
    "            ## predictions\n",
    "        # training\n",
    "        pred_l = K @alpha\n",
    "        y_preds += [pred_l]\n",
    "        loss_l = hinge_loss(y, pred_l)\n",
    "        acc_l = accuracy(y, pred_l)\n",
    "\n",
    "        \n",
    "        # validation\n",
    "        pred_l_val = K_val@alpha\n",
    "        y_preds_val += [pred_l_val]\n",
    "        loss_l_val = hinge_loss(y_val, pred_l_val)\n",
    "        acc_l_val = accuracy(y_val,pred_l_val)\n",
    "        \n",
    "\n",
    "        print(15*\"-\", f\" lambda = {l} \", 15*\"-\")\n",
    "        print(f\"Training: loss = {loss_l:.4f}, accuracy = {acc_l:.6f}\")\n",
    "        print(f\"Validation: loss = {loss_l_val:.4f}, accuracy = {acc_l_val:.6f}\")\n",
    "        \n",
    "        losses += [loss_l]\n",
    "        accuracies += [acc_l]\n",
    "        \n",
    "        losses_val += [loss_l_val]\n",
    "        accuracies_val += [acc_l_val]\n",
    "    \n",
    "        alphas +=[alpha] \n",
    "        \n",
    "    return(alphas, losses, accuracies, losses_val, accuracies_val)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proud-northeast",
   "metadata": {},
   "source": [
    "# Testing the accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "excellent-module",
   "metadata": {},
   "source": [
    "## Splitting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "criminal-crime",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "Xtr0, Xval0, ytr0, yval0 = train_test_split(Xtr0_mat100, Ytr0, test_size=0.5, random_state=42)\n",
    "Xtr1, Xval1, ytr1, yval1 = train_test_split(Xtr1_mat100, Ytr1, test_size=0.5, random_state=42)\n",
    "Xtr2, Xval2, ytr2, yval2 = train_test_split(Xtr2_mat100, Ytr2, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cheap-lighter",
   "metadata": {},
   "source": [
    "## Create the kernel matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "convinced-warner",
   "metadata": {},
   "outputs": [],
   "source": [
    "K_tr0 = gaussian_kernel3(Xtr0,0.5)\n",
    "K_tr1 = gaussian_kernel3(Xtr1,0.5)\n",
    "K_tr2 = gaussian_kernel3(Xtr2,0.5)\n",
    "\n",
    "K_val0 = gaussian_kernel3(Xval0,0.5)\n",
    "K_val1 = gaussian_kernel3(Xval1,0.5)\n",
    "K_val2 = gaussian_kernel3(Xval2,0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "violent-mortgage",
   "metadata": {},
   "source": [
    "## Testing KRR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "surrounded-revision",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*************KRR for dataset 0*************\n",
      "\n",
      "***********lambda = 0***********\n",
      "Training: loss = 497.5500, accuracy = 1.000000\n",
      "Validation: loss = 883887190.5092, accuracy = 0.483000\n",
      "***********lambda = 1e-10***********\n",
      "Training: loss = 497.3042, accuracy = 1.000000\n",
      "Validation: loss = 880451639.5200, accuracy = 0.483000\n",
      "***********lambda = 1e-09***********\n",
      "Training: loss = 495.1282, accuracy = 1.000000\n",
      "Validation: loss = 850853163.6289, accuracy = 0.484000\n",
      "***********lambda = 1e-08***********\n",
      "Training: loss = 476.3027, accuracy = 1.000000\n",
      "Validation: loss = 640425725.6463, accuracy = 0.481000\n",
      "***********lambda = 1e-07***********\n",
      "Training: loss = 393.9756, accuracy = 0.999000\n",
      "Validation: loss = 160132915.1740, accuracy = 0.487000\n",
      "***********lambda = 1e-06***********\n",
      "Training: loss = 309.6085, accuracy = 0.877000\n",
      "Validation: loss = 5538134.4045, accuracy = 0.477000\n",
      "***********lambda = 1e-05***********\n",
      "Training: loss = 287.5814, accuracy = 0.717000\n",
      "Validation: loss = 100382.3624, accuracy = 0.481000\n",
      "***********lambda = 0.0001***********\n",
      "Training: loss = 268.9717, accuracy = 0.675000\n",
      "Validation: loss = 1621.3964, accuracy = 0.485000\n",
      "***********lambda = 0.001***********\n",
      "Training: loss = 251.8760, accuracy = 0.619000\n",
      "Validation: loss = 271.6793, accuracy = 0.490000\n",
      "***********lambda = 0.01***********\n",
      "Training: loss = 248.9193, accuracy = 0.535000\n",
      "Validation: loss = 252.1538, accuracy = 0.503000\n",
      "***********lambda = 0.1***********\n",
      "Training: loss = 250.6821, accuracy = 0.535000\n",
      "Validation: loss = 255.8232, accuracy = 0.503000\n",
      "***********lambda = 1***********\n",
      "Training: loss = 304.4440, accuracy = 0.535000\n",
      "Validation: loss = 321.8142, accuracy = 0.503000\n",
      "***********lambda = 10***********\n",
      "Training: loss = 428.4130, accuracy = 0.535000\n",
      "Validation: loss = 457.7824, accuracy = 0.503000\n",
      "*************KRR for dataset 1*************\n",
      "\n",
      "***********lambda = 0***********\n",
      "Training: loss = 499.8380, accuracy = 1.000000\n",
      "Validation: loss = 1210568348.7498, accuracy = 0.472000\n",
      "***********lambda = 1e-10***********\n",
      "Training: loss = 499.5660, accuracy = 1.000000\n",
      "Validation: loss = 1207281354.3489, accuracy = 0.472000\n",
      "***********lambda = 1e-09***********\n",
      "Training: loss = 497.1571, accuracy = 1.000000\n",
      "Validation: loss = 1178465228.4054, accuracy = 0.472000\n",
      "***********lambda = 1e-08***********\n",
      "Training: loss = 476.2990, accuracy = 1.000000\n",
      "Validation: loss = 948864321.0218, accuracy = 0.474000\n",
      "***********lambda = 1e-07***********\n",
      "Training: loss = 387.3107, accuracy = 0.999000\n",
      "Validation: loss = 280339586.2163, accuracy = 0.470000\n",
      "***********lambda = 1e-06***********\n",
      "Training: loss = 302.5167, accuracy = 0.875000\n",
      "Validation: loss = 13931546.4281, accuracy = 0.481000\n",
      "***********lambda = 1e-05***********\n",
      "Training: loss = 283.0117, accuracy = 0.704000\n",
      "Validation: loss = 292736.6573, accuracy = 0.483000\n",
      "***********lambda = 0.0001***********\n",
      "Training: loss = 267.6845, accuracy = 0.663000\n",
      "Validation: loss = 4533.8532, accuracy = 0.483000\n",
      "***********lambda = 0.001***********\n",
      "Training: loss = 252.9634, accuracy = 0.632000\n",
      "Validation: loss = 350.5202, accuracy = 0.487000\n",
      "***********lambda = 0.01***********\n",
      "Training: loss = 250.1525, accuracy = 0.533000\n",
      "Validation: loss = 254.9815, accuracy = 0.490000\n",
      "***********lambda = 0.1***********\n",
      "Training: loss = 252.0528, accuracy = 0.509000\n",
      "Validation: loss = 254.7318, accuracy = 0.490000\n",
      "***********lambda = 1***********\n",
      "Training: loss = 311.9506, accuracy = 0.509000\n",
      "Validation: loss = 321.9010, accuracy = 0.490000\n",
      "***********lambda = 10***********\n",
      "Training: loss = 450.2059, accuracy = 0.509000\n",
      "Validation: loss = 467.5630, accuracy = 0.490000\n",
      "*************KRR for dataset 2*************\n",
      "\n",
      "***********lambda = 0***********\n",
      "Training: loss = 499.9980, accuracy = 1.000000\n",
      "Validation: loss = 228148373.4248, accuracy = 0.489000\n",
      "***********lambda = 1e-10***********\n",
      "Training: loss = 499.7884, accuracy = 1.000000\n",
      "Validation: loss = 232481788.7124, accuracy = 0.505000\n",
      "***********lambda = 1e-09***********\n",
      "Training: loss = 497.9328, accuracy = 1.000000\n",
      "Validation: loss = 213391233.5153, accuracy = 0.512000\n",
      "***********lambda = 1e-08***********\n",
      "Training: loss = 481.7602, accuracy = 1.000000\n",
      "Validation: loss = 156192530.7362, accuracy = 0.513000\n",
      "***********lambda = 1e-07***********\n",
      "Training: loss = 409.6664, accuracy = 1.000000\n",
      "Validation: loss = 65742016.2281, accuracy = 0.497000\n",
      "***********lambda = 1e-06***********\n",
      "Training: loss = 337.5715, accuracy = 0.881000\n",
      "Validation: loss = 8866593.6460, accuracy = 0.488000\n",
      "***********lambda = 1e-05***********\n",
      "Training: loss = 319.3397, accuracy = 0.772000\n",
      "Validation: loss = 236788.0662, accuracy = 0.496000\n",
      "***********lambda = 0.0001***********\n",
      "Training: loss = 297.0264, accuracy = 0.730000\n",
      "Validation: loss = 2769.6510, accuracy = 0.496000\n",
      "***********lambda = 0.001***********\n",
      "Training: loss = 260.4882, accuracy = 0.700000\n",
      "Validation: loss = 275.1597, accuracy = 0.497000\n",
      "***********lambda = 0.01***********\n",
      "Training: loss = 250.3703, accuracy = 0.671000\n",
      "Validation: loss = 250.3162, accuracy = 0.486000\n",
      "***********lambda = 0.1***********\n",
      "Training: loss = 252.2179, accuracy = 0.499000\n",
      "Validation: loss = 251.5936, accuracy = 0.504000\n",
      "***********lambda = 1***********\n",
      "Training: loss = 314.5944, accuracy = 0.499000\n",
      "Validation: loss = 311.9106, accuracy = 0.504000\n",
      "***********lambda = 10***********\n",
      "Training: loss = 458.5056, accuracy = 0.499000\n",
      "Validation: loss = 453.9122, accuracy = 0.504000\n"
     ]
    }
   ],
   "source": [
    "lambdas = [0] + [10**i for i in range(-10,2)]\n",
    "print(\"*************KRR for dataset 0*************\\n\")\n",
    "alphas_tr0, loss_tr0, acc_0, loss_val0, acc_val0 = KRR(K_tr0, ytr0[:,1], K_val0, yval0[:,1], lambdas)\n",
    "print(\"*************KRR for dataset 1*************\\n\")\n",
    "alphas_tr1, loss_tr1, acc_1, loss_val1, acc_val1 = KRR(K_tr1, ytr1[:,1], K_val1, yval1[:,1],lambdas)\n",
    "print(\"*************KRR for dataset 2*************\\n\")\n",
    "alphas_tr2, loss_tr2, acc_2, loss_val2, acc_val2 = KRR(K_tr2, ytr2[:,1], K_val2, yval2[:,1],lambdas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "modular-basics",
   "metadata": {},
   "source": [
    "## Testing KLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "id": "irish-rover",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*************KLR for dataset 0*************\n",
      "\n",
      "1e-08 10\n",
      "1e-08 20\n",
      "1e-08 30\n",
      "1e-08 40\n",
      "1e-08 50\n",
      "1e-08 60\n",
      "1e-08 70\n",
      "1e-08 80\n",
      "1e-08 90\n",
      "1e-08 100\n",
      "***********lambda = 1e-08***********\n",
      "Training: loss = 0.5439, accuracy = 0.993000\n",
      "Validation: loss = inf, accuracy = 0.485000\n",
      "*************KLR for dataset 1*************\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Claudia/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:2: RuntimeWarning: overflow encountered in exp\n",
      "  \n",
      "/Users/Claudia/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:6: RuntimeWarning: divide by zero encountered in log\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1e-08 10\n",
      "1e-08 20\n",
      "1e-08 30\n",
      "1e-08 40\n",
      "1e-08 50\n",
      "1e-08 60\n",
      "1e-08 70\n",
      "1e-08 80\n",
      "1e-08 90\n",
      "1e-08 100\n",
      "***********lambda = 1e-08***********\n",
      "Training: loss = 0.5364, accuracy = 0.991000\n",
      "Validation: loss = inf, accuracy = 0.474000\n",
      "*************KLR for dataset 2*************\n",
      "\n",
      "1e-08 10\n",
      "1e-08 20\n",
      "1e-08 30\n",
      "1e-08 40\n",
      "1e-08 50\n",
      "1e-08 60\n",
      "1e-08 70\n",
      "1e-08 80\n",
      "1e-08 90\n",
      "1e-08 100\n",
      "***********lambda = 1e-08***********\n",
      "Training: loss = 0.5267, accuracy = 0.992000\n",
      "Validation: loss = inf, accuracy = 0.493000\n"
     ]
    }
   ],
   "source": [
    "lambdas = [1e-8]\n",
    "print(\"*************KLR for dataset 0*************\\n\")\n",
    "alphas_tr0_klr, loss_tr0_klr, acc_0_klr, loss_val0_klr, acc_val0_klr = KLR(K_tr0, ytr0[:,1], K_val0, yval0[:,1], lambdas, tresh=1e-5)\n",
    "print(\"*************KLR for dataset 1*************\\n\")\n",
    "alphas_tr1_klr, loss_tr1_klr, acc_1_klr, loss_val1_klr, acc_val1_klr = KLR(K_tr1, ytr1[:,1], K_val1, yval1[:,1], lambdas, tresh=1e-5)\n",
    "print(\"*************KLR for dataset 2*************\\n\")\n",
    "alphas_tr2_klr, loss_tr2_klr, acc_2_klr, loss_val2_klr, acc_val2_klr = KLR(K_tr2, ytr2[:,1], K_val2, yval2[:,1], lambdas, tresh=1e-5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "involved-momentum",
   "metadata": {},
   "source": [
    "## Testing SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "measured-prison",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************* SVM for dataset 0 *************\n",
      "\n",
      "---------------  lambda = 1e-10  ---------------\n",
      "Training: loss = 0.5350, accuracy = 1.000000\n",
      "Validation: loss = 332.0737, accuracy = 0.478000\n",
      "---------------  lambda = 1e-09  ---------------\n",
      "Training: loss = 0.5350, accuracy = 1.000000\n",
      "Validation: loss = 331.9922, accuracy = 0.478000\n",
      "---------------  lambda = 1e-08  ---------------\n",
      "Training: loss = 0.5350, accuracy = 1.000000\n",
      "Validation: loss = 332.0481, accuracy = 0.478000\n",
      "---------------  lambda = 1e-07  ---------------\n",
      "Training: loss = 0.5350, accuracy = 1.000000\n",
      "Validation: loss = 331.9515, accuracy = 0.478000\n",
      "---------------  lambda = 1e-06  ---------------\n",
      "Training: loss = 0.5350, accuracy = 1.000000\n",
      "Validation: loss = 332.0244, accuracy = 0.478000\n",
      "---------------  lambda = 1e-05  ---------------\n",
      "Training: loss = 0.5350, accuracy = 1.000000\n",
      "Validation: loss = 332.0678, accuracy = 0.478000\n",
      "---------------  lambda = 0.0001  ---------------\n",
      "Training: loss = 0.5496, accuracy = 0.995000\n",
      "Validation: loss = 166.7553, accuracy = 0.482000\n",
      "---------------  lambda = 0.001  ---------------\n",
      "Training: loss = 0.8468, accuracy = 0.612000\n",
      "Validation: loss = 0.7878, accuracy = 0.504000\n",
      "---------------  lambda = 0.01  ---------------\n",
      "Training: loss = 0.9842, accuracy = 0.535000\n",
      "Validation: loss = 0.5298, accuracy = 0.504000\n",
      "---------------  lambda = 0.1  ---------------\n",
      "Training: loss = 0.9984, accuracy = 0.535000\n",
      "Validation: loss = 0.5115, accuracy = 0.502000\n",
      "---------------  lambda = 1  ---------------\n",
      "Training: loss = 0.9998, accuracy = 0.535000\n",
      "Validation: loss = 0.8633, accuracy = 0.500000\n",
      "---------------  lambda = 10  ---------------\n",
      "Training: loss = 1.0000, accuracy = 0.535000\n",
      "Validation: loss = 0.9863, accuracy = 0.503000\n",
      "\n",
      "\n",
      "************* SVM for dataset 1 *************\n",
      "\n",
      "---------------  lambda = 1e-10  ---------------\n",
      "Training: loss = 0.5090, accuracy = 1.000000\n",
      "Validation: loss = 363.7223, accuracy = 0.462000\n",
      "---------------  lambda = 1e-09  ---------------\n",
      "Training: loss = 0.5090, accuracy = 1.000000\n",
      "Validation: loss = 363.7576, accuracy = 0.462000\n",
      "---------------  lambda = 1e-08  ---------------\n",
      "Training: loss = 0.5090, accuracy = 1.000000\n",
      "Validation: loss = 363.7301, accuracy = 0.462000\n",
      "---------------  lambda = 1e-07  ---------------\n",
      "Training: loss = 0.5090, accuracy = 1.000000\n",
      "Validation: loss = 363.7194, accuracy = 0.462000\n",
      "---------------  lambda = 1e-06  ---------------\n",
      "Training: loss = 0.5090, accuracy = 1.000000\n",
      "Validation: loss = 363.7449, accuracy = 0.462000\n",
      "---------------  lambda = 1e-05  ---------------\n",
      "Training: loss = 0.5090, accuracy = 1.000000\n",
      "Validation: loss = 363.7236, accuracy = 0.462000\n",
      "---------------  lambda = 0.0001  ---------------\n",
      "Training: loss = 0.5256, accuracy = 0.989000\n",
      "Validation: loss = 373.6265, accuracy = 0.465000\n",
      "---------------  lambda = 0.001  ---------------\n",
      "Training: loss = 0.8470, accuracy = 0.576000\n",
      "Validation: loss = 33.0447, accuracy = 0.472000\n",
      "---------------  lambda = 0.01  ---------------\n",
      "Training: loss = 0.9831, accuracy = 0.509000\n",
      "Validation: loss = 1.9650, accuracy = 0.523000\n",
      "---------------  lambda = 0.1  ---------------\n",
      "Training: loss = 0.9983, accuracy = 0.509000\n",
      "Validation: loss = 0.9257, accuracy = 0.487000\n",
      "---------------  lambda = 1  ---------------\n",
      "Training: loss = 0.9998, accuracy = 0.509000\n",
      "Validation: loss = 0.9860, accuracy = 0.490000\n",
      "---------------  lambda = 10  ---------------\n",
      "Training: loss = 1.0000, accuracy = 0.509000\n",
      "Validation: loss = 0.9986, accuracy = 0.490000\n",
      "\n",
      "\n",
      "************* SVM for dataset 2 *************\n",
      "\n",
      "---------------  lambda = 1e-10  ---------------\n",
      "Training: loss = 0.4990, accuracy = 1.000000\n",
      "Validation: loss = 46.7219, accuracy = 0.494000\n",
      "---------------  lambda = 1e-09  ---------------\n",
      "Training: loss = 0.4990, accuracy = 1.000000\n",
      "Validation: loss = 46.7261, accuracy = 0.494000\n",
      "---------------  lambda = 1e-08  ---------------\n",
      "Training: loss = 0.4990, accuracy = 1.000000\n",
      "Validation: loss = 46.7178, accuracy = 0.494000\n",
      "---------------  lambda = 1e-07  ---------------\n",
      "Training: loss = 0.4990, accuracy = 1.000000\n",
      "Validation: loss = 46.7242, accuracy = 0.494000\n",
      "---------------  lambda = 1e-06  ---------------\n",
      "Training: loss = 0.4990, accuracy = 1.000000\n",
      "Validation: loss = 46.7282, accuracy = 0.494000\n",
      "---------------  lambda = 1e-05  ---------------\n",
      "Training: loss = 0.4990, accuracy = 1.000000\n",
      "Validation: loss = 46.7207, accuracy = 0.494000\n",
      "---------------  lambda = 0.0001  ---------------\n",
      "Training: loss = 0.5086, accuracy = 0.998000\n",
      "Validation: loss = 26.6115, accuracy = 0.491000\n",
      "---------------  lambda = 0.001  ---------------\n",
      "Training: loss = 0.7707, accuracy = 0.692000\n",
      "Validation: loss = 0.6539, accuracy = 0.494000\n",
      "---------------  lambda = 0.01  ---------------\n",
      "Training: loss = 0.9742, accuracy = 0.499000\n",
      "Validation: loss = 0.5203, accuracy = 0.494000\n",
      "---------------  lambda = 0.1  ---------------\n",
      "Training: loss = 0.9974, accuracy = 0.499000\n",
      "Validation: loss = 0.5114, accuracy = 0.490000\n",
      "---------------  lambda = 1  ---------------\n",
      "Training: loss = 0.9997, accuracy = 0.499000\n",
      "Validation: loss = 0.8051, accuracy = 0.487000\n",
      "---------------  lambda = 10  ---------------\n",
      "Training: loss = 1.0000, accuracy = 0.499000\n",
      "Validation: loss = 0.9805, accuracy = 0.504000\n"
     ]
    }
   ],
   "source": [
    "lambdas = [10**i for i in range(-10,2)]\n",
    "print(\"************* SVM for dataset 0 *************\\n\")\n",
    "alphas_tr0, loss_tr0, acc_0, loss_val0, acc_val0 = SVM(K_tr0, ytr0[:,1], K_val0, yval0[:,1], lambdas)\n",
    "\n",
    "print(\"\")\n",
    "print(\"\")\n",
    "print(\"************* SVM for dataset 1 *************\\n\")\n",
    "alphas_tr1, loss_tr1, acc_1, loss_val1, acc_val1 = SVM(K_tr1, ytr1[:,1], K_val1, yval1[:,1],lambdas)\n",
    "\n",
    "print(\"\")\n",
    "print(\"\")\n",
    "print(\"************* SVM for dataset 2 *************\\n\")\n",
    "alphas_tr2, loss_tr2, acc_2, loss_val2, acc_val2 = SVM(K_tr2, ytr2[:,1], K_val2, yval2[:,1],lambdas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "written-dakota",
   "metadata": {},
   "source": [
    "## Making predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lovely-adaptation",
   "metadata": {},
   "source": [
    "### First create the kernels for each testing set with the chosen parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "recognized-congo",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xte0 = np.genfromtxt(\"data/Xte0_mat100.csv\", delimiter='')\n",
    "Xte1 = np.genfromtxt(\"data/Xte1_mat100.csv\", delimiter='')\n",
    "Xte2 = np.genfromtxt(\"data/Xte2_mat100.csv\", delimiter='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "experimental-detective",
   "metadata": {},
   "outputs": [],
   "source": [
    "K_te0 = gaussian_kernel(Xte0,1)\n",
    "K_te1 = gaussian_kernel(Xte1,1)\n",
    "K_te2 = gaussian_kernel(Xte2,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "id": "cooperative-schema",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_predictions_csv(test_kernels, test_alphas, path):\n",
    "    \n",
    "    predictions = np.zeros(3000, dtype=int)\n",
    "    \n",
    "    for i in range(3):\n",
    "        y_pred = test_kernels[i] @ test_alphas[i]\n",
    "        y_pred[y_pred >= 0.5] = 1\n",
    "        y_pred[y_pred < 0.5] = 0\n",
    "        \n",
    "        predictions[1000*i:1000*(i+1)] = y_pred\n",
    "    \n",
    "    #predictions = predictions.astype(int)\n",
    "    pred = pd.DataFrame({\"Bound\" : predictions})\n",
    "    pred.to_csv(path, index=True,index_label=\"Id\")\n",
    "    print(\"saving predictions\")\n",
    "    #np.savetxt(\"data/Ytest_KRR.csv\", predictions, header = \"Id, Bound\", delimiter =\",\")\n",
    "    print(\"saved predictions\")\n",
    "    return(predictions)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cubic-response",
   "metadata": {},
   "source": [
    "Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "id": "major-collapse",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving predictions\n",
      "saved predictions\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, ..., 0, 1, 1])"
      ]
     },
     "execution_count": 393,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_kernels = [K_te0, K_te1, K_te2]\n",
    "#test_alphas = [alphas_tr0[-4], alphas_tr1[-4], alphas_tr2[-3]] # il faut choisir l'alpha associé à un bon lambda!\n",
    "test_alphas = [alphas_tr0_klr[0], alphas_tr1_klr[0], alphas_tr2_klr[0]]\n",
    "write_predictions_csv(test_kernels, test_alphas, path =\"data/Ytest_KLR.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "id": "proved-privilege",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving predictions\n",
      "saved predictions\n"
     ]
    }
   ],
   "source": [
    "test_kernels = [K_te0, K_te1, K_te2]\n",
    "test_alphas = [alphas_tr0[8], alphas_tr1[8], alphas_tr2[10]] # il faut choisir l'alpha associé à un bon lambda!\n",
    "\n",
    "mmm=write_predictions_csv(test_kernels, test_alphas, path=\"data/Ytest_SVM.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
